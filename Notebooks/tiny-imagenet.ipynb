{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(59, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(60, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(253, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(62, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(253, 63, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(253, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 127, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 493, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(493, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(253, 493, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(493, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(493, 119, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(119, 123, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(123, 493, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(493, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(493, 125, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(125, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(121, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(121, 493, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(493, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(493, 125, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 493, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(493, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(493, 246, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(246, 255, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(255, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(493, 975, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(975, 245, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(245, 247, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(247, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(975, 252, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(252, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(975, 254, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(254, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(254, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(254, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(254, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(975, 249, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(249, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(249, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(252, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(975, 250, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(250, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(252, 975, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(975, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(975, 505, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(505, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(505, 505, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(505, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(505, 1211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(975, 1211, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1211, 496, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(496, 506, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(506, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(506, 1211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1211, 486, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(486, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(486, 447, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(447, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(447, 1211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=1211, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 20.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.447 (1.447)\tData 1.256 (1.256)\tLoss 8.6782 (8.6782)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:20,  2.81it/s]Epoch: [0][1000/1563]\tTime 0.372 (0.360)\tData 0.011 (0.020)\tLoss 3.2608 (3.8301)\tPrec@1 23.438 (19.082)\tPrec@5 23.438 (19.082)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.322 (1.322)\tLoss 1.1676 (1.1676)\tPrec@1 71.875 (71.875)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 44.280 Prec@5 72.500\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.637 (1.637)\tData 1.423 (1.423)\tLoss 2.8683 (2.8683)\tPrec@1 31.250 (31.250)\tPrec@5 31.250 (31.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:22,  2.78it/s]Epoch: [1][1000/1563]\tTime 0.370 (0.361)\tData 0.015 (0.021)\tLoss 2.9151 (2.6253)\tPrec@1 35.938 (38.785)\tPrec@5 35.938 (38.785)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.464 (1.464)\tLoss 1.0166 (1.0166)\tPrec@1 73.438 (73.438)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 51.900 Prec@5 78.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.493 (1.493)\tData 1.271 (1.271)\tLoss 1.9226 (1.9226)\tPrec@1 54.688 (54.688)\tPrec@5 54.688 (54.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:20,  2.81it/s]Epoch: [2][1000/1563]\tTime 0.371 (0.361)\tData 0.019 (0.021)\tLoss 2.2904 (2.3449)\tPrec@1 42.188 (44.574)\tPrec@5 42.188 (44.574)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.307 (1.307)\tLoss 1.1483 (1.1483)\tPrec@1 73.438 (73.438)\tPrec@5 85.938 (85.938)\n",
      " * Prec@1 55.100 Prec@5 79.540\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.512 (1.512)\tData 1.305 (1.305)\tLoss 2.2361 (2.2361)\tPrec@1 45.312 (45.312)\tPrec@5 45.312 (45.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:21,  2.79it/s]Epoch: [3][1000/1563]\tTime 0.360 (0.361)\tData 0.022 (0.021)\tLoss 2.2782 (2.1938)\tPrec@1 43.750 (48.108)\tPrec@5 43.750 (48.108)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.395 (1.395)\tLoss 1.0563 (1.0563)\tPrec@1 73.438 (73.438)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 57.700 Prec@5 82.060\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.487 (1.487)\tData 1.265 (1.265)\tLoss 2.2983 (2.2983)\tPrec@1 43.750 (43.750)\tPrec@5 43.750 (43.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:24,  2.76it/s]Epoch: [4][1000/1563]\tTime 0.365 (0.361)\tData 0.022 (0.021)\tLoss 2.3140 (2.0740)\tPrec@1 43.750 (50.351)\tPrec@5 43.750 (50.351)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.502 (1.502)\tLoss 0.7845 (0.7845)\tPrec@1 79.688 (79.688)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 59.500 Prec@5 83.240\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.524 (1.524)\tData 1.309 (1.309)\tLoss 1.6187 (1.6187)\tPrec@1 59.375 (59.375)\tPrec@5 59.375 (59.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.79it/s]Epoch: [5][1000/1563]\tTime 0.359 (0.361)\tData 0.024 (0.021)\tLoss 2.0961 (1.9973)\tPrec@1 51.562 (52.085)\tPrec@5 51.562 (52.085)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.395 (1.395)\tLoss 0.7058 (0.7058)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 60.360 Prec@5 83.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.486 (1.486)\tData 1.260 (1.260)\tLoss 1.8767 (1.8767)\tPrec@1 53.125 (53.125)\tPrec@5 53.125 (53.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:22,  2.78it/s]Epoch: [6][1000/1563]\tTime 0.352 (0.360)\tData 0.022 (0.021)\tLoss 1.7929 (1.9169)\tPrec@1 57.812 (53.748)\tPrec@5 57.812 (53.748)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.492 (1.492)\tLoss 0.8216 (0.8216)\tPrec@1 76.562 (76.562)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 62.720 Prec@5 85.200\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.462 (1.462)\tData 1.258 (1.258)\tLoss 2.0200 (2.0200)\tPrec@1 50.000 (50.000)\tPrec@5 50.000 (50.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:21,  2.79it/s]Epoch: [7][1000/1563]\tTime 0.375 (0.361)\tData 0.021 (0.021)\tLoss 1.4390 (1.8586)\tPrec@1 70.312 (54.973)\tPrec@5 70.312 (54.973)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.370 (1.370)\tLoss 0.8807 (0.8807)\tPrec@1 76.562 (76.562)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 63.100 Prec@5 84.980\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.540 (1.540)\tData 1.333 (1.333)\tLoss 1.8090 (1.8090)\tPrec@1 51.562 (51.562)\tPrec@5 51.562 (51.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:25,  2.74it/s]Epoch: [8][1000/1563]\tTime 0.364 (0.361)\tData 0.023 (0.021)\tLoss 2.0611 (1.8060)\tPrec@1 46.875 (56.035)\tPrec@5 46.875 (56.035)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.388 (1.388)\tLoss 0.8233 (0.8233)\tPrec@1 79.688 (79.688)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 62.600 Prec@5 84.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.500 (1.500)\tData 1.309 (1.309)\tLoss 1.7395 (1.7395)\tPrec@1 56.250 (56.250)\tPrec@5 56.250 (56.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:21,  2.79it/s]Epoch: [9][1000/1563]\tTime 0.350 (0.360)\tData 0.023 (0.021)\tLoss 2.0566 (1.7577)\tPrec@1 53.125 (57.063)\tPrec@5 53.125 (57.063)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.543 (1.543)\tLoss 0.6387 (0.6387)\tPrec@1 81.250 (81.250)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 64.960 Prec@5 86.320\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [10][0/1563]\tTime 1.544 (1.544)\tData 1.334 (1.334)\tLoss 1.7033 (1.7033)\tPrec@1 56.250 (56.250)\tPrec@5 56.250 (56.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:21,  2.80it/s]Epoch: [10][1000/1563]\tTime 0.385 (0.363)\tData 0.040 (0.022)\tLoss 2.0932 (1.7078)\tPrec@1 51.562 (58.307)\tPrec@5 51.562 (58.307)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.463 (1.463)\tLoss 0.7275 (0.7275)\tPrec@1 82.812 (82.812)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 63.440 Prec@5 85.380\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [11][0/1563]\tTime 1.520 (1.520)\tData 1.318 (1.318)\tLoss 1.4748 (1.4748)\tPrec@1 59.375 (59.375)\tPrec@5 59.375 (59.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:20,  2.81it/s]Epoch: [11][1000/1563]\tTime 0.360 (0.361)\tData 0.023 (0.022)\tLoss 1.6548 (1.6634)\tPrec@1 59.375 (59.102)\tPrec@5 59.375 (59.102)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.453 (1.453)\tLoss 0.7572 (0.7572)\tPrec@1 79.688 (79.688)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.760 Prec@5 86.180\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [12][0/1563]\tTime 1.553 (1.553)\tData 1.349 (1.349)\tLoss 1.2990 (1.2990)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:20,  2.81it/s]Epoch: [12][1000/1563]\tTime 0.356 (0.360)\tData 0.012 (0.022)\tLoss 2.4810 (1.6309)\tPrec@1 45.312 (59.835)\tPrec@5 45.312 (59.835)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.391 (1.391)\tLoss 0.6635 (0.6635)\tPrec@1 81.250 (81.250)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 65.120 Prec@5 86.320\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [13][0/1563]\tTime 1.462 (1.462)\tData 1.248 (1.248)\tLoss 1.4127 (1.4127)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.80it/s]Epoch: [13][1000/1563]\tTime 0.358 (0.362)\tData 0.020 (0.022)\tLoss 1.3763 (1.5987)\tPrec@1 60.938 (60.777)\tPrec@5 60.938 (60.777)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.388 (1.388)\tLoss 0.6523 (0.6523)\tPrec@1 85.938 (85.938)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 64.320 Prec@5 85.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [14][0/1563]\tTime 1.433 (1.433)\tData 1.225 (1.225)\tLoss 1.6854 (1.6854)\tPrec@1 54.688 (54.688)\tPrec@5 54.688 (54.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:19,  2.82it/s]Epoch: [14][1000/1563]\tTime 0.356 (0.361)\tData 0.023 (0.022)\tLoss 1.4832 (1.5569)\tPrec@1 65.625 (61.738)\tPrec@5 65.625 (61.738)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.386 (1.386)\tLoss 0.5485 (0.5485)\tPrec@1 85.938 (85.938)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 65.540 Prec@5 86.160\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [15][0/1563]\tTime 1.638 (1.638)\tData 1.419 (1.419)\tLoss 1.4990 (1.4990)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:25,  2.75it/s]Epoch: [15][1000/1563]\tTime 0.362 (0.362)\tData 0.023 (0.023)\tLoss 2.0214 (1.5405)\tPrec@1 48.438 (61.954)\tPrec@5 48.438 (61.954)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.407 (1.407)\tLoss 0.8004 (0.8004)\tPrec@1 76.562 (76.562)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 64.780 Prec@5 85.540\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [16][0/1563]\tTime 1.530 (1.530)\tData 1.312 (1.312)\tLoss 1.7414 (1.7414)\tPrec@1 56.250 (56.250)\tPrec@5 56.250 (56.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:23,  2.76it/s]Epoch: [16][1000/1563]\tTime 0.361 (0.362)\tData 0.019 (0.023)\tLoss 1.5612 (1.5141)\tPrec@1 56.250 (62.558)\tPrec@5 56.250 (62.558)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.554 (1.554)\tLoss 0.7768 (0.7768)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 66.480 Prec@5 86.640\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [17][0/1563]\tTime 1.606 (1.606)\tData 1.385 (1.385)\tLoss 1.1993 (1.1993)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:24,  2.75it/s]Epoch: [17][1000/1563]\tTime 0.352 (0.361)\tData 0.023 (0.023)\tLoss 1.1428 (1.4789)\tPrec@1 68.750 (63.235)\tPrec@5 68.750 (63.235)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.462 (1.462)\tLoss 0.5553 (0.5553)\tPrec@1 89.062 (89.062)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.380 Prec@5 86.060\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [18][0/1563]\tTime 1.577 (1.577)\tData 1.389 (1.389)\tLoss 1.7267 (1.7267)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.80it/s]Epoch: [18][1000/1563]\tTime 0.352 (0.361)\tData 0.025 (0.023)\tLoss 1.4606 (1.4637)\tPrec@1 65.625 (63.807)\tPrec@5 65.625 (63.807)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.492 (1.492)\tLoss 0.5716 (0.5716)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 67.020 Prec@5 87.120\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [19][0/1563]\tTime 1.578 (1.578)\tData 1.382 (1.382)\tLoss 1.5810 (1.5810)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:22,  2.78it/s]Epoch: [19][1000/1563]\tTime 0.363 (0.363)\tData 0.020 (0.023)\tLoss 1.2060 (1.4567)\tPrec@1 67.188 (63.953)\tPrec@5 67.188 (63.953)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.462 (1.462)\tLoss 0.7501 (0.7501)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 66.380 Prec@5 86.300\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [20][0/1563]\tTime 1.486 (1.486)\tData 1.271 (1.271)\tLoss 1.1505 (1.1505)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:20,  2.81it/s]Epoch: [20][1000/1563]\tTime 0.355 (0.360)\tData 0.014 (0.023)\tLoss 1.4462 (1.4072)\tPrec@1 64.062 (65.032)\tPrec@5 64.062 (65.032)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.397 (1.397)\tLoss 0.5052 (0.5052)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 66.080 Prec@5 86.120\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [21][0/1563]\tTime 1.481 (1.481)\tData 1.280 (1.280)\tLoss 1.5031 (1.5031)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:20,  2.80it/s]Epoch: [21][1000/1563]\tTime 0.349 (0.361)\tData 0.022 (0.023)\tLoss 1.4686 (1.3867)\tPrec@1 68.750 (65.458)\tPrec@5 68.750 (65.458)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.6508 (0.6508)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.820 Prec@5 86.180\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [22][0/1563]\tTime 1.537 (1.537)\tData 1.342 (1.342)\tLoss 1.4647 (1.4647)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:22,  2.78it/s]Epoch: [22][1000/1563]\tTime 0.367 (0.360)\tData 0.025 (0.023)\tLoss 1.0452 (1.3698)\tPrec@1 73.438 (65.623)\tPrec@5 73.438 (65.623)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.333 (1.333)\tLoss 0.7338 (0.7338)\tPrec@1 81.250 (81.250)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.740 Prec@5 86.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [23][0/1563]\tTime 2.399 (2.399)\tData 2.188 (2.188)\tLoss 1.5270 (1.5270)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:22,  2.78it/s]Epoch: [23][1000/1563]\tTime 0.363 (0.362)\tData 0.022 (0.024)\tLoss 1.1663 (1.3515)\tPrec@1 62.500 (66.235)\tPrec@5 62.500 (66.235)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.524 (1.524)\tLoss 0.6545 (0.6545)\tPrec@1 81.250 (81.250)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.800 Prec@5 86.620\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [24][0/1563]\tTime 1.596 (1.596)\tData 1.385 (1.385)\tLoss 1.5010 (1.5010)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:22,  2.78it/s]Epoch: [24][1000/1563]\tTime 0.361 (0.362)\tData 0.022 (0.024)\tLoss 1.3053 (1.3532)\tPrec@1 65.625 (66.145)\tPrec@5 65.625 (66.145)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.514 (1.514)\tLoss 0.4929 (0.4929)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.720 Prec@5 87.060\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [25][0/1563]\tTime 1.618 (1.618)\tData 1.411 (1.411)\tLoss 1.6165 (1.6165)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:21,  2.80it/s]Epoch: [25][1000/1563]\tTime 0.353 (0.361)\tData 0.025 (0.024)\tLoss 1.1288 (1.3059)\tPrec@1 75.000 (67.283)\tPrec@5 75.000 (67.283)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.524 (1.524)\tLoss 0.5984 (0.5984)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 66.360 Prec@5 86.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [26][0/1563]\tTime 1.572 (1.572)\tData 1.346 (1.346)\tLoss 1.6528 (1.6528)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:21,  2.80it/s]Epoch: [26][1000/1563]\tTime 0.358 (0.363)\tData 0.022 (0.024)\tLoss 1.2066 (1.2942)\tPrec@1 65.625 (67.723)\tPrec@5 65.625 (67.723)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.463 (1.463)\tLoss 0.8658 (0.8658)\tPrec@1 78.125 (78.125)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 66.120 Prec@5 86.760\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [27][0/1563]\tTime 1.452 (1.452)\tData 1.255 (1.255)\tLoss 1.1987 (1.1987)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.79it/s]Epoch: [27][1000/1563]\tTime 0.359 (0.361)\tData 0.020 (0.024)\tLoss 1.3633 (1.2914)\tPrec@1 65.625 (67.737)\tPrec@5 65.625 (67.737)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 2.076 (2.076)\tLoss 0.5824 (0.5824)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.700 Prec@5 86.200\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [28][0/1563]\tTime 1.448 (1.448)\tData 1.251 (1.251)\tLoss 1.1131 (1.1131)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:20,  2.80it/s]Epoch: [28][1000/1563]\tTime 0.362 (0.361)\tData 0.022 (0.024)\tLoss 1.1720 (1.2789)\tPrec@1 71.875 (67.793)\tPrec@5 71.875 (67.793)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.412 (1.412)\tLoss 0.6528 (0.6528)\tPrec@1 79.688 (79.688)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 66.720 Prec@5 86.560\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [29][0/1563]\tTime 1.522 (1.522)\tData 1.315 (1.315)\tLoss 1.1305 (1.1305)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:25,  2.74it/s]Epoch: [29][1000/1563]\tTime 0.355 (0.361)\tData 0.027 (0.025)\tLoss 1.3282 (1.2651)\tPrec@1 65.625 (68.435)\tPrec@5 65.625 (68.435)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.411 (1.411)\tLoss 0.7336 (0.7336)\tPrec@1 84.375 (84.375)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.920 Prec@5 86.920\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [30][0/1563]\tTime 1.623 (1.623)\tData 1.411 (1.411)\tLoss 0.8233 (0.8233)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:24,  2.76it/s]Epoch: [30][1000/1563]\tTime 0.365 (0.363)\tData 0.024 (0.025)\tLoss 0.8402 (1.0198)\tPrec@1 81.250 (74.641)\tPrec@5 81.250 (74.641)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.575 (1.575)\tLoss 0.5982 (0.5982)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 71.260 Prec@5 89.200\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [31][0/1563]\tTime 1.490 (1.490)\tData 1.267 (1.267)\tLoss 0.7713 (0.7713)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:04<03:25,  2.74it/s]Epoch: [31][1000/1563]\tTime 0.356 (0.364)\tData 0.023 (0.025)\tLoss 0.9094 (0.9103)\tPrec@1 78.125 (77.711)\tPrec@5 78.125 (77.711)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:28<00:00,  2.75it/s]\n",
      "Test: [0/79]\tTime 1.478 (1.478)\tLoss 0.4898 (0.4898)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.860 Prec@5 89.500\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [32][0/1563]\tTime 1.531 (1.531)\tData 1.319 (1.319)\tLoss 1.1886 (1.1886)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:21,  2.80it/s]Epoch: [32][1000/1563]\tTime 0.354 (0.364)\tData 0.026 (0.025)\tLoss 1.1555 (0.8637)\tPrec@1 71.875 (78.587)\tPrec@5 71.875 (78.587)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:26<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.400 (1.400)\tLoss 0.5082 (0.5082)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 72.160 Prec@5 89.800\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [33][0/1563]\tTime 1.485 (1.485)\tData 1.269 (1.269)\tLoss 0.9146 (0.9146)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:20,  2.80it/s]Epoch: [33][1000/1563]\tTime 0.363 (0.362)\tData 0.024 (0.025)\tLoss 1.0944 (0.8479)\tPrec@1 71.875 (78.968)\tPrec@5 71.875 (78.968)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.401 (1.401)\tLoss 0.5255 (0.5255)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.460 Prec@5 89.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [34][0/1563]\tTime 1.552 (1.552)\tData 1.320 (1.320)\tLoss 0.6605 (0.6605)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.79it/s]Epoch: [34][1000/1563]\tTime 0.351 (0.361)\tData 0.026 (0.025)\tLoss 0.8398 (0.8263)\tPrec@1 81.250 (79.602)\tPrec@5 81.250 (79.602)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.306 (1.306)\tLoss 0.5049 (0.5049)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.080 Prec@5 89.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [35][0/1563]\tTime 1.564 (1.564)\tData 1.361 (1.361)\tLoss 0.5908 (0.5908)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:04<03:28,  2.70it/s]Epoch: [35][1000/1563]\tTime 0.355 (0.364)\tData 0.028 (0.026)\tLoss 0.8693 (0.8056)\tPrec@1 78.125 (80.104)\tPrec@5 78.125 (80.104)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:27<00:00,  2.75it/s]\n",
      "Test: [0/79]\tTime 1.457 (1.457)\tLoss 0.5270 (0.5270)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.460 Prec@5 90.080\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [36][0/1563]\tTime 1.594 (1.594)\tData 1.393 (1.393)\tLoss 0.7721 (0.7721)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:21,  2.80it/s]Epoch: [36][1000/1563]\tTime 0.360 (0.363)\tData 0.025 (0.026)\tLoss 1.0785 (0.7979)\tPrec@1 70.312 (80.470)\tPrec@5 70.312 (80.470)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.559 (1.559)\tLoss 0.4247 (0.4247)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.100 Prec@5 89.980\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [37][0/1563]\tTime 1.562 (1.562)\tData 1.361 (1.361)\tLoss 0.7748 (0.7748)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.79it/s]Epoch: [37][1000/1563]\tTime 0.350 (0.361)\tData 0.026 (0.026)\tLoss 0.6899 (0.7692)\tPrec@1 79.688 (81.127)\tPrec@5 79.688 (81.127)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.457 (1.457)\tLoss 0.4784 (0.4784)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.260 Prec@5 89.880\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [38][0/1563]\tTime 1.461 (1.461)\tData 1.261 (1.261)\tLoss 0.6655 (0.6655)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:22,  2.78it/s]Epoch: [38][1000/1563]\tTime 0.380 (0.362)\tData 0.021 (0.026)\tLoss 0.8833 (0.7659)\tPrec@1 81.250 (81.094)\tPrec@5 81.250 (81.094)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.503 (1.503)\tLoss 0.4968 (0.4968)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.420 Prec@5 89.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [39][0/1563]\tTime 1.532 (1.532)\tData 1.339 (1.339)\tLoss 0.6668 (0.6668)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:20,  2.81it/s]Epoch: [39][1000/1563]\tTime 0.358 (0.361)\tData 0.021 (0.026)\tLoss 0.8020 (0.7660)\tPrec@1 79.688 (81.091)\tPrec@5 79.688 (81.091)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.487 (1.487)\tLoss 0.4915 (0.4915)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.880 Prec@5 89.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [40][0/1563]\tTime 1.498 (1.498)\tData 1.284 (1.284)\tLoss 0.8720 (0.8720)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:20,  2.81it/s]Epoch: [40][1000/1563]\tTime 0.364 (0.362)\tData 0.013 (0.026)\tLoss 0.6076 (0.7529)\tPrec@1 87.500 (81.392)\tPrec@5 87.500 (81.392)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.443 (1.443)\tLoss 0.4792 (0.4792)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 72.160 Prec@5 89.860\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [41][0/1563]\tTime 1.469 (1.469)\tData 1.265 (1.265)\tLoss 0.7735 (0.7735)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:19,  2.82it/s]Epoch: [41][1000/1563]\tTime 0.358 (0.360)\tData 0.027 (0.026)\tLoss 0.5097 (0.7344)\tPrec@1 87.500 (81.963)\tPrec@5 87.500 (81.963)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.397 (1.397)\tLoss 0.4440 (0.4440)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 72.720 Prec@5 89.600\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [42][0/1563]\tTime 1.620 (1.620)\tData 1.410 (1.410)\tLoss 0.9124 (0.9124)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:24,  2.76it/s]Epoch: [42][1000/1563]\tTime 0.364 (0.362)\tData 0.028 (0.026)\tLoss 0.6551 (0.7322)\tPrec@1 84.375 (81.899)\tPrec@5 84.375 (81.899)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.445 (1.445)\tLoss 0.4706 (0.4706)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.620 Prec@5 89.800\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [43][0/1563]\tTime 1.596 (1.596)\tData 1.387 (1.387)\tLoss 0.6464 (0.6464)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:22,  2.78it/s]Epoch: [43][1000/1563]\tTime 0.373 (0.363)\tData 0.031 (0.027)\tLoss 0.4726 (0.7294)\tPrec@1 89.062 (82.074)\tPrec@5 89.062 (82.074)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:26<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.452 (1.452)\tLoss 0.4556 (0.4556)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.500 Prec@5 89.620\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [44][0/1563]\tTime 1.554 (1.554)\tData 1.339 (1.339)\tLoss 0.4828 (0.4828)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:25,  2.75it/s]Epoch: [44][1000/1563]\tTime 0.357 (0.363)\tData 0.023 (0.027)\tLoss 0.7392 (0.7155)\tPrec@1 84.375 (82.446)\tPrec@5 84.375 (82.446)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.530 (1.530)\tLoss 0.4327 (0.4327)\tPrec@1 89.062 (89.062)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.040 Prec@5 89.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [45][0/1563]\tTime 1.622 (1.622)\tData 1.420 (1.420)\tLoss 0.6859 (0.6859)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:25,  2.74it/s]Epoch: [45][1000/1563]\tTime 0.359 (0.363)\tData 0.028 (0.027)\tLoss 0.5945 (0.7135)\tPrec@1 84.375 (82.188)\tPrec@5 84.375 (82.188)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:26<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.493 (1.493)\tLoss 0.4218 (0.4218)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 72.040 Prec@5 89.800\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [46][0/1563]\tTime 1.553 (1.553)\tData 1.358 (1.358)\tLoss 0.6436 (0.6436)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:19,  2.82it/s]Epoch: [46][1000/1563]\tTime 0.358 (0.361)\tData 0.027 (0.027)\tLoss 0.6523 (0.7055)\tPrec@1 82.812 (82.775)\tPrec@5 82.812 (82.775)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.450 (1.450)\tLoss 0.4762 (0.4762)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 72.320 Prec@5 90.080\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [47][0/1563]\tTime 1.480 (1.480)\tData 1.265 (1.265)\tLoss 0.8257 (0.8257)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:22,  2.78it/s]Epoch: [47][1000/1563]\tTime 0.353 (0.363)\tData 0.023 (0.027)\tLoss 0.7154 (0.7000)\tPrec@1 84.375 (82.817)\tPrec@5 84.375 (82.817)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.436 (1.436)\tLoss 0.5035 (0.5035)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.300 Prec@5 89.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [48][0/1563]\tTime 1.460 (1.460)\tData 1.267 (1.267)\tLoss 0.5882 (0.5882)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:20,  2.80it/s]Epoch: [48][1000/1563]\tTime 0.351 (0.361)\tData 0.025 (0.027)\tLoss 0.6813 (0.6924)\tPrec@1 84.375 (82.845)\tPrec@5 84.375 (82.845)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.425 (1.425)\tLoss 0.5037 (0.5037)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.180 Prec@5 89.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [49][0/1563]\tTime 1.592 (1.592)\tData 1.382 (1.382)\tLoss 0.7981 (0.7981)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:25,  2.75it/s]Epoch: [49][1000/1563]\tTime 0.371 (0.361)\tData 0.029 (0.027)\tLoss 0.6095 (0.6808)\tPrec@1 85.938 (83.385)\tPrec@5 85.938 (83.385)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.436 (1.436)\tLoss 0.5114 (0.5114)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.680 Prec@5 89.660\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [50][0/1563]\tTime 1.653 (1.653)\tData 1.457 (1.457)\tLoss 0.6845 (0.6845)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:22,  2.78it/s]Epoch: [50][1000/1563]\tTime 0.357 (0.363)\tData 0.028 (0.027)\tLoss 0.7118 (0.6731)\tPrec@1 87.500 (83.385)\tPrec@5 87.500 (83.385)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.451 (1.451)\tLoss 0.5242 (0.5242)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.400 Prec@5 89.920\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [51][0/1563]\tTime 1.597 (1.597)\tData 1.399 (1.399)\tLoss 0.5952 (0.5952)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:24,  2.75it/s]Epoch: [51][1000/1563]\tTime 0.368 (0.363)\tData 0.025 (0.027)\tLoss 0.5371 (0.6736)\tPrec@1 81.250 (83.576)\tPrec@5 81.250 (83.576)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:25<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.496 (1.496)\tLoss 0.4409 (0.4409)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.600 Prec@5 89.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [52][0/1563]\tTime 1.617 (1.617)\tData 1.403 (1.403)\tLoss 0.6846 (0.6846)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:22,  2.78it/s]Epoch: [52][1000/1563]\tTime 0.369 (0.363)\tData 0.028 (0.028)\tLoss 0.9241 (0.6738)\tPrec@1 73.438 (83.426)\tPrec@5 73.438 (83.426)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:26<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.466 (1.466)\tLoss 0.4447 (0.4447)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.260 Prec@5 89.760\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [53][0/1563]\tTime 1.555 (1.555)\tData 1.344 (1.344)\tLoss 0.8556 (0.8556)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:22,  2.78it/s]Epoch: [53][1000/1563]\tTime 0.348 (0.362)\tData 0.028 (0.028)\tLoss 0.6493 (0.6685)\tPrec@1 82.812 (83.582)\tPrec@5 82.812 (83.582)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.461 (1.461)\tLoss 0.4559 (0.4559)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.980 Prec@5 89.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [54][0/1563]\tTime 1.465 (1.465)\tData 1.261 (1.261)\tLoss 0.8211 (0.8211)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:25,  2.74it/s]Epoch: [54][1000/1563]\tTime 0.364 (0.362)\tData 0.023 (0.028)\tLoss 0.7275 (0.6713)\tPrec@1 81.250 (83.538)\tPrec@5 81.250 (83.538)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.505 (1.505)\tLoss 0.4529 (0.4529)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.560 Prec@5 89.760\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [55][0/1563]\tTime 1.494 (1.494)\tData 1.307 (1.307)\tLoss 0.6684 (0.6684)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.80it/s]Epoch: [55][1000/1563]\tTime 0.363 (0.361)\tData 0.022 (0.028)\tLoss 0.6494 (0.6557)\tPrec@1 84.375 (83.821)\tPrec@5 84.375 (83.821)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.348 (1.348)\tLoss 0.4450 (0.4450)\tPrec@1 89.062 (89.062)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.500 Prec@5 89.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [56][0/1563]\tTime 1.427 (1.427)\tData 1.228 (1.228)\tLoss 0.8009 (0.8009)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:58<03:21,  2.80it/s]Epoch: [56][1000/1563]\tTime 0.352 (0.359)\tData 0.024 (0.029)\tLoss 0.8120 (0.6534)\tPrec@1 84.375 (83.900)\tPrec@5 84.375 (83.900)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:19<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.358 (1.358)\tLoss 0.4944 (0.4944)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.280 Prec@5 89.400\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [57][0/1563]\tTime 1.456 (1.456)\tData 1.248 (1.248)\tLoss 0.5675 (0.5675)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:58<03:20,  2.81it/s]Epoch: [57][1000/1563]\tTime 0.361 (0.359)\tData 0.027 (0.029)\tLoss 0.5981 (0.6493)\tPrec@1 82.812 (84.050)\tPrec@5 82.812 (84.050)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.730 (1.730)\tLoss 0.4999 (0.4999)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.180 Prec@5 89.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [58][0/1563]\tTime 1.474 (1.474)\tData 1.278 (1.278)\tLoss 0.5945 (0.5945)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:58<03:20,  2.81it/s]Epoch: [58][1000/1563]\tTime 0.357 (0.359)\tData 0.025 (0.029)\tLoss 0.5540 (0.6460)\tPrec@1 85.938 (84.153)\tPrec@5 85.938 (84.153)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:19<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.378 (1.378)\tLoss 0.4982 (0.4982)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.620 Prec@5 89.180\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [59][0/1563]\tTime 1.427 (1.427)\tData 1.221 (1.221)\tLoss 0.8352 (0.8352)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:58<03:21,  2.80it/s]Epoch: [59][1000/1563]\tTime 0.357 (0.359)\tData 0.027 (0.029)\tLoss 0.3861 (0.6435)\tPrec@1 92.188 (84.239)\tPrec@5 92.188 (84.239)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:19<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.409 (1.409)\tLoss 0.5101 (0.5101)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.400 Prec@5 89.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [60][0/1563]\tTime 1.497 (1.497)\tData 1.298 (1.298)\tLoss 0.7840 (0.7840)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:22,  2.78it/s]Epoch: [60][1000/1563]\tTime 0.353 (0.361)\tData 0.030 (0.029)\tLoss 0.5301 (0.6221)\tPrec@1 85.938 (84.848)\tPrec@5 85.938 (84.848)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.434 (1.434)\tLoss 0.5156 (0.5156)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.400 Prec@5 89.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [61][0/1563]\tTime 1.480 (1.480)\tData 1.281 (1.281)\tLoss 0.5295 (0.5295)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [61][1000/1563]\tTime 0.349 (0.359)\tData 0.030 (0.030)\tLoss 0.6885 (0.6128)\tPrec@1 81.250 (85.023)\tPrec@5 81.250 (85.023)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.380 (1.380)\tLoss 0.5194 (0.5194)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.820 Prec@5 89.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [62][0/1563]\tTime 1.475 (1.475)\tData 1.264 (1.264)\tLoss 0.5620 (0.5620)\tPrec@1 90.625 (90.625)\tPrec@5 90.625 (90.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:58<03:21,  2.80it/s]Epoch: [62][1000/1563]\tTime 0.362 (0.359)\tData 0.031 (0.030)\tLoss 0.7323 (0.6051)\tPrec@1 82.812 (85.283)\tPrec@5 82.812 (85.283)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:19<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.382 (1.382)\tLoss 0.5290 (0.5290)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.920 Prec@5 89.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [63][0/1563]\tTime 1.499 (1.499)\tData 1.271 (1.271)\tLoss 0.9398 (0.9398)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:03<03:23,  2.76it/s]Epoch: [63][1000/1563]\tTime 0.363 (0.364)\tData 0.028 (0.030)\tLoss 0.4152 (0.6073)\tPrec@1 93.750 (85.251)\tPrec@5 93.750 (85.251)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:27<00:00,  2.75it/s]\n",
      "Test: [0/79]\tTime 1.385 (1.385)\tLoss 0.5135 (0.5135)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.000 Prec@5 89.560\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [64][0/1563]\tTime 1.440 (1.440)\tData 1.209 (1.209)\tLoss 0.3849 (0.3849)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:20,  2.81it/s]Epoch: [64][1000/1563]\tTime 0.353 (0.361)\tData 0.027 (0.030)\tLoss 0.5830 (0.6069)\tPrec@1 89.062 (85.182)\tPrec@5 89.062 (85.182)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.348 (1.348)\tLoss 0.5042 (0.5042)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.840 Prec@5 89.640\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [65][0/1563]\tTime 1.467 (1.467)\tData 1.252 (1.252)\tLoss 0.7433 (0.7433)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:22,  2.78it/s]Epoch: [65][1000/1563]\tTime 0.352 (0.360)\tData 0.030 (0.030)\tLoss 0.3118 (0.6023)\tPrec@1 92.188 (85.305)\tPrec@5 92.188 (85.305)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.387 (1.387)\tLoss 0.4974 (0.4974)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 73.020 Prec@5 89.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [66][0/1563]\tTime 1.509 (1.509)\tData 1.283 (1.283)\tLoss 0.4853 (0.4853)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.80it/s]Epoch: [66][1000/1563]\tTime 0.355 (0.359)\tData 0.029 (0.031)\tLoss 0.7391 (0.6061)\tPrec@1 87.500 (85.135)\tPrec@5 87.500 (85.135)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.374 (1.374)\tLoss 0.4994 (0.4994)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.640 Prec@5 89.420\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [67][0/1563]\tTime 1.783 (1.783)\tData 1.594 (1.594)\tLoss 0.3248 (0.3248)\tPrec@1 93.750 (93.750)\tPrec@5 93.750 (93.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:19,  2.82it/s]Epoch: [67][1000/1563]\tTime 0.356 (0.360)\tData 0.025 (0.031)\tLoss 0.8344 (0.5964)\tPrec@1 79.688 (85.522)\tPrec@5 79.688 (85.522)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.372 (1.372)\tLoss 0.5070 (0.5070)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.840 Prec@5 89.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [68][0/1563]\tTime 1.471 (1.471)\tData 1.263 (1.263)\tLoss 0.7699 (0.7699)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:21,  2.80it/s]Epoch: [68][1000/1563]\tTime 0.353 (0.360)\tData 0.032 (0.031)\tLoss 0.4825 (0.5930)\tPrec@1 89.062 (85.577)\tPrec@5 89.062 (85.577)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.412 (1.412)\tLoss 0.4980 (0.4980)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.960 Prec@5 89.380\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [69][0/1563]\tTime 1.485 (1.485)\tData 1.278 (1.278)\tLoss 0.9198 (0.9198)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [69][1000/1563]\tTime 0.360 (0.360)\tData 0.030 (0.031)\tLoss 0.3013 (0.5943)\tPrec@1 90.625 (85.554)\tPrec@5 90.625 (85.554)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.375 (1.375)\tLoss 0.4922 (0.4922)\tPrec@1 89.062 (89.062)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.580 Prec@5 89.360\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [70][0/1563]\tTime 1.501 (1.501)\tData 1.299 (1.299)\tLoss 0.3788 (0.3788)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:22,  2.79it/s]Epoch: [70][1000/1563]\tTime 0.356 (0.360)\tData 0.032 (0.031)\tLoss 0.6494 (0.5929)\tPrec@1 79.688 (85.511)\tPrec@5 79.688 (85.511)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.362 (1.362)\tLoss 0.4811 (0.4811)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.880 Prec@5 89.240\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [71][0/1563]\tTime 1.454 (1.454)\tData 1.262 (1.262)\tLoss 0.3957 (0.3957)\tPrec@1 92.188 (92.188)\tPrec@5 92.188 (92.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:22,  2.77it/s]Epoch: [71][1000/1563]\tTime 0.364 (0.361)\tData 0.028 (0.032)\tLoss 0.5007 (0.5890)\tPrec@1 87.500 (85.535)\tPrec@5 87.500 (85.535)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.339 (1.339)\tLoss 0.4945 (0.4945)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.780 Prec@5 89.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [72][0/1563]\tTime 1.445 (1.445)\tData 1.240 (1.240)\tLoss 0.8047 (0.8047)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:22,  2.78it/s]Epoch: [72][1000/1563]\tTime 0.364 (0.359)\tData 0.020 (0.031)\tLoss 0.6452 (0.5942)\tPrec@1 85.938 (85.536)\tPrec@5 85.938 (85.536)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.402 (1.402)\tLoss 0.4918 (0.4918)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.460 Prec@5 89.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [73][0/1563]\tTime 1.520 (1.520)\tData 1.325 (1.325)\tLoss 0.4927 (0.4927)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [73][1000/1563]\tTime 0.360 (0.359)\tData 0.031 (0.032)\tLoss 0.6307 (0.5961)\tPrec@1 87.500 (85.447)\tPrec@5 87.500 (85.447)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.377 (1.377)\tLoss 0.4713 (0.4713)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.940 Prec@5 89.540\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [74][0/1563]\tTime 1.485 (1.485)\tData 1.275 (1.275)\tLoss 0.5648 (0.5648)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [74][1000/1563]\tTime 0.353 (0.360)\tData 0.029 (0.032)\tLoss 0.3997 (0.5910)\tPrec@1 90.625 (85.660)\tPrec@5 90.625 (85.660)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.365 (1.365)\tLoss 0.4966 (0.4966)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.900 Prec@5 89.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [75][0/1563]\tTime 1.450 (1.450)\tData 1.256 (1.256)\tLoss 0.6872 (0.6872)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:26,  2.72it/s]Epoch: [75][1000/1563]\tTime 0.360 (0.361)\tData 0.024 (0.032)\tLoss 0.5349 (0.5898)\tPrec@1 84.375 (85.561)\tPrec@5 84.375 (85.561)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.349 (1.349)\tLoss 0.4973 (0.4973)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.720 Prec@5 89.600\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [76][0/1563]\tTime 1.491 (1.491)\tData 1.284 (1.284)\tLoss 0.5684 (0.5684)\tPrec@1 90.625 (90.625)\tPrec@5 90.625 (90.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [76][1000/1563]\tTime 0.357 (0.359)\tData 0.033 (0.031)\tLoss 0.5018 (0.5940)\tPrec@1 90.625 (85.483)\tPrec@5 90.625 (85.483)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.360 (1.360)\tLoss 0.5286 (0.5286)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.700 Prec@5 89.460\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [77][0/1563]\tTime 1.456 (1.456)\tData 1.251 (1.251)\tLoss 0.5552 (0.5552)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.80it/s]Epoch: [77][1000/1563]\tTime 0.363 (0.360)\tData 0.033 (0.032)\tLoss 0.6048 (0.5988)\tPrec@1 82.812 (85.396)\tPrec@5 82.812 (85.396)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.362 (1.362)\tLoss 0.5097 (0.5097)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.840 Prec@5 89.340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [78][0/1563]\tTime 1.485 (1.485)\tData 1.267 (1.267)\tLoss 0.5735 (0.5735)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:22,  2.78it/s]Epoch: [78][1000/1563]\tTime 0.363 (0.360)\tData 0.032 (0.032)\tLoss 0.4020 (0.5860)\tPrec@1 90.625 (85.686)\tPrec@5 90.625 (85.686)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.365 (1.365)\tLoss 0.5091 (0.5091)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 73.000 Prec@5 89.660\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [79][0/1563]\tTime 1.466 (1.466)\tData 1.259 (1.259)\tLoss 0.7330 (0.7330)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [79][1000/1563]\tTime 0.355 (0.360)\tData 0.032 (0.033)\tLoss 0.6509 (0.5799)\tPrec@1 79.688 (85.849)\tPrec@5 79.688 (85.849)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.508 (1.508)\tLoss 0.5032 (0.5032)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.900 Prec@5 89.480\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [80][0/1563]\tTime 1.460 (1.460)\tData 1.252 (1.252)\tLoss 0.6732 (0.6732)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:23,  2.77it/s]Epoch: [80][1000/1563]\tTime 0.353 (0.360)\tData 0.032 (0.032)\tLoss 0.6707 (0.5880)\tPrec@1 87.500 (85.571)\tPrec@5 87.500 (85.571)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.386 (1.386)\tLoss 0.5019 (0.5019)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.980 Prec@5 89.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [81][0/1563]\tTime 1.434 (1.434)\tData 1.228 (1.228)\tLoss 0.7733 (0.7733)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:20,  2.80it/s]Epoch: [81][1000/1563]\tTime 0.360 (0.360)\tData 0.029 (0.032)\tLoss 0.5743 (0.5996)\tPrec@1 85.938 (85.418)\tPrec@5 85.938 (85.418)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.390 (1.390)\tLoss 0.4811 (0.4811)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.940 Prec@5 89.400\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [82][0/1563]\tTime 1.490 (1.490)\tData 1.294 (1.294)\tLoss 0.4275 (0.4275)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:21,  2.79it/s]Epoch: [82][1000/1563]\tTime 0.356 (0.360)\tData 0.027 (0.033)\tLoss 0.4496 (0.5951)\tPrec@1 87.500 (85.440)\tPrec@5 87.500 (85.440)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.366 (1.366)\tLoss 0.4902 (0.4902)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.820 Prec@5 89.420\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [83][0/1563]\tTime 1.438 (1.438)\tData 1.238 (1.238)\tLoss 0.2714 (0.2714)\tPrec@1 93.750 (93.750)\tPrec@5 93.750 (93.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:19,  2.82it/s]Epoch: [83][1000/1563]\tTime 0.362 (0.359)\tData 0.029 (0.032)\tLoss 0.5824 (0.5745)\tPrec@1 81.250 (86.092)\tPrec@5 81.250 (86.092)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:20<00:00,  2.79it/s]\n",
      "Test: [0/79]\tTime 1.355 (1.355)\tLoss 0.5068 (0.5068)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.840 Prec@5 89.380\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [84][0/1563]\tTime 1.471 (1.471)\tData 1.281 (1.281)\tLoss 0.6148 (0.6148)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:19,  2.82it/s]Epoch: [84][1000/1563]\tTime 0.361 (0.360)\tData 0.028 (0.033)\tLoss 0.8443 (0.5896)\tPrec@1 73.438 (85.642)\tPrec@5 73.438 (85.642)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.354 (1.354)\tLoss 0.5092 (0.5092)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.820 Prec@5 89.620\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [85][0/1563]\tTime 1.509 (1.509)\tData 1.313 (1.313)\tLoss 0.4568 (0.4568)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:22,  2.78it/s]Epoch: [85][1000/1563]\tTime 0.363 (0.361)\tData 0.035 (0.033)\tLoss 0.8106 (0.5797)\tPrec@1 82.812 (85.810)\tPrec@5 82.812 (85.810)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.412 (1.412)\tLoss 0.5328 (0.5328)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.100 Prec@5 89.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [86][0/1563]\tTime 1.483 (1.483)\tData 1.277 (1.277)\tLoss 0.6762 (0.6762)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:21,  2.79it/s]Epoch: [86][1000/1563]\tTime 0.376 (0.361)\tData 0.031 (0.033)\tLoss 0.5407 (0.5817)\tPrec@1 89.062 (85.850)\tPrec@5 89.062 (85.850)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:23<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.373 (1.373)\tLoss 0.4877 (0.4877)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.920 Prec@5 89.500\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [87][0/1563]\tTime 1.496 (1.496)\tData 1.278 (1.278)\tLoss 0.8194 (0.8194)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:00<03:22,  2.78it/s]Epoch: [87][1000/1563]\tTime 0.360 (0.360)\tData 0.032 (0.034)\tLoss 0.8516 (0.5853)\tPrec@1 82.812 (85.795)\tPrec@5 82.812 (85.795)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:22<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.4988 (0.4988)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.820 Prec@5 89.560\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [88][0/1563]\tTime 1.480 (1.480)\tData 1.275 (1.275)\tLoss 0.9168 (0.9168)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:01<03:22,  2.78it/s]Epoch: [88][1000/1563]\tTime 0.360 (0.362)\tData 0.033 (0.033)\tLoss 0.7264 (0.5793)\tPrec@1 84.375 (85.878)\tPrec@5 84.375 (85.878)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:24<00:00,  2.77it/s]\n",
      "Test: [0/79]\tTime 1.366 (1.366)\tLoss 0.5153 (0.5153)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.840 Prec@5 89.660\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [89][0/1563]\tTime 1.748 (1.748)\tData 1.521 (1.521)\tLoss 0.5579 (0.5579)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:59<03:23,  2.77it/s]Epoch: [89][1000/1563]\tTime 0.355 (0.360)\tData 0.033 (0.034)\tLoss 0.5571 (0.5826)\tPrec@1 89.062 (85.806)\tPrec@5 89.062 (85.806)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:21<00:00,  2.78it/s]\n",
      "Test: [0/79]\tTime 1.367 (1.367)\tLoss 0.4928 (0.4928)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.780 Prec@5 89.480\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.01 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round0.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(44, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(47, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(216, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(56, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(57, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(216, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(57, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(216, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(120, 125, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(125, 412, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(216, 412, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(412, 90, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(90, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(100, 412, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(412, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(118, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(118, 412, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(412, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(124, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 412, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(412, 242, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(242, 253, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(253, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(412, 737, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(737, 214, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(214, 245, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(245, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(737, 230, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(230, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(250, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(737, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(241, 245, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(245, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(737, 235, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(235, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(240, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(737, 239, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(239, 245, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(245, 737, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(737, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(737, 501, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(501, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(501, 500, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(500, 917, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(917, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(737, 917, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(917, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(917, 485, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(485, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(485, 474, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(474, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(474, 917, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(917, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(917, 467, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(467, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(467, 390, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(390, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(390, 917, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(917, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=917, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 16.6M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.611 (1.611)\tData 1.445 (1.445)\tLoss 1.3770 (1.3770)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:08,  2.99it/s]Epoch: [0][1000/1563]\tTime 0.340 (0.327)\tData 0.007 (0.015)\tLoss 1.0729 (1.5013)\tPrec@1 71.875 (62.294)\tPrec@5 71.875 (62.294)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:32<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.516 (1.516)\tLoss 0.5715 (0.5715)\tPrec@1 82.812 (82.812)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 62.720 Prec@5 84.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.559 (1.559)\tData 1.359 (1.359)\tLoss 1.1784 (1.1784)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:06,  3.02it/s]Epoch: [1][1000/1563]\tTime 0.340 (0.328)\tData 0.016 (0.020)\tLoss 1.9820 (1.4319)\tPrec@1 53.125 (63.988)\tPrec@5 53.125 (63.988)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.357 (1.357)\tLoss 0.7724 (0.7724)\tPrec@1 81.250 (81.250)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 64.380 Prec@5 85.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.562 (1.562)\tData 1.363 (1.363)\tLoss 0.9549 (0.9549)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:02,  3.09it/s]Epoch: [2][1000/1563]\tTime 0.319 (0.328)\tData 0.020 (0.020)\tLoss 1.1475 (1.3824)\tPrec@1 70.312 (65.338)\tPrec@5 70.312 (65.338)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.429 (1.429)\tLoss 0.4460 (0.4460)\tPrec@1 89.062 (89.062)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 63.900 Prec@5 85.120\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.486 (1.486)\tData 1.291 (1.291)\tLoss 1.2227 (1.2227)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:06,  3.02it/s]Epoch: [3][1000/1563]\tTime 0.314 (0.328)\tData 0.017 (0.020)\tLoss 1.5769 (1.3554)\tPrec@1 60.938 (65.867)\tPrec@5 60.938 (65.867)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.416 (1.416)\tLoss 0.6706 (0.6706)\tPrec@1 81.250 (81.250)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.800 Prec@5 85.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.560 (1.560)\tData 1.378 (1.378)\tLoss 1.1208 (1.1208)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:04,  3.05it/s]Epoch: [4][1000/1563]\tTime 0.322 (0.328)\tData 0.018 (0.020)\tLoss 1.2279 (1.3288)\tPrec@1 73.438 (66.482)\tPrec@5 73.438 (66.482)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.410 (1.410)\tLoss 0.9308 (0.9308)\tPrec@1 79.688 (79.688)\tPrec@5 85.938 (85.938)\n",
      " * Prec@1 66.080 Prec@5 86.100\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.574 (1.574)\tData 1.376 (1.376)\tLoss 1.2586 (1.2586)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:01,  3.10it/s]Epoch: [5][1000/1563]\tTime 0.325 (0.328)\tData 0.016 (0.020)\tLoss 1.1924 (1.3134)\tPrec@1 70.312 (66.821)\tPrec@5 70.312 (66.821)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:32<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.403 (1.403)\tLoss 0.6426 (0.6426)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.880 Prec@5 85.480\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.463 (1.463)\tData 1.276 (1.276)\tLoss 1.1314 (1.1314)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:10,  2.95it/s]Epoch: [6][1000/1563]\tTime 0.333 (0.329)\tData 0.020 (0.021)\tLoss 1.1942 (1.2869)\tPrec@1 70.312 (67.504)\tPrec@5 70.312 (67.504)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.438 (1.438)\tLoss 0.5961 (0.5961)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 65.100 Prec@5 85.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.492 (1.492)\tData 1.298 (1.298)\tLoss 0.9685 (0.9685)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:05,  3.03it/s]Epoch: [7][1000/1563]\tTime 0.345 (0.330)\tData 0.017 (0.021)\tLoss 1.3503 (1.2736)\tPrec@1 60.938 (67.931)\tPrec@5 60.938 (67.931)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.402 (1.402)\tLoss 0.6595 (0.6595)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.400 Prec@5 85.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.694 (1.694)\tData 1.507 (1.507)\tLoss 1.5108 (1.5108)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:06,  3.01it/s]Epoch: [8][1000/1563]\tTime 0.365 (0.330)\tData 0.018 (0.021)\tLoss 1.1313 (1.2593)\tPrec@1 71.875 (68.112)\tPrec@5 71.875 (68.112)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.662 (1.662)\tLoss 0.7084 (0.7084)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 65.920 Prec@5 86.000\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.512 (1.512)\tData 1.313 (1.313)\tLoss 0.9439 (0.9439)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:11,  2.94it/s]Epoch: [9][1000/1563]\tTime 0.321 (0.329)\tData 0.022 (0.021)\tLoss 1.4756 (1.2398)\tPrec@1 51.562 (68.631)\tPrec@5 51.562 (68.631)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.417 (1.417)\tLoss 0.5960 (0.5960)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 66.380 Prec@5 86.540\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [10][0/1563]\tTime 1.589 (1.589)\tData 1.399 (1.399)\tLoss 1.1989 (1.1989)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:10,  2.95it/s]Epoch: [10][1000/1563]\tTime 0.346 (0.330)\tData 0.019 (0.021)\tLoss 1.3350 (1.2217)\tPrec@1 65.625 (69.115)\tPrec@5 65.625 (69.115)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.407 (1.407)\tLoss 0.5353 (0.5353)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.500 Prec@5 86.420\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [11][0/1563]\tTime 1.459 (1.459)\tData 1.289 (1.289)\tLoss 1.4302 (1.4302)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:08,  2.98it/s]Epoch: [11][1000/1563]\tTime 0.328 (0.329)\tData 0.017 (0.021)\tLoss 1.2929 (1.2158)\tPrec@1 68.750 (69.552)\tPrec@5 68.750 (69.552)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.422 (1.422)\tLoss 0.5665 (0.5665)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.180 Prec@5 85.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [12][0/1563]\tTime 1.490 (1.490)\tData 1.299 (1.299)\tLoss 1.1239 (1.1239)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:09,  2.98it/s]Epoch: [12][1000/1563]\tTime 0.329 (0.329)\tData 0.023 (0.022)\tLoss 1.2853 (1.2013)\tPrec@1 64.062 (69.705)\tPrec@5 64.062 (69.705)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.372 (1.372)\tLoss 0.8802 (0.8802)\tPrec@1 81.250 (81.250)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 66.060 Prec@5 85.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [13][0/1563]\tTime 1.495 (1.495)\tData 1.304 (1.304)\tLoss 1.1560 (1.1560)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:07,  3.01it/s]Epoch: [13][1000/1563]\tTime 0.336 (0.331)\tData 0.019 (0.022)\tLoss 1.3630 (1.1875)\tPrec@1 65.625 (69.974)\tPrec@5 65.625 (69.974)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.633 (1.633)\tLoss 0.8829 (0.8829)\tPrec@1 79.688 (79.688)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.760 Prec@5 85.160\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [14][0/1563]\tTime 1.508 (1.508)\tData 1.307 (1.307)\tLoss 1.2007 (1.2007)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:02,  3.09it/s]Epoch: [14][1000/1563]\tTime 0.334 (0.330)\tData 0.021 (0.021)\tLoss 1.1663 (1.1692)\tPrec@1 64.062 (70.489)\tPrec@5 64.062 (70.489)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.426 (1.426)\tLoss 0.8285 (0.8285)\tPrec@1 78.125 (78.125)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 66.040 Prec@5 86.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [15][0/1563]\tTime 1.502 (1.502)\tData 1.311 (1.311)\tLoss 0.9733 (0.9733)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:04,  3.06it/s]Epoch: [15][1000/1563]\tTime 0.328 (0.331)\tData 0.021 (0.022)\tLoss 1.0586 (1.1610)\tPrec@1 73.438 (70.553)\tPrec@5 73.438 (70.553)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.408 (1.408)\tLoss 0.7579 (0.7579)\tPrec@1 82.812 (82.812)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.500 Prec@5 85.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [16][0/1563]\tTime 1.501 (1.501)\tData 1.302 (1.302)\tLoss 0.9981 (0.9981)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:08,  2.98it/s]Epoch: [16][1000/1563]\tTime 0.332 (0.329)\tData 0.024 (0.022)\tLoss 1.1050 (1.1590)\tPrec@1 68.750 (70.848)\tPrec@5 68.750 (70.848)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:32<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.396 (1.396)\tLoss 0.5330 (0.5330)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.760 Prec@5 85.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [17][0/1563]\tTime 1.482 (1.482)\tData 1.297 (1.297)\tLoss 1.2930 (1.2930)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:06,  3.02it/s]Epoch: [17][1000/1563]\tTime 0.341 (0.329)\tData 0.019 (0.022)\tLoss 0.8057 (1.1419)\tPrec@1 78.125 (70.773)\tPrec@5 78.125 (70.773)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.423 (1.423)\tLoss 0.9089 (0.9089)\tPrec@1 78.125 (78.125)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 65.580 Prec@5 85.880\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [18][0/1563]\tTime 1.498 (1.498)\tData 1.322 (1.322)\tLoss 1.1916 (1.1916)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:05,  3.03it/s]Epoch: [18][1000/1563]\tTime 0.322 (0.328)\tData 0.022 (0.022)\tLoss 1.2864 (1.1351)\tPrec@1 64.062 (71.318)\tPrec@5 64.062 (71.318)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.439 (1.439)\tLoss 0.7377 (0.7377)\tPrec@1 82.812 (82.812)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.920 Prec@5 86.120\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [19][0/1563]\tTime 1.582 (1.582)\tData 1.403 (1.403)\tLoss 0.8484 (0.8484)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:04,  3.06it/s]Epoch: [19][1000/1563]\tTime 0.321 (0.329)\tData 0.021 (0.023)\tLoss 1.1009 (1.1255)\tPrec@1 73.438 (71.558)\tPrec@5 73.438 (71.558)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:33<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.465 (1.465)\tLoss 0.5816 (0.5816)\tPrec@1 84.375 (84.375)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.860 Prec@5 85.540\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [20][0/1563]\tTime 1.598 (1.598)\tData 1.420 (1.420)\tLoss 1.0229 (1.0229)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:04,  3.05it/s]Epoch: [20][1000/1563]\tTime 0.317 (0.330)\tData 0.021 (0.023)\tLoss 0.6654 (0.9070)\tPrec@1 85.938 (77.260)\tPrec@5 85.938 (77.260)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.444 (1.444)\tLoss 0.4655 (0.4655)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.320 Prec@5 88.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [21][0/1563]\tTime 1.585 (1.585)\tData 1.392 (1.392)\tLoss 0.7095 (0.7095)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:07,  3.00it/s]Epoch: [21][1000/1563]\tTime 0.320 (0.332)\tData 0.023 (0.023)\tLoss 0.5820 (0.7851)\tPrec@1 85.938 (80.580)\tPrec@5 85.938 (80.580)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:37<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.444 (1.444)\tLoss 0.4483 (0.4483)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.400 Prec@5 89.080\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [22][0/1563]\tTime 1.598 (1.598)\tData 1.399 (1.399)\tLoss 0.4646 (0.4646)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:07,  3.01it/s]Epoch: [22][1000/1563]\tTime 0.324 (0.332)\tData 0.019 (0.023)\tLoss 0.7627 (0.7472)\tPrec@1 81.250 (81.700)\tPrec@5 81.250 (81.700)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:37<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.483 (1.483)\tLoss 0.4502 (0.4502)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.260 Prec@5 89.480\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [23][0/1563]\tTime 1.542 (1.542)\tData 1.346 (1.346)\tLoss 0.6995 (0.6995)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:06,  3.03it/s]Epoch: [23][1000/1563]\tTime 0.334 (0.331)\tData 0.023 (0.023)\tLoss 0.8460 (0.7310)\tPrec@1 78.125 (82.048)\tPrec@5 78.125 (82.048)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.484 (1.484)\tLoss 0.4948 (0.4948)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.300 Prec@5 89.220\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [24][0/1563]\tTime 1.504 (1.504)\tData 1.321 (1.321)\tLoss 0.7529 (0.7529)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:01,  3.09it/s]Epoch: [24][1000/1563]\tTime 0.325 (0.332)\tData 0.024 (0.024)\tLoss 0.7849 (0.7044)\tPrec@1 84.375 (82.886)\tPrec@5 84.375 (82.886)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:37<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.422 (1.422)\tLoss 0.4646 (0.4646)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.180 Prec@5 89.180\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [25][0/1563]\tTime 1.489 (1.489)\tData 1.298 (1.298)\tLoss 0.3361 (0.3361)\tPrec@1 92.188 (92.188)\tPrec@5 92.188 (92.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:03,  3.07it/s]Epoch: [25][1000/1563]\tTime 0.323 (0.331)\tData 0.021 (0.024)\tLoss 0.6026 (0.6962)\tPrec@1 85.938 (82.962)\tPrec@5 85.938 (82.962)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.402 (1.402)\tLoss 0.4501 (0.4501)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.540 Prec@5 89.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [26][0/1563]\tTime 1.589 (1.589)\tData 1.390 (1.390)\tLoss 0.4297 (0.4297)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:04,  3.05it/s]Epoch: [26][1000/1563]\tTime 0.326 (0.330)\tData 0.013 (0.024)\tLoss 0.6185 (0.6824)\tPrec@1 85.938 (83.184)\tPrec@5 85.938 (83.184)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.432 (1.432)\tLoss 0.4749 (0.4749)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.400 Prec@5 88.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [27][0/1563]\tTime 1.499 (1.499)\tData 1.312 (1.312)\tLoss 0.7251 (0.7251)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:07,  3.00it/s]Epoch: [27][1000/1563]\tTime 0.329 (0.330)\tData 0.025 (0.024)\tLoss 0.8587 (0.6816)\tPrec@1 79.688 (83.195)\tPrec@5 79.688 (83.195)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.445 (1.445)\tLoss 0.4632 (0.4632)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.560 Prec@5 89.160\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [28][0/1563]\tTime 1.600 (1.600)\tData 1.398 (1.398)\tLoss 0.7147 (0.7147)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:05,  3.04it/s]Epoch: [28][1000/1563]\tTime 0.348 (0.330)\tData 0.023 (0.024)\tLoss 0.3500 (0.6625)\tPrec@1 90.625 (83.915)\tPrec@5 90.625 (83.915)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.439 (1.439)\tLoss 0.4855 (0.4855)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.520 Prec@5 88.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [29][0/1563]\tTime 1.463 (1.463)\tData 1.287 (1.287)\tLoss 0.4427 (0.4427)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:04,  3.05it/s]Epoch: [29][1000/1563]\tTime 0.343 (0.331)\tData 0.025 (0.024)\tLoss 0.7145 (0.6543)\tPrec@1 85.938 (84.071)\tPrec@5 85.938 (84.071)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.433 (1.433)\tLoss 0.4697 (0.4697)\tPrec@1 90.625 (90.625)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.680 Prec@5 88.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [30][0/1563]\tTime 1.565 (1.565)\tData 1.372 (1.372)\tLoss 0.5514 (0.5514)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:07,  3.00it/s]Epoch: [30][1000/1563]\tTime 0.334 (0.330)\tData 0.025 (0.024)\tLoss 0.5163 (0.6440)\tPrec@1 84.375 (84.292)\tPrec@5 84.375 (84.292)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.431 (1.431)\tLoss 0.4824 (0.4824)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.600 Prec@5 89.040\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [31][0/1563]\tTime 1.493 (1.493)\tData 1.316 (1.316)\tLoss 0.6104 (0.6104)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:29<03:07,  3.00it/s]Epoch: [31][1000/1563]\tTime 0.327 (0.330)\tData 0.024 (0.024)\tLoss 0.5723 (0.6381)\tPrec@1 84.375 (84.436)\tPrec@5 84.375 (84.436)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:34<00:00,  3.04it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.5205 (0.5205)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.540 Prec@5 88.920\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [32][0/1563]\tTime 1.873 (1.873)\tData 1.719 (1.719)\tLoss 0.7989 (0.7989)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:07,  3.00it/s]Epoch: [32][1000/1563]\tTime 0.338 (0.331)\tData 0.024 (0.025)\tLoss 0.7838 (0.6391)\tPrec@1 81.250 (84.372)\tPrec@5 81.250 (84.372)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.463 (1.463)\tLoss 0.5194 (0.5194)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.780 Prec@5 88.980\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [33][0/1563]\tTime 1.591 (1.591)\tData 1.392 (1.392)\tLoss 0.8877 (0.8877)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:03,  3.07it/s]Epoch: [33][1000/1563]\tTime 0.338 (0.330)\tData 0.073 (0.025)\tLoss 0.5310 (0.6261)\tPrec@1 89.062 (84.782)\tPrec@5 89.062 (84.782)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.443 (1.443)\tLoss 0.5430 (0.5430)\tPrec@1 89.062 (89.062)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 71.420 Prec@5 88.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [34][0/1563]\tTime 1.493 (1.493)\tData 1.333 (1.333)\tLoss 0.5309 (0.5309)\tPrec@1 90.625 (90.625)\tPrec@5 90.625 (90.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:05,  3.03it/s]Epoch: [34][1000/1563]\tTime 0.330 (0.330)\tData 0.026 (0.025)\tLoss 0.4123 (0.6226)\tPrec@1 89.062 (84.932)\tPrec@5 89.062 (84.932)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.403 (1.403)\tLoss 0.4937 (0.4937)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.660 Prec@5 89.020\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [35][0/1563]\tTime 1.502 (1.502)\tData 1.296 (1.296)\tLoss 0.4535 (0.4535)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:04,  3.06it/s]Epoch: [35][1000/1563]\tTime 0.318 (0.331)\tData 0.023 (0.025)\tLoss 0.7303 (0.6216)\tPrec@1 81.250 (84.901)\tPrec@5 81.250 (84.901)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.494 (1.494)\tLoss 0.4922 (0.4922)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.720 Prec@5 88.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [36][0/1563]\tTime 1.515 (1.515)\tData 1.318 (1.318)\tLoss 0.7421 (0.7421)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:02,  3.09it/s]Epoch: [36][1000/1563]\tTime 0.318 (0.331)\tData 0.026 (0.026)\tLoss 0.4626 (0.6109)\tPrec@1 90.625 (85.038)\tPrec@5 90.625 (85.038)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:35<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.731 (1.731)\tLoss 0.5174 (0.5174)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.880 Prec@5 88.640\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [37][0/1563]\tTime 1.554 (1.554)\tData 1.363 (1.363)\tLoss 0.5897 (0.5897)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:05,  3.04it/s]Epoch: [37][1000/1563]\tTime 0.328 (0.332)\tData 0.023 (0.026)\tLoss 0.6315 (0.5999)\tPrec@1 84.375 (85.276)\tPrec@5 84.375 (85.276)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:38<00:00,  3.01it/s]\n",
      "Test: [0/79]\tTime 1.480 (1.480)\tLoss 0.5293 (0.5293)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.760 Prec@5 88.500\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [38][0/1563]\tTime 1.710 (1.710)\tData 1.526 (1.526)\tLoss 0.9459 (0.9459)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:06,  3.02it/s]Epoch: [38][1000/1563]\tTime 0.334 (0.331)\tData 0.025 (0.026)\tLoss 0.7020 (0.6004)\tPrec@1 79.688 (85.449)\tPrec@5 79.688 (85.449)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.455 (1.455)\tLoss 0.5366 (0.5366)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.980 Prec@5 88.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [39][0/1563]\tTime 1.615 (1.615)\tData 1.421 (1.421)\tLoss 0.7221 (0.7221)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:03,  3.07it/s]Epoch: [39][1000/1563]\tTime 0.326 (0.331)\tData 0.024 (0.026)\tLoss 0.8011 (0.6056)\tPrec@1 76.562 (85.326)\tPrec@5 76.562 (85.326)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.452 (1.452)\tLoss 0.5099 (0.5099)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.840 Prec@5 88.660\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [40][0/1563]\tTime 1.545 (1.545)\tData 1.359 (1.359)\tLoss 0.5456 (0.5456)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:06,  3.01it/s]Epoch: [40][1000/1563]\tTime 0.324 (0.333)\tData 0.025 (0.027)\tLoss 0.4374 (0.5906)\tPrec@1 90.625 (85.644)\tPrec@5 90.625 (85.644)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:39<00:00,  3.01it/s]\n",
      "Test: [0/79]\tTime 1.453 (1.453)\tLoss 0.4920 (0.4920)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 71.740 Prec@5 88.660\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [41][0/1563]\tTime 1.512 (1.512)\tData 1.327 (1.327)\tLoss 0.8940 (0.8940)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:03,  3.06it/s]Epoch: [41][1000/1563]\tTime 0.331 (0.332)\tData 0.026 (0.026)\tLoss 0.7136 (0.5775)\tPrec@1 79.688 (86.133)\tPrec@5 79.688 (86.133)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:37<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.485 (1.485)\tLoss 0.5021 (0.5021)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.120 Prec@5 88.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [42][0/1563]\tTime 1.653 (1.653)\tData 1.450 (1.450)\tLoss 0.7768 (0.7768)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:06,  3.01it/s]Epoch: [42][1000/1563]\tTime 0.321 (0.333)\tData 0.025 (0.026)\tLoss 0.2783 (0.5793)\tPrec@1 95.312 (85.997)\tPrec@5 95.312 (85.997)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:38<00:00,  3.01it/s]\n",
      "Test: [0/79]\tTime 1.443 (1.443)\tLoss 0.5219 (0.5219)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.120 Prec@5 88.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [43][0/1563]\tTime 1.571 (1.571)\tData 1.378 (1.378)\tLoss 0.4823 (0.4823)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:04,  3.06it/s]Epoch: [43][1000/1563]\tTime 0.332 (0.332)\tData 0.022 (0.026)\tLoss 0.6181 (0.5667)\tPrec@1 85.938 (86.292)\tPrec@5 85.938 (86.292)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:38<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.475 (1.475)\tLoss 0.4757 (0.4757)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.000 Prec@5 88.860\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [44][0/1563]\tTime 1.549 (1.549)\tData 1.350 (1.350)\tLoss 0.7479 (0.7479)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:09,  2.97it/s]Epoch: [44][1000/1563]\tTime 0.334 (0.333)\tData 0.026 (0.027)\tLoss 0.3746 (0.5739)\tPrec@1 90.625 (85.942)\tPrec@5 90.625 (85.942)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:39<00:00,  3.01it/s]\n",
      "Test: [0/79]\tTime 1.477 (1.477)\tLoss 0.5059 (0.5059)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.180 Prec@5 88.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [45][0/1563]\tTime 1.556 (1.556)\tData 1.361 (1.361)\tLoss 0.6222 (0.6222)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:33<03:04,  3.05it/s]Epoch: [45][1000/1563]\tTime 0.327 (0.333)\tData 0.025 (0.027)\tLoss 0.2365 (0.5804)\tPrec@1 95.312 (85.931)\tPrec@5 95.312 (85.931)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:39<00:00,  3.01it/s]\n",
      "Test: [0/79]\tTime 1.448 (1.448)\tLoss 0.4818 (0.4818)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.220 Prec@5 88.620\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [46][0/1563]\tTime 1.623 (1.623)\tData 1.417 (1.417)\tLoss 0.4754 (0.4754)\tPrec@1 90.625 (90.625)\tPrec@5 90.625 (90.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:05,  3.03it/s]Epoch: [46][1000/1563]\tTime 0.331 (0.332)\tData 0.026 (0.027)\tLoss 0.5049 (0.5709)\tPrec@1 90.625 (86.070)\tPrec@5 90.625 (86.070)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:38<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.442 (1.442)\tLoss 0.4865 (0.4865)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.100 Prec@5 88.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [47][0/1563]\tTime 1.513 (1.513)\tData 1.318 (1.318)\tLoss 0.8546 (0.8546)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:31<03:03,  3.07it/s]Epoch: [47][1000/1563]\tTime 0.325 (0.331)\tData 0.026 (0.027)\tLoss 0.2450 (0.5646)\tPrec@1 96.875 (86.311)\tPrec@5 96.875 (86.311)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.472 (1.472)\tLoss 0.5109 (0.5109)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.260 Prec@5 88.700\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [48][0/1563]\tTime 1.619 (1.619)\tData 1.423 (1.423)\tLoss 0.9248 (0.9248)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:32<03:02,  3.09it/s]Epoch: [48][1000/1563]\tTime 0.326 (0.332)\tData 0.037 (0.028)\tLoss 0.4927 (0.5690)\tPrec@1 85.938 (86.237)\tPrec@5 85.938 (86.237)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:38<00:00,  3.02it/s]\n",
      "Test: [0/79]\tTime 1.518 (1.518)\tLoss 0.4915 (0.4915)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.020 Prec@5 88.800\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [49][0/1563]\tTime 1.515 (1.515)\tData 1.321 (1.321)\tLoss 0.4183 (0.4183)\tPrec@1 90.625 (90.625)\tPrec@5 90.625 (90.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:30<03:04,  3.06it/s]Epoch: [49][1000/1563]\tTime 0.331 (0.331)\tData 0.023 (0.027)\tLoss 0.6467 (0.5673)\tPrec@1 87.500 (86.245)\tPrec@5 87.500 (86.245)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:36<00:00,  3.03it/s]\n",
      "Test: [0/79]\tTime 1.512 (1.512)\tLoss 0.4962 (0.4962)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.160 Prec@5 88.680\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.01 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 1 --epochs 50 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round1.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 39, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(39, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 201, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 201, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(201, 49, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(49, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(55, 201, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(201, 54, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(54, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(63, 201, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(201, 109, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(109, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(109, 119, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(119, 370, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(370, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(201, 370, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(370, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(370, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(69, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(90, 370, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(370, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(370, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(112, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(114, 370, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(370, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(370, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(118, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 370, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(370, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(370, 231, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(231, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(231, 247, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(247, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(370, 616, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(616, 178, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(178, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(178, 217, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(217, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(217, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(616, 202, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(202, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(202, 235, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(235, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(616, 211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(211, 228, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(228, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(616, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(204, 211, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(211, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(616, 215, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(215, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(215, 218, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(218, 616, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(616, 495, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(495, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(495, 492, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(492, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(492, 781, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(781, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(616, 781, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(781, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(781, 442, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(442, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(442, 428, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(428, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(428, 781, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(781, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(781, 433, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(433, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(433, 318, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(318, 781, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(781, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=781, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 13.6M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.438 (1.438)\tData 1.288 (1.288)\tLoss 1.1641 (1.1641)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:47,  3.36it/s]Epoch: [0][1000/1563]\tTime 0.294 (0.294)\tData 0.019 (0.021)\tLoss 1.8374 (1.3851)\tPrec@1 56.250 (64.963)\tPrec@5 56.250 (64.963)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.397 (1.397)\tLoss 0.5637 (0.5637)\tPrec@1 81.250 (81.250)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 62.240 Prec@5 84.460\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.553 (1.553)\tData 1.376 (1.376)\tLoss 1.4917 (1.4917)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:42,  3.47it/s]Epoch: [1][1000/1563]\tTime 0.284 (0.293)\tData 0.018 (0.022)\tLoss 0.9522 (1.3299)\tPrec@1 75.000 (66.508)\tPrec@5 75.000 (66.508)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.438 (1.438)\tLoss 0.5732 (0.5732)\tPrec@1 84.375 (84.375)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 63.800 Prec@5 84.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.577 (1.577)\tData 1.401 (1.401)\tLoss 1.3118 (1.3118)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:47,  3.36it/s]Epoch: [2][1000/1563]\tTime 0.286 (0.293)\tData 0.024 (0.021)\tLoss 1.8570 (1.2808)\tPrec@1 56.250 (67.832)\tPrec@5 56.250 (67.832)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.7654 (0.7654)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 64.900 Prec@5 85.400\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.556 (1.556)\tData 1.389 (1.389)\tLoss 1.4142 (1.4142)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:45,  3.40it/s]Epoch: [3][1000/1563]\tTime 0.309 (0.293)\tData 0.020 (0.022)\tLoss 0.8739 (1.2649)\tPrec@1 73.438 (67.980)\tPrec@5 73.438 (67.980)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.407 (1.407)\tLoss 0.8894 (0.8894)\tPrec@1 76.562 (76.562)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 64.300 Prec@5 85.140\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.492 (1.492)\tData 1.316 (1.316)\tLoss 1.1396 (1.1396)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:44,  3.41it/s]Epoch: [4][1000/1563]\tTime 0.296 (0.293)\tData 0.019 (0.022)\tLoss 1.4232 (1.2269)\tPrec@1 62.500 (68.997)\tPrec@5 62.500 (68.997)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.400 (1.400)\tLoss 0.5999 (0.5999)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.080 Prec@5 84.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.572 (1.572)\tData 1.389 (1.389)\tLoss 1.2546 (1.2546)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:41,  3.49it/s]Epoch: [5][1000/1563]\tTime 0.268 (0.293)\tData 0.021 (0.022)\tLoss 1.3825 (1.2248)\tPrec@1 65.625 (68.990)\tPrec@5 65.625 (68.990)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.439 (1.439)\tLoss 0.6044 (0.6044)\tPrec@1 81.250 (81.250)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.480 Prec@5 85.020\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.467 (1.467)\tData 1.292 (1.292)\tLoss 1.0328 (1.0328)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:44,  3.42it/s]Epoch: [6][1000/1563]\tTime 0.281 (0.293)\tData 0.017 (0.022)\tLoss 1.2520 (1.2130)\tPrec@1 67.188 (69.318)\tPrec@5 67.188 (69.318)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.414 (1.414)\tLoss 0.7984 (0.7984)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.760 Prec@5 84.820\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.421 (1.421)\tData 1.282 (1.282)\tLoss 0.9628 (0.9628)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:42,  3.46it/s]Epoch: [7][1000/1563]\tTime 0.298 (0.293)\tData 0.018 (0.022)\tLoss 1.0611 (1.1997)\tPrec@1 76.562 (69.774)\tPrec@5 76.562 (69.774)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.426 (1.426)\tLoss 0.6862 (0.6862)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 65.160 Prec@5 85.400\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.591 (1.591)\tData 1.410 (1.410)\tLoss 0.8532 (0.8532)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:45,  3.41it/s]Epoch: [8][1000/1563]\tTime 0.278 (0.293)\tData 0.019 (0.022)\tLoss 1.1600 (1.1711)\tPrec@1 70.312 (70.392)\tPrec@5 70.312 (70.392)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.393 (1.393)\tLoss 0.7647 (0.7647)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.920 Prec@5 85.420\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.470 (1.470)\tData 1.280 (1.280)\tLoss 1.2409 (1.2409)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:44,  3.42it/s]Epoch: [9][1000/1563]\tTime 0.285 (0.292)\tData 0.018 (0.023)\tLoss 0.8522 (1.1808)\tPrec@1 75.000 (70.174)\tPrec@5 75.000 (70.174)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:35<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.439 (1.439)\tLoss 1.0125 (1.0125)\tPrec@1 78.125 (78.125)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.440 Prec@5 85.520\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [10][0/1563]\tTime 1.545 (1.545)\tData 1.370 (1.370)\tLoss 0.8776 (0.8776)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:41,  3.48it/s]Epoch: [10][1000/1563]\tTime 0.301 (0.292)\tData 0.024 (0.023)\tLoss 1.0705 (1.1337)\tPrec@1 70.312 (71.046)\tPrec@5 70.312 (71.046)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:35<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.437 (1.437)\tLoss 0.7078 (0.7078)\tPrec@1 82.812 (82.812)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.080 Prec@5 85.000\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [11][0/1563]\tTime 1.524 (1.524)\tData 1.343 (1.343)\tLoss 1.1150 (1.1150)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:33<00:00,  3.44it/s]\n",
      "Test: [0/79]\tTime 1.444 (1.444)\tLoss 0.5891 (0.5891)\tPrec@1 84.375 (84.375)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 63.440 Prec@5 85.360\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [12][0/1563]\tTime 1.500 (1.500)\tData 1.329 (1.329)\tLoss 1.0265 (1.0265)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:38,  3.55it/s]Epoch: [12][1000/1563]\tTime 0.300 (0.293)\tData 0.012 (0.022)\tLoss 0.9594 (1.1331)\tPrec@1 75.000 (71.232)\tPrec@5 75.000 (71.232)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:38<00:00,  3.41it/s]\n",
      "Test: [0/79]\tTime 1.426 (1.426)\tLoss 0.7231 (0.7231)\tPrec@1 84.375 (84.375)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.120 Prec@5 84.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [13][0/1563]\tTime 1.498 (1.498)\tData 1.330 (1.330)\tLoss 1.2107 (1.2107)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:45,  3.41it/s]Epoch: [13][1000/1563]\tTime 0.271 (0.293)\tData 0.019 (0.023)\tLoss 1.2974 (1.1307)\tPrec@1 60.938 (71.454)\tPrec@5 60.938 (71.454)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.427 (1.427)\tLoss 0.6913 (0.6913)\tPrec@1 82.812 (82.812)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.540 Prec@5 85.700\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [14][0/1563]\tTime 1.561 (1.561)\tData 1.392 (1.392)\tLoss 0.9757 (0.9757)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:43,  3.45it/s]Epoch: [14][1000/1563]\tTime 0.292 (0.292)\tData 0.017 (0.024)\tLoss 1.4378 (1.1150)\tPrec@1 64.062 (71.897)\tPrec@5 64.062 (71.897)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.451 (1.451)\tLoss 0.9345 (0.9345)\tPrec@1 78.125 (78.125)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.740 Prec@5 84.600\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [15][0/1563]\tTime 1.487 (1.487)\tData 1.317 (1.317)\tLoss 1.1674 (1.1674)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:51<02:48,  3.33it/s]Epoch: [15][1000/1563]\tTime 0.285 (0.291)\tData 0.018 (0.024)\tLoss 1.2329 (1.0967)\tPrec@1 75.000 (72.164)\tPrec@5 75.000 (72.164)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:35<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.417 (1.417)\tLoss 0.7081 (0.7081)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 65.640 Prec@5 85.100\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [16][0/1563]\tTime 1.562 (1.562)\tData 1.392 (1.392)\tLoss 1.0311 (1.0311)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:40,  3.51it/s]Epoch: [16][1000/1563]\tTime 0.287 (0.293)\tData 0.019 (0.022)\tLoss 1.0076 (1.1075)\tPrec@1 75.000 (71.947)\tPrec@5 75.000 (71.947)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.41it/s]\n",
      "Test: [0/79]\tTime 1.389 (1.389)\tLoss 0.7189 (0.7189)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 64.380 Prec@5 84.700\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [17][0/1563]\tTime 1.513 (1.513)\tData 1.339 (1.339)\tLoss 1.4193 (1.4193)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:47,  3.36it/s]Epoch: [17][1000/1563]\tTime 0.298 (0.292)\tData 0.022 (0.023)\tLoss 1.3218 (1.0949)\tPrec@1 64.062 (72.364)\tPrec@5 64.062 (72.364)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.42it/s]\n",
      "Test: [0/79]\tTime 1.415 (1.415)\tLoss 1.1324 (1.1324)\tPrec@1 73.438 (73.438)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 63.000 Prec@5 83.860\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [18][0/1563]\tTime 1.518 (1.518)\tData 1.353 (1.353)\tLoss 1.3663 (1.3663)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:40,  3.50it/s]Epoch: [18][1000/1563]\tTime 0.280 (0.293)\tData 0.022 (0.023)\tLoss 1.0007 (1.0747)\tPrec@1 76.562 (72.732)\tPrec@5 76.562 (72.732)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:37<00:00,  3.41it/s]\n",
      "Test: [0/79]\tTime 1.468 (1.468)\tLoss 0.7395 (0.7395)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 63.880 Prec@5 84.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [19][0/1563]\tTime 1.796 (1.796)\tData 1.623 (1.623)\tLoss 0.7633 (0.7633)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:54<02:50,  3.30it/s]Epoch: [19][1000/1563]\tTime 0.298 (0.294)\tData 0.020 (0.023)\tLoss 1.4415 (1.0804)\tPrec@1 65.625 (72.732)\tPrec@5 65.625 (72.732)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:38<00:00,  3.41it/s]\n",
      "Test: [0/79]\tTime 1.453 (1.453)\tLoss 0.7343 (0.7343)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 65.200 Prec@5 84.680\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [20][0/1563]\tTime 1.564 (1.564)\tData 1.375 (1.375)\tLoss 1.0640 (1.0640)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:50<02:42,  3.47it/s]Epoch: [20][1000/1563]\tTime 0.278 (0.291)\tData 0.022 (0.026)\tLoss 0.7125 (0.8606)\tPrec@1 82.812 (78.451)\tPrec@5 82.812 (78.451)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:33<00:00,  3.44it/s]\n",
      "Test: [0/79]\tTime 1.449 (1.449)\tLoss 0.7437 (0.7437)\tPrec@1 87.500 (87.500)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 69.660 Prec@5 88.000\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [21][0/1563]\tTime 1.601 (1.601)\tData 1.421 (1.421)\tLoss 0.6999 (0.6999)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:45,  3.40it/s]Epoch: [21][1000/1563]\tTime 0.299 (0.292)\tData 0.024 (0.024)\tLoss 0.5434 (0.7557)\tPrec@1 87.500 (81.312)\tPrec@5 87.500 (81.312)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.461 (1.461)\tLoss 0.7373 (0.7373)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 70.180 Prec@5 88.260\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [22][0/1563]\tTime 1.579 (1.579)\tData 1.396 (1.396)\tLoss 0.6687 (0.6687)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:52<02:49,  3.33it/s]Epoch: [22][1000/1563]\tTime 0.282 (0.292)\tData 0.024 (0.024)\tLoss 0.4875 (0.7253)\tPrec@1 85.938 (82.124)\tPrec@5 85.938 (82.124)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:36<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.427 (1.427)\tLoss 0.6800 (0.6800)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 70.200 Prec@5 88.320\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [23][0/1563]\tTime 1.565 (1.565)\tData 1.405 (1.405)\tLoss 0.3764 (0.3764)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:51<02:39,  3.54it/s]Epoch: [23][1000/1563]\tTime 0.297 (0.292)\tData 0.025 (0.024)\tLoss 0.7527 (0.6898)\tPrec@1 81.250 (83.125)\tPrec@5 81.250 (83.125)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:35<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.421 (1.421)\tLoss 0.6507 (0.6507)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 69.820 Prec@5 88.440\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [24][0/1563]\tTime 1.466 (1.466)\tData 1.312 (1.312)\tLoss 0.7629 (0.7629)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:51<02:43,  3.44it/s]Epoch: [24][1000/1563]\tTime 0.297 (0.292)\tData 0.023 (0.023)\tLoss 0.8041 (0.6776)\tPrec@1 78.125 (83.551)\tPrec@5 78.125 (83.551)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:35<00:00,  3.43it/s]\n",
      "Test: [0/79]\tTime 1.435 (1.435)\tLoss 0.6499 (0.6499)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 70.240 Prec@5 88.880\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [25][0/1563]\tTime 1.548 (1.548)\tData 1.374 (1.374)\tLoss 0.7168 (0.7168)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:53<02:44,  3.42it/s]Epoch: [25][1000/1563]\tTime 0.304 (0.293)\tData 0.023 (0.023)\tLoss 0.6470 (0.6626)\tPrec@1 84.375 (83.751)\tPrec@5 84.375 (83.751)\t\n",
      " 94%|████████████████████████████████████▋  | 1470/1563 [07:10<00:27,  3.35it/s]^C\n",
      " 94%|████████████████████████████████████▋  | 1470/1563 [07:10<00:27,  3.41it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py\", line 325, in <module>\n",
      "    main()\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py\", line 215, in main\n",
      "    train(train_loader, model, criterion, optimizer, epoch, args.print_freq)\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py\", line 249, in train\n",
      "    target = target.cuda()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.01 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 2 --epochs 50 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py\", line 14, in <module>\r\n",
      "    from models import *\r\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/models/__init__.py\", line 5, in <module>\r\n",
      "    from .resnet import resnet18, resnet34, resnet50, resnet101, resnet152\r\n",
      "  File \"/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/models/resnet.py\", line 5, in <module>\r\n",
      "    import torchvision.models\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torchvision/__init__.py\", line 5, in <module>\r\n",
      "    from torchvision import datasets\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torchvision/datasets/__init__.py\", line 1, in <module>\r\n",
      "    from ._optical_flow import KittiFlow, Sintel, FlyingChairs, FlyingThings3D, HD1K\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torchvision/datasets/_optical_flow.py\", line 13, in <module>\r\n",
      "    from .utils import verify_str_arg\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torchvision/datasets/utils.py\", line 21, in <module>\r\n",
      "    import requests\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/requests/__init__.py\", line 43, in <module>\r\n",
      "    import urllib3\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/urllib3/__init__.py\", line 7, in <module>\r\n",
      "    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 11, in <module>\r\n",
      "    from .exceptions import (\r\n",
      "  File \"/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/urllib3/exceptions.py\", line 2, in <module>\r\n",
      "    from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 911, in get_code\r\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 580, in _compile_bytecode\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.01 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 3 --epochs 50 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Number of Parameters: 25.6M\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.236 (2.236)\tData 1.239 (1.239)\tLoss 9.4101 (9.4101)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:22<03:41,  2.54it/s]Epoch: [0][1000/1563]\tTime 0.378 (0.382)\tData 0.022 (0.020)\tLoss 3.2527 (4.0461)\tPrec@1 34.375 (15.199)\tPrec@5 34.375 (15.199)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:58<00:00,  2.61it/s]\n",
      "Test: [0/79]\tTime 1.328 (1.328)\tLoss 2.0157 (2.0157)\tPrec@1 59.375 (59.375)\tPrec@5 76.562 (76.562)\n",
      " * Prec@1 44.860 Prec@5 72.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.512 (1.512)\tData 1.279 (1.279)\tLoss 2.6117 (2.6117)\tPrec@1 32.812 (32.812)\tPrec@5 32.812 (32.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:39,  2.56it/s]Epoch: [1][1000/1563]\tTime 0.387 (0.385)\tData 0.017 (0.020)\tLoss 2.4355 (2.6570)\tPrec@1 46.875 (38.199)\tPrec@5 46.875 (38.199)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:00<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.392 (1.392)\tLoss 1.2859 (1.2859)\tPrec@1 64.062 (64.062)\tPrec@5 84.375 (84.375)\n",
      " * Prec@1 52.560 Prec@5 78.100\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.491 (1.491)\tData 1.261 (1.261)\tLoss 2.4263 (2.4263)\tPrec@1 40.625 (40.625)\tPrec@5 40.625 (40.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:35,  2.62it/s]Epoch: [2][1000/1563]\tTime 0.387 (0.385)\tData 0.021 (0.020)\tLoss 2.5528 (2.3406)\tPrec@1 40.625 (44.618)\tPrec@5 40.625 (44.618)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.401 (1.401)\tLoss 0.7179 (0.7179)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 56.860 Prec@5 81.600\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.498 (1.498)\tData 1.265 (1.265)\tLoss 1.9994 (1.9994)\tPrec@1 45.312 (45.312)\tPrec@5 45.312 (45.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:36,  2.60it/s]Epoch: [3][1000/1563]\tTime 0.376 (0.386)\tData 0.011 (0.020)\tLoss 2.3627 (2.1798)\tPrec@1 40.625 (47.866)\tPrec@5 40.625 (47.866)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.415 (1.415)\tLoss 0.8268 (0.8268)\tPrec@1 76.562 (76.562)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 60.260 Prec@5 83.040\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.484 (1.484)\tData 1.253 (1.253)\tLoss 2.2607 (2.2607)\tPrec@1 43.750 (43.750)\tPrec@5 43.750 (43.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:37,  2.59it/s]Epoch: [4][1000/1563]\tTime 0.385 (0.386)\tData 0.022 (0.020)\tLoss 1.9695 (2.0642)\tPrec@1 45.312 (50.706)\tPrec@5 45.312 (50.706)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.9621 (0.9621)\tPrec@1 76.562 (76.562)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 59.960 Prec@5 83.380\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.558 (1.558)\tData 1.329 (1.329)\tLoss 1.9928 (1.9928)\tPrec@1 53.125 (53.125)\tPrec@5 53.125 (53.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:37,  2.59it/s]Epoch: [5][1000/1563]\tTime 0.388 (0.385)\tData 0.010 (0.020)\tLoss 2.0408 (1.9668)\tPrec@1 53.125 (52.615)\tPrec@5 53.125 (52.615)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:00<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.436 (1.436)\tLoss 0.7338 (0.7338)\tPrec@1 78.125 (78.125)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 62.000 Prec@5 84.100\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.550 (1.550)\tData 1.315 (1.315)\tLoss 2.1371 (2.1371)\tPrec@1 51.562 (51.562)\tPrec@5 51.562 (51.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:38,  2.57it/s]Epoch: [6][1000/1563]\tTime 0.381 (0.386)\tData 0.022 (0.020)\tLoss 1.8527 (1.8913)\tPrec@1 53.125 (54.204)\tPrec@5 53.125 (54.204)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:02<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.399 (1.399)\tLoss 0.7228 (0.7228)\tPrec@1 79.688 (79.688)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 62.720 Prec@5 84.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.593 (1.593)\tData 1.368 (1.368)\tLoss 1.5600 (1.5600)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:40,  2.56it/s]Epoch: [7][1000/1563]\tTime 0.386 (0.387)\tData 0.020 (0.020)\tLoss 1.9718 (1.8188)\tPrec@1 50.000 (55.638)\tPrec@5 50.000 (55.638)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.440 (1.440)\tLoss 1.4082 (1.4082)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 63.380 Prec@5 85.300\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.595 (1.595)\tData 1.378 (1.378)\tLoss 1.6920 (1.6920)\tPrec@1 50.000 (50.000)\tPrec@5 50.000 (50.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:38,  2.58it/s]Epoch: [8][1000/1563]\tTime 0.386 (0.385)\tData 0.012 (0.020)\tLoss 1.6295 (1.7706)\tPrec@1 56.250 (56.915)\tPrec@5 56.250 (56.915)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.405 (1.405)\tLoss 0.8512 (0.8512)\tPrec@1 79.688 (79.688)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 64.100 Prec@5 85.720\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.557 (1.557)\tData 1.325 (1.325)\tLoss 1.6445 (1.6445)\tPrec@1 53.125 (53.125)\tPrec@5 53.125 (53.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:28<03:37,  2.59it/s]Epoch: [9][1000/1563]\tTime 0.402 (0.388)\tData 0.020 (0.021)\tLoss 1.4431 (1.7158)\tPrec@1 65.625 (58.195)\tPrec@5 65.625 (58.195)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 2.057 (2.057)\tLoss 0.7934 (0.7934)\tPrec@1 81.250 (81.250)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 64.200 Prec@5 86.060\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [10][0/1563]\tTime 1.553 (1.553)\tData 1.330 (1.330)\tLoss 1.5489 (1.5489)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:36,  2.60it/s]Epoch: [10][1000/1563]\tTime 0.390 (0.385)\tData 0.022 (0.021)\tLoss 2.2373 (1.6800)\tPrec@1 43.750 (58.827)\tPrec@5 43.750 (58.827)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.412 (1.412)\tLoss 0.6886 (0.6886)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.200 Prec@5 86.280\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [11][0/1563]\tTime 1.502 (1.502)\tData 1.286 (1.286)\tLoss 1.6765 (1.6765)\tPrec@1 57.812 (57.812)\tPrec@5 57.812 (57.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:34,  2.63it/s]Epoch: [11][1000/1563]\tTime 0.372 (0.386)\tData 0.021 (0.021)\tLoss 1.6245 (1.6316)\tPrec@1 62.500 (59.885)\tPrec@5 62.500 (59.885)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.381 (1.381)\tLoss 0.7257 (0.7257)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.260 Prec@5 85.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [12][0/1563]\tTime 1.584 (1.584)\tData 1.359 (1.359)\tLoss 1.4017 (1.4017)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:34,  2.63it/s]Epoch: [12][1000/1563]\tTime 0.388 (0.387)\tData 0.019 (0.021)\tLoss 1.2612 (1.5929)\tPrec@1 62.500 (60.552)\tPrec@5 62.500 (60.552)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.6023 (0.6023)\tPrec@1 85.938 (85.938)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 64.700 Prec@5 85.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [13][0/1563]\tTime 1.694 (1.694)\tData 1.465 (1.465)\tLoss 1.7584 (1.7584)\tPrec@1 57.812 (57.812)\tPrec@5 57.812 (57.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:28<03:43,  2.52it/s]Epoch: [13][1000/1563]\tTime 0.405 (0.388)\tData 0.019 (0.021)\tLoss 1.7179 (1.5590)\tPrec@1 56.250 (61.473)\tPrec@5 56.250 (61.473)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:06<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.629 (1.629)\tLoss 0.5138 (0.5138)\tPrec@1 89.062 (89.062)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 66.080 Prec@5 86.360\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [14][0/1563]\tTime 1.527 (1.527)\tData 1.304 (1.304)\tLoss 1.3025 (1.3025)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:35,  2.62it/s]Epoch: [14][1000/1563]\tTime 0.384 (0.386)\tData 0.019 (0.021)\tLoss 1.6542 (1.5209)\tPrec@1 54.688 (62.377)\tPrec@5 54.688 (62.377)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:02<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.404 (1.404)\tLoss 0.7258 (0.7258)\tPrec@1 81.250 (81.250)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 66.240 Prec@5 86.600\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [15][0/1563]\tTime 1.534 (1.534)\tData 1.309 (1.309)\tLoss 1.9269 (1.9269)\tPrec@1 56.250 (56.250)\tPrec@5 56.250 (56.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:36,  2.60it/s]Epoch: [15][1000/1563]\tTime 0.392 (0.386)\tData 0.022 (0.022)\tLoss 1.3797 (1.2344)\tPrec@1 64.062 (69.406)\tPrec@5 64.062 (69.406)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.412 (1.412)\tLoss 0.5131 (0.5131)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 72.980 Prec@5 90.300\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [16][0/1563]\tTime 1.547 (1.547)\tData 1.315 (1.315)\tLoss 0.8226 (0.8226)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:36,  2.60it/s]Epoch: [16][1000/1563]\tTime 0.382 (0.387)\tData 0.017 (0.022)\tLoss 1.2621 (1.1139)\tPrec@1 70.312 (72.420)\tPrec@5 70.312 (72.420)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:04<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.436 (1.436)\tLoss 0.4892 (0.4892)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.080 Prec@5 90.320\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [17][0/1563]\tTime 1.503 (1.503)\tData 1.282 (1.282)\tLoss 1.0478 (1.0478)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:38,  2.58it/s]Epoch: [17][1000/1563]\tTime 0.379 (0.385)\tData 0.020 (0.021)\tLoss 1.2078 (1.0558)\tPrec@1 70.312 (73.910)\tPrec@5 70.312 (73.910)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.4942 (0.4942)\tPrec@1 82.812 (82.812)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 73.400 Prec@5 90.580\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [18][0/1563]\tTime 1.530 (1.530)\tData 1.303 (1.303)\tLoss 1.2733 (1.2733)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:35,  2.61it/s]Epoch: [18][1000/1563]\tTime 0.386 (0.386)\tData 0.022 (0.022)\tLoss 0.8698 (1.0378)\tPrec@1 78.125 (74.210)\tPrec@5 78.125 (74.210)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:02<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.452 (1.452)\tLoss 0.4578 (0.4578)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.380 Prec@5 90.700\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [19][0/1563]\tTime 1.544 (1.544)\tData 1.333 (1.333)\tLoss 1.1838 (1.1838)\tPrec@1 68.750 (68.750)\tPrec@5 68.750 (68.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:29<03:44,  2.51it/s]Epoch: [19][1000/1563]\tTime 0.406 (0.389)\tData 0.023 (0.023)\tLoss 0.4630 (0.9979)\tPrec@1 87.500 (75.311)\tPrec@5 87.500 (75.311)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:08<00:00,  2.57it/s]\n",
      "Test: [0/79]\tTime 1.420 (1.420)\tLoss 0.4339 (0.4339)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.540 Prec@5 90.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [20][0/1563]\tTime 1.512 (1.512)\tData 1.296 (1.296)\tLoss 0.9820 (0.9820)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:33,  2.64it/s]Epoch: [20][1000/1563]\tTime 0.378 (0.384)\tData 0.025 (0.022)\tLoss 1.0073 (0.9910)\tPrec@1 78.125 (75.378)\tPrec@5 78.125 (75.378)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:00<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.468 (1.468)\tLoss 0.4496 (0.4496)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.660 Prec@5 90.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [21][0/1563]\tTime 1.558 (1.558)\tData 1.320 (1.320)\tLoss 0.9026 (0.9026)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:40,  2.56it/s]Epoch: [21][1000/1563]\tTime 0.388 (0.386)\tData 0.019 (0.022)\tLoss 1.2218 (0.9596)\tPrec@1 73.438 (76.344)\tPrec@5 73.438 (76.344)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.421 (1.421)\tLoss 0.4641 (0.4641)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.940 Prec@5 91.040\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [22][0/1563]\tTime 1.537 (1.537)\tData 1.319 (1.319)\tLoss 0.8276 (0.8276)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:35,  2.62it/s]Epoch: [22][1000/1563]\tTime 0.372 (0.388)\tData 0.023 (0.023)\tLoss 1.0141 (0.9487)\tPrec@1 70.312 (76.363)\tPrec@5 70.312 (76.363)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.430 (1.430)\tLoss 0.4280 (0.4280)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.880 Prec@5 90.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [23][0/1563]\tTime 1.533 (1.533)\tData 1.307 (1.307)\tLoss 0.8713 (0.8713)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:39,  2.57it/s]Epoch: [23][1000/1563]\tTime 0.401 (0.386)\tData 0.011 (0.022)\tLoss 1.0454 (0.9368)\tPrec@1 73.438 (76.745)\tPrec@5 73.438 (76.745)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.431 (1.431)\tLoss 0.3937 (0.3937)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.700 Prec@5 90.640\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [24][0/1563]\tTime 1.496 (1.496)\tData 1.275 (1.275)\tLoss 0.8857 (0.8857)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:38,  2.57it/s]Epoch: [24][1000/1563]\tTime 0.388 (0.388)\tData 0.011 (0.023)\tLoss 1.1206 (0.9177)\tPrec@1 70.312 (77.204)\tPrec@5 70.312 (77.204)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.374 (1.374)\tLoss 0.4434 (0.4434)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.940 Prec@5 90.860\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [25][0/1563]\tTime 1.509 (1.509)\tData 1.299 (1.299)\tLoss 0.8886 (0.8886)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:29<03:43,  2.52it/s]Epoch: [25][1000/1563]\tTime 0.396 (0.389)\tData 0.023 (0.023)\tLoss 0.9919 (0.9031)\tPrec@1 76.562 (77.529)\tPrec@5 76.562 (77.529)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:08<00:00,  2.57it/s]\n",
      "Test: [0/79]\tTime 1.393 (1.393)\tLoss 0.4768 (0.4768)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 74.200 Prec@5 90.820\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [26][0/1563]\tTime 2.042 (2.042)\tData 1.806 (1.806)\tLoss 0.7329 (0.7329)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:28<03:37,  2.59it/s]Epoch: [26][1000/1563]\tTime 0.373 (0.388)\tData 0.018 (0.023)\tLoss 1.1091 (0.8964)\tPrec@1 76.562 (77.880)\tPrec@5 76.562 (77.880)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.398 (1.398)\tLoss 0.4359 (0.4359)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 74.380 Prec@5 90.880\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [27][0/1563]\tTime 1.536 (1.536)\tData 1.322 (1.322)\tLoss 0.9481 (0.9481)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:25<03:30,  2.68it/s]Epoch: [27][1000/1563]\tTime 0.375 (0.385)\tData 0.021 (0.023)\tLoss 1.0056 (0.8842)\tPrec@1 71.875 (78.166)\tPrec@5 71.875 (78.166)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.393 (1.393)\tLoss 0.4965 (0.4965)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 74.200 Prec@5 90.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [28][0/1563]\tTime 1.506 (1.506)\tData 1.280 (1.280)\tLoss 0.7674 (0.7674)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:30<03:46,  2.49it/s]Epoch: [28][1000/1563]\tTime 0.392 (0.390)\tData 0.020 (0.023)\tLoss 0.7154 (0.8698)\tPrec@1 81.250 (78.356)\tPrec@5 81.250 (78.356)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:08<00:00,  2.57it/s]\n",
      "Test: [0/79]\tTime 1.423 (1.423)\tLoss 0.3858 (0.3858)\tPrec@1 90.625 (90.625)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.840 Prec@5 90.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [29][0/1563]\tTime 1.540 (1.540)\tData 1.313 (1.313)\tLoss 0.7081 (0.7081)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:30,  2.67it/s]Epoch: [29][1000/1563]\tTime 0.387 (0.385)\tData 0.021 (0.022)\tLoss 1.0050 (0.8647)\tPrec@1 70.312 (78.624)\tPrec@5 70.312 (78.624)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:00<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.404 (1.404)\tLoss 0.4358 (0.4358)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.560 Prec@5 90.800\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [30][0/1563]\tTime 1.525 (1.525)\tData 1.311 (1.311)\tLoss 0.5914 (0.5914)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:37,  2.59it/s]Epoch: [30][1000/1563]\tTime 0.383 (0.387)\tData 0.024 (0.023)\tLoss 0.9155 (0.8249)\tPrec@1 75.000 (79.452)\tPrec@5 75.000 (79.452)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.407 (1.407)\tLoss 0.4456 (0.4456)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.880 Prec@5 91.080\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [31][0/1563]\tTime 1.536 (1.536)\tData 1.328 (1.328)\tLoss 0.7306 (0.7306)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:29<03:37,  2.59it/s]Epoch: [31][1000/1563]\tTime 0.369 (0.389)\tData 0.024 (0.023)\tLoss 0.7471 (0.8234)\tPrec@1 79.688 (79.642)\tPrec@5 79.688 (79.642)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:07<00:00,  2.57it/s]\n",
      "Test: [0/79]\tTime 1.383 (1.383)\tLoss 0.4274 (0.4274)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.860 Prec@5 91.040\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [32][0/1563]\tTime 1.535 (1.535)\tData 1.299 (1.299)\tLoss 0.7423 (0.7423)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:35,  2.61it/s]Epoch: [32][1000/1563]\tTime 0.369 (0.387)\tData 0.022 (0.023)\tLoss 1.0059 (0.8187)\tPrec@1 75.000 (79.792)\tPrec@5 75.000 (79.792)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.410 (1.410)\tLoss 0.4256 (0.4256)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.900 Prec@5 90.900\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [33][0/1563]\tTime 1.540 (1.540)\tData 1.326 (1.326)\tLoss 0.8114 (0.8114)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:37,  2.59it/s]Epoch: [33][1000/1563]\tTime 0.396 (0.388)\tData 0.014 (0.024)\tLoss 0.8008 (0.8138)\tPrec@1 81.250 (79.769)\tPrec@5 81.250 (79.769)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.4467 (0.4467)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.180 Prec@5 90.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [34][0/1563]\tTime 1.521 (1.521)\tData 1.305 (1.305)\tLoss 0.8834 (0.8834)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:23<03:39,  2.56it/s]Epoch: [34][1000/1563]\tTime 0.403 (0.384)\tData 0.024 (0.023)\tLoss 0.7440 (0.8029)\tPrec@1 81.250 (80.171)\tPrec@5 81.250 (80.171)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:59<00:00,  2.61it/s]\n",
      "Test: [0/79]\tTime 1.395 (1.395)\tLoss 0.4474 (0.4474)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 74.180 Prec@5 90.820\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [35][0/1563]\tTime 1.555 (1.555)\tData 1.333 (1.333)\tLoss 0.6159 (0.6159)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:26<03:32,  2.64it/s]Epoch: [35][1000/1563]\tTime 0.394 (0.387)\tData 0.026 (0.023)\tLoss 0.6841 (0.8027)\tPrec@1 81.250 (80.168)\tPrec@5 81.250 (80.168)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:03<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.380 (1.380)\tLoss 0.4456 (0.4456)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.240 Prec@5 90.940\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [36][0/1563]\tTime 1.540 (1.540)\tData 1.318 (1.318)\tLoss 1.2649 (1.2649)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:36,  2.60it/s]Epoch: [36][1000/1563]\tTime 0.396 (0.385)\tData 0.017 (0.023)\tLoss 0.7381 (0.8083)\tPrec@1 81.250 (80.025)\tPrec@5 81.250 (80.025)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.425 (1.425)\tLoss 0.4410 (0.4410)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.900 Prec@5 90.780\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [37][0/1563]\tTime 1.512 (1.512)\tData 1.305 (1.305)\tLoss 0.7851 (0.7851)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:27<03:34,  2.62it/s]Epoch: [37][1000/1563]\tTime 0.389 (0.387)\tData 0.026 (0.024)\tLoss 1.0465 (0.8050)\tPrec@1 73.438 (80.084)\tPrec@5 73.438 (80.084)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:04<00:00,  2.59it/s]\n",
      "Test: [0/79]\tTime 1.397 (1.397)\tLoss 0.4107 (0.4107)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.160 Prec@5 90.960\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [38][0/1563]\tTime 1.519 (1.519)\tData 1.307 (1.307)\tLoss 0.7451 (0.7451)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 1000/1563 [06:24<03:37,  2.58it/s]Epoch: [38][1000/1563]\tTime 0.382 (0.385)\tData 0.024 (0.023)\tLoss 0.6359 (0.8022)\tPrec@1 81.250 (80.229)\tPrec@5 81.250 (80.229)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:01<00:00,  2.60it/s]\n",
      "Test: [0/79]\tTime 1.420 (1.420)\tLoss 0.4310 (0.4310)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.840 Prec@5 90.840\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [39][0/1563]\tTime 1.562 (1.562)\tData 1.340 (1.340)\tLoss 0.7848 (0.7848)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:28<03:43,  2.52it/s]Epoch: [39][1000/1563]\tTime 0.392 (0.388)\tData 0.021 (0.024)\tLoss 0.8654 (0.8046)\tPrec@1 78.125 (80.111)\tPrec@5 78.125 (80.111)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [10:05<00:00,  2.58it/s]\n",
      "Test: [0/79]\tTime 2.064 (2.064)\tLoss 0.4433 (0.4433)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.840 Prec@5 91.040\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.01 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --round 0 --epochs 40 --adj 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round0.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 49, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(49, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(56, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(241, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(62, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(241, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(241, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(122, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 487, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(487, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(241, 487, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(487, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(487, 107, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(107, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(111, 487, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(487, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(487, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 487, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(487, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(487, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 487, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(487, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(487, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(487, 940, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(940, 246, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(246, 251, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(251, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(251, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(940, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(940, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(255, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(940, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(254, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(254, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(940, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(253, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(252, 940, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(940, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(940, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(511, 510, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(510, 1311, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1311, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(940, 1311, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1311, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1311, 491, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(491, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(491, 473, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(473, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(473, 1311, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1311, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1311, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(510, 446, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(446, 1311, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1311, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=1311, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 20.7M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.389 (1.389)\tData 1.254 (1.254)\tLoss 1.0251 (1.0251)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:02<03:26,  2.72it/s]Epoch: [0][1000/1563]\tTime 0.368 (0.362)\tData 0.019 (0.020)\tLoss 0.9183 (1.0731)\tPrec@1 79.688 (75.053)\tPrec@5 79.688 (75.053)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:26<00:00,  2.76it/s]\n",
      "Test: [0/79]\tTime 1.341 (1.341)\tLoss 0.4056 (0.4056)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 72.980 Prec@5 90.540\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.487 (1.487)\tData 1.278 (1.278)\tLoss 1.0323 (1.0323)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:05<03:22,  2.78it/s]Epoch: [1][1000/1563]\tTime 0.375 (0.365)\tData 0.018 (0.020)\tLoss 1.1285 (0.9768)\tPrec@1 70.312 (76.898)\tPrec@5 70.312 (76.898)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:30<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.391 (1.391)\tLoss 0.4278 (0.4278)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.640 Prec@5 90.480\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.512 (1.512)\tData 1.292 (1.292)\tLoss 1.2163 (1.2163)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:04<03:26,  2.72it/s]Epoch: [2][1000/1563]\tTime 0.369 (0.364)\tData 0.017 (0.020)\tLoss 0.7504 (0.9385)\tPrec@1 82.812 (77.621)\tPrec@5 82.812 (77.621)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:29<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.390 (1.390)\tLoss 0.5009 (0.5009)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 73.340 Prec@5 90.580\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.516 (1.516)\tData 1.304 (1.304)\tLoss 0.9058 (0.9058)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:04<03:28,  2.70it/s]Epoch: [3][1000/1563]\tTime 0.363 (0.365)\tData 0.009 (0.020)\tLoss 1.2814 (0.9216)\tPrec@1 70.312 (77.922)\tPrec@5 70.312 (77.922)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:29<00:00,  2.75it/s]\n",
      "Test: [0/79]\tTime 1.410 (1.410)\tLoss 0.4298 (0.4298)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.220 Prec@5 90.680\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.488 (1.488)\tData 1.293 (1.293)\tLoss 1.1358 (1.1358)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:05<03:21,  2.79it/s]Epoch: [4][1000/1563]\tTime 0.356 (0.365)\tData 0.019 (0.020)\tLoss 0.9768 (0.9120)\tPrec@1 75.000 (78.269)\tPrec@5 75.000 (78.269)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:30<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.342 (1.342)\tLoss 0.4868 (0.4868)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.620 Prec@5 90.220\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.519 (1.519)\tData 1.316 (1.316)\tLoss 0.7834 (0.7834)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:05<03:24,  2.75it/s]Epoch: [5][1000/1563]\tTime 0.363 (0.365)\tData 0.017 (0.020)\tLoss 1.2476 (0.8785)\tPrec@1 70.312 (78.879)\tPrec@5 70.312 (78.879)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:29<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.372 (1.372)\tLoss 0.5190 (0.5190)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.900 Prec@5 90.600\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.511 (1.511)\tData 1.299 (1.299)\tLoss 0.8354 (0.8354)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:05<03:23,  2.76it/s]Epoch: [6][1000/1563]\tTime 0.359 (0.366)\tData 0.021 (0.020)\tLoss 0.6918 (0.8440)\tPrec@1 82.812 (79.581)\tPrec@5 82.812 (79.581)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:30<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.360 (1.360)\tLoss 0.5055 (0.5055)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.820 Prec@5 90.640\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.527 (1.527)\tData 1.309 (1.309)\tLoss 0.9024 (0.9024)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:05<03:28,  2.70it/s]Epoch: [7][1000/1563]\tTime 0.367 (0.366)\tData 0.023 (0.020)\tLoss 1.0641 (0.8497)\tPrec@1 73.438 (79.638)\tPrec@5 73.438 (79.638)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:30<00:00,  2.74it/s]\n",
      "Test: [0/79]\tTime 1.923 (1.923)\tLoss 0.5354 (0.5354)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.200 Prec@5 90.600\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.723 (1.723)\tData 1.493 (1.493)\tLoss 1.3643 (1.3643)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:06<03:27,  2.72it/s]Epoch: [8][1000/1563]\tTime 0.358 (0.367)\tData 0.021 (0.020)\tLoss 0.9392 (0.8380)\tPrec@1 75.000 (79.780)\tPrec@5 75.000 (79.780)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:32<00:00,  2.73it/s]\n",
      "Test: [0/79]\tTime 1.586 (1.586)\tLoss 0.5190 (0.5190)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.220 Prec@5 90.600\n",
      "resnet50tiny-round1.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.521 (1.521)\tData 1.313 (1.313)\tLoss 0.8913 (0.8913)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [06:04<03:21,  2.79it/s]Epoch: [9][1000/1563]\tTime 0.356 (0.365)\tData 0.018 (0.020)\tLoss 0.8539 (0.8398)\tPrec@1 79.688 (79.751)\tPrec@5 79.688 (79.751)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [09:29<00:00,  2.75it/s]\n",
      "Test: [0/79]\tTime 1.438 (1.438)\tLoss 0.5027 (0.5027)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 74.100 Prec@5 90.880\n",
      "resnet50tiny-round1.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 1 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round1.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 43, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(43, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 223, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(223, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 223, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(223, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(223, 53, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(53, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(53, 223, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(223, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(223, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(59, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(63, 223, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(223, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(223, 119, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(119, 127, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 423, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(223, 423, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(423, 86, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(86, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(97, 423, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(423, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(127, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(126, 423, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(423, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(127, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 423, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(423, 247, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(247, 254, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(254, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(254, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(423, 816, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(816, 228, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(228, 241, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(241, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(816, 233, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(233, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(816, 239, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(239, 249, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(249, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(249, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(232, 239, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(239, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(816, 253, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(253, 251, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(251, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(251, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(816, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(511, 500, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(500, 938, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(938, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(816, 938, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(938, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(938, 464, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(464, 420, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(420, 938, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(938, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(938, 469, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(469, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(469, 366, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(366, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(366, 938, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(938, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=938, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 16.8M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.407 (1.407)\tData 1.240 (1.240)\tLoss 1.6062 (1.6062)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:40<03:12,  2.92it/s]Epoch: [0][1000/1563]\tTime 0.340 (0.341)\tData 0.019 (0.019)\tLoss 1.5226 (1.2028)\tPrec@1 64.062 (72.345)\tPrec@5 64.062 (72.345)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:51<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.331 (1.331)\tLoss 0.5537 (0.5537)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.020 Prec@5 89.840\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.460 (1.460)\tData 1.273 (1.273)\tLoss 0.8794 (0.8794)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:40<03:12,  2.92it/s]Epoch: [1][1000/1563]\tTime 0.343 (0.340)\tData 0.018 (0.020)\tLoss 1.1774 (1.0694)\tPrec@1 75.000 (74.933)\tPrec@5 75.000 (74.933)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:51<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.402 (1.402)\tLoss 0.4818 (0.4818)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.600 Prec@5 90.320\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.483 (1.483)\tData 1.287 (1.287)\tLoss 1.2759 (1.2759)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:40<03:08,  2.98it/s]Epoch: [2][1000/1563]\tTime 0.343 (0.341)\tData 0.017 (0.019)\tLoss 1.1230 (1.0224)\tPrec@1 67.188 (75.660)\tPrec@5 67.188 (75.660)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:52<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.347 (1.347)\tLoss 0.5147 (0.5147)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.900 Prec@5 90.260\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.462 (1.462)\tData 1.277 (1.277)\tLoss 0.8716 (0.8716)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:40<03:05,  3.03it/s]Epoch: [3][1000/1563]\tTime 0.340 (0.341)\tData 0.017 (0.019)\tLoss 1.0803 (0.9996)\tPrec@1 73.438 (76.046)\tPrec@5 73.438 (76.046)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:51<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.330 (1.330)\tLoss 0.5575 (0.5575)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.980 Prec@5 90.040\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.451 (1.451)\tData 1.277 (1.277)\tLoss 0.7340 (0.7340)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:41<03:09,  2.97it/s]Epoch: [4][1000/1563]\tTime 0.332 (0.341)\tData 0.011 (0.020)\tLoss 0.8241 (0.9655)\tPrec@1 79.688 (76.847)\tPrec@5 79.688 (76.847)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:52<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.379 (1.379)\tLoss 0.5354 (0.5354)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.780 Prec@5 90.140\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.495 (1.495)\tData 1.298 (1.298)\tLoss 0.8058 (0.8058)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:41<03:11,  2.95it/s]Epoch: [5][1000/1563]\tTime 0.342 (0.342)\tData 0.017 (0.020)\tLoss 0.9696 (0.9471)\tPrec@1 78.125 (77.173)\tPrec@5 78.125 (77.173)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:53<00:00,  2.93it/s]\n",
      "Test: [0/79]\tTime 1.384 (1.384)\tLoss 0.5011 (0.5011)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 72.980 Prec@5 90.100\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.483 (1.483)\tData 1.289 (1.289)\tLoss 0.8920 (0.8920)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:41<03:10,  2.96it/s]Epoch: [6][1000/1563]\tTime 0.341 (0.342)\tData 0.020 (0.020)\tLoss 1.0021 (0.9220)\tPrec@1 75.000 (77.689)\tPrec@5 75.000 (77.689)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:52<00:00,  2.93it/s]\n",
      "Test: [0/79]\tTime 1.375 (1.375)\tLoss 0.4600 (0.4600)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.740 Prec@5 90.440\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.454 (1.454)\tData 1.266 (1.266)\tLoss 0.9204 (0.9204)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:41<03:06,  3.03it/s]Epoch: [7][1000/1563]\tTime 0.343 (0.342)\tData 0.019 (0.020)\tLoss 0.6017 (0.8916)\tPrec@1 87.500 (78.656)\tPrec@5 87.500 (78.656)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:53<00:00,  2.93it/s]\n",
      "Test: [0/79]\tTime 1.393 (1.393)\tLoss 0.4750 (0.4750)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.340 Prec@5 90.560\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.659 (1.659)\tData 1.469 (1.469)\tLoss 0.7903 (0.7903)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:41<03:11,  2.94it/s]Epoch: [8][1000/1563]\tTime 0.342 (0.341)\tData 0.018 (0.020)\tLoss 0.7625 (0.8649)\tPrec@1 75.000 (79.236)\tPrec@5 75.000 (79.236)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:52<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.580 (1.580)\tLoss 0.4886 (0.4886)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.600 Prec@5 90.700\n",
      "resnet50tiny-round2.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.477 (1.477)\tData 1.280 (1.280)\tLoss 1.0742 (1.0742)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:40<03:16,  2.86it/s]Epoch: [9][1000/1563]\tTime 0.355 (0.340)\tData 0.020 (0.020)\tLoss 0.5635 (0.8477)\tPrec@1 85.938 (79.502)\tPrec@5 85.938 (79.502)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:51<00:00,  2.94it/s]\n",
      "Test: [0/79]\tTime 1.352 (1.352)\tLoss 0.4565 (0.4565)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.360 Prec@5 90.800\n",
      "resnet50tiny-round2.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 2 --epochs 20 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round2.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 41, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(41, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 212, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 212, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(212, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(50, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(49, 212, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(212, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(58, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(63, 212, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(212, 119, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(119, 124, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(124, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(212, 393, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(393, 84, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(84, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(90, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(393, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(124, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(122, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(393, 121, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(121, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(121, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(393, 247, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(247, 252, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(252, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(393, 710, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(710, 203, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(203, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(203, 234, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(234, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(710, 218, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(218, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(248, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(710, 227, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(227, 245, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(245, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(710, 225, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(225, 234, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(234, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(710, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(241, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(244, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(710, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(511, 489, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(489, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(489, 820, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(820, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(710, 820, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(820, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(820, 445, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(445, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(445, 395, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(395, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(395, 820, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(820, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(820, 430, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(430, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(430, 316, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(316, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(316, 820, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(820, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=820, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 14.8M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 1.384 (1.384)\tData 1.230 (1.230)\tLoss 1.5040 (1.5040)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:23<03:01,  3.10it/s]Epoch: [0][1000/1563]\tTime 0.321 (0.323)\tData 0.019 (0.021)\tLoss 1.2834 (1.1111)\tPrec@1 64.062 (73.606)\tPrec@5 64.062 (73.606)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:26<00:00,  3.09it/s]\n",
      "Test: [0/79]\tTime 1.321 (1.321)\tLoss 0.5681 (0.5681)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.480 Prec@5 89.180\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.450 (1.450)\tData 1.262 (1.262)\tLoss 0.9009 (0.9009)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:02,  3.08it/s]Epoch: [1][1000/1563]\tTime 0.311 (0.327)\tData 0.018 (0.021)\tLoss 1.1791 (1.0338)\tPrec@1 65.625 (75.229)\tPrec@5 65.625 (75.229)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:29<00:00,  3.07it/s]\n",
      "Test: [0/79]\tTime 1.433 (1.433)\tLoss 0.5342 (0.5342)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.500 Prec@5 89.880\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.480 (1.480)\tData 1.299 (1.299)\tLoss 0.8727 (0.8727)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:04,  3.06it/s]Epoch: [2][1000/1563]\tTime 0.329 (0.327)\tData 0.022 (0.021)\tLoss 0.8940 (1.0027)\tPrec@1 76.562 (75.913)\tPrec@5 76.562 (75.913)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.436 (1.436)\tLoss 0.4785 (0.4785)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.000 Prec@5 89.480\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.531 (1.531)\tData 1.341 (1.341)\tLoss 1.1987 (1.1987)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:01,  3.10it/s]Epoch: [3][1000/1563]\tTime 0.322 (0.327)\tData 0.022 (0.021)\tLoss 1.0977 (0.9653)\tPrec@1 73.438 (76.475)\tPrec@5 73.438 (76.475)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.5072 (0.5072)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.440 Prec@5 89.820\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.478 (1.478)\tData 1.289 (1.289)\tLoss 0.8093 (0.8093)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:05,  3.03it/s]Epoch: [4][1000/1563]\tTime 0.332 (0.327)\tData 0.007 (0.021)\tLoss 0.9356 (0.9515)\tPrec@1 75.000 (76.695)\tPrec@5 75.000 (76.695)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.418 (1.418)\tLoss 0.5475 (0.5475)\tPrec@1 89.062 (89.062)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.900 Prec@5 89.660\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.513 (1.513)\tData 1.312 (1.312)\tLoss 0.7326 (0.7326)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:00,  3.11it/s]Epoch: [5][1000/1563]\tTime 0.324 (0.327)\tData 0.020 (0.021)\tLoss 0.7959 (0.9114)\tPrec@1 82.812 (77.888)\tPrec@5 82.812 (77.888)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.417 (1.417)\tLoss 0.4967 (0.4967)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.060 Prec@5 89.700\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.599 (1.599)\tData 1.409 (1.409)\tLoss 0.7257 (0.7257)\tPrec@1 84.375 (84.375)\tPrec@5 84.375 (84.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:06,  3.02it/s]Epoch: [6][1000/1563]\tTime 0.334 (0.328)\tData 0.020 (0.021)\tLoss 0.6881 (0.8776)\tPrec@1 85.938 (78.880)\tPrec@5 85.938 (78.880)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.417 (1.417)\tLoss 0.5143 (0.5143)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.880 Prec@5 90.000\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.498 (1.498)\tData 1.302 (1.302)\tLoss 0.7296 (0.7296)\tPrec@1 82.812 (82.812)\tPrec@5 82.812 (82.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:06,  3.02it/s]Epoch: [7][1000/1563]\tTime 0.328 (0.326)\tData 0.024 (0.021)\tLoss 0.7222 (0.8604)\tPrec@1 76.562 (79.091)\tPrec@5 76.562 (79.091)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:29<00:00,  3.07it/s]\n",
      "Test: [0/79]\tTime 1.422 (1.422)\tLoss 0.5036 (0.5036)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.860 Prec@5 89.920\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.697 (1.697)\tData 1.516 (1.516)\tLoss 1.0967 (1.0967)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:02,  3.08it/s]Epoch: [8][1000/1563]\tTime 0.322 (0.327)\tData 0.018 (0.022)\tLoss 0.9996 (0.8563)\tPrec@1 76.562 (79.286)\tPrec@5 76.562 (79.286)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.652 (1.652)\tLoss 0.5155 (0.5155)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.980 Prec@5 90.160\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.668 (1.668)\tData 1.467 (1.467)\tLoss 1.0802 (1.0802)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:05,  3.04it/s]Epoch: [9][1000/1563]\tTime 0.337 (0.327)\tData 0.024 (0.022)\tLoss 0.7817 (0.8528)\tPrec@1 82.812 (79.544)\tPrec@5 82.812 (79.544)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.389 (1.389)\tLoss 0.5244 (0.5244)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.060 Prec@5 90.180\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [10][0/1563]\tTime 1.504 (1.504)\tData 1.323 (1.323)\tLoss 1.0371 (1.0371)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<03:03,  3.07it/s]Epoch: [10][1000/1563]\tTime 0.326 (0.327)\tData 0.020 (0.021)\tLoss 1.1786 (0.8457)\tPrec@1 68.750 (79.652)\tPrec@5 68.750 (79.652)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.464 (1.464)\tLoss 0.5024 (0.5024)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.860 Prec@5 90.100\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [11][0/1563]\tTime 1.465 (1.465)\tData 1.293 (1.293)\tLoss 0.8744 (0.8744)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:26<02:56,  3.19it/s]Epoch: [11][1000/1563]\tTime 0.326 (0.327)\tData 0.022 (0.021)\tLoss 0.9721 (0.8572)\tPrec@1 76.562 (79.460)\tPrec@5 76.562 (79.460)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:29<00:00,  3.07it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.5330 (0.5330)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.980 Prec@5 90.080\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [12][0/1563]\tTime 1.472 (1.472)\tData 1.272 (1.272)\tLoss 0.8044 (0.8044)\tPrec@1 85.938 (85.938)\tPrec@5 85.938 (85.938)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:02,  3.08it/s]Epoch: [12][1000/1563]\tTime 0.329 (0.327)\tData 0.020 (0.022)\tLoss 0.7613 (0.8607)\tPrec@1 79.688 (79.222)\tPrec@5 79.688 (79.222)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.485 (1.485)\tLoss 0.4780 (0.4780)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.900 Prec@5 90.000\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [13][0/1563]\tTime 1.476 (1.476)\tData 1.304 (1.304)\tLoss 0.9904 (0.9904)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<02:59,  3.14it/s]Epoch: [13][1000/1563]\tTime 0.332 (0.327)\tData 0.028 (0.023)\tLoss 0.7851 (0.8499)\tPrec@1 76.562 (79.449)\tPrec@5 76.562 (79.449)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.451 (1.451)\tLoss 0.4998 (0.4998)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.780 Prec@5 90.160\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-08.\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [14][0/1563]\tTime 1.485 (1.485)\tData 1.310 (1.310)\tLoss 0.6691 (0.6691)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:01,  3.11it/s]Epoch: [14][1000/1563]\tTime 0.327 (0.327)\tData 0.020 (0.022)\tLoss 0.6676 (0.8469)\tPrec@1 84.375 (79.716)\tPrec@5 84.375 (79.716)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.458 (1.458)\tLoss 0.5025 (0.5025)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.000 Prec@5 89.960\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [15][0/1563]\tTime 1.554 (1.554)\tData 1.362 (1.362)\tLoss 1.2170 (1.2170)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:01,  3.10it/s]Epoch: [15][1000/1563]\tTime 0.331 (0.328)\tData 0.033 (0.022)\tLoss 0.8948 (0.8446)\tPrec@1 73.438 (79.603)\tPrec@5 73.438 (79.603)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.489 (1.489)\tLoss 0.5121 (0.5121)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.780 Prec@5 90.180\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [16][0/1563]\tTime 1.711 (1.711)\tData 1.520 (1.520)\tLoss 0.7693 (0.7693)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<02:59,  3.13it/s]Epoch: [16][1000/1563]\tTime 0.328 (0.327)\tData 0.025 (0.022)\tLoss 1.0100 (0.8523)\tPrec@1 75.000 (79.664)\tPrec@5 75.000 (79.664)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:30<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.476 (1.476)\tLoss 0.5069 (0.5069)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 73.120 Prec@5 90.000\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [17][0/1563]\tTime 1.536 (1.536)\tData 1.346 (1.346)\tLoss 0.7885 (0.7885)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:00,  3.13it/s]Epoch: [17][1000/1563]\tTime 0.319 (0.328)\tData 0.024 (0.022)\tLoss 0.9838 (0.8442)\tPrec@1 78.125 (79.775)\tPrec@5 78.125 (79.775)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.05it/s]\n",
      "Test: [0/79]\tTime 1.493 (1.493)\tLoss 0.4915 (0.4915)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.880 Prec@5 89.940\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [18][0/1563]\tTime 1.562 (1.562)\tData 1.371 (1.371)\tLoss 0.7121 (0.7121)\tPrec@1 81.250 (81.250)\tPrec@5 81.250 (81.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:28<03:02,  3.08it/s]Epoch: [18][1000/1563]\tTime 0.326 (0.328)\tData 0.021 (0.023)\tLoss 0.6061 (0.8532)\tPrec@1 84.375 (79.558)\tPrec@5 84.375 (79.558)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.462 (1.462)\tLoss 0.5136 (0.5136)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.720 Prec@5 90.160\n",
      "resnet50tiny-round3.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [19][0/1563]\tTime 1.524 (1.524)\tData 1.350 (1.350)\tLoss 0.8606 (0.8606)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:27<03:06,  3.02it/s]Epoch: [19][1000/1563]\tTime 0.323 (0.328)\tData 0.025 (0.023)\tLoss 0.9978 (0.8512)\tPrec@1 76.562 (79.541)\tPrec@5 76.562 (79.541)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:31<00:00,  3.06it/s]\n",
      "Test: [0/79]\tTime 1.431 (1.431)\tLoss 0.5133 (0.5133)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.940 Prec@5 90.220\n",
      "resnet50tiny-round3.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 3 --epochs 20 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round3.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0200), tensor(0.0200), tensor(0.0200), tensor(0.0200), tensor(0.0200)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 41, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(41, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(44, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(50, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(48, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(200, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(56, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(61, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(200, 115, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(115, 119, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(119, 371, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(371, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(200, 371, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(371, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(371, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(78, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(86, 371, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(371, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(371, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(120, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(121, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(121, 371, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(371, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(371, 119, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(119, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 371, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(371, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(371, 242, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(242, 252, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(252, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(371, 650, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(650, 196, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(196, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(226, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(226, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(650, 203, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(203, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(203, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(238, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(650, 218, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(218, 235, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(235, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(650, 214, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(214, 222, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(222, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(650, 235, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(235, 239, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(239, 650, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(650, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(650, 497, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(497, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(497, 473, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(473, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(473, 722, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(722, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(650, 722, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(722, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(722, 423, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(423, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(423, 354, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(354, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(354, 722, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(722, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(722, 396, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(396, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(248, 722, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(722, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=722, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 13.0M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.754 (2.754)\tData 1.987 (1.987)\tLoss 1.4820 (1.4820)\tPrec@1 64.062 (64.062)\tPrec@5 64.062 (64.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:06<02:52,  3.26it/s]Epoch: [0][1000/1563]\tTime 0.299 (0.306)\tData 0.020 (0.021)\tLoss 0.9301 (1.2647)\tPrec@1 82.812 (70.425)\tPrec@5 82.812 (70.425)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:57<00:00,  3.27it/s]\n",
      "Test: [0/79]\tTime 3.104 (3.104)\tLoss 0.6124 (0.6124)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 69.200 Prec@5 87.960\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.934 (1.934)\tData 1.756 (1.756)\tLoss 1.2115 (1.2115)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<02:51,  3.28it/s]Epoch: [1][1000/1563]\tTime 0.311 (0.313)\tData 0.007 (0.028)\tLoss 1.0154 (1.1133)\tPrec@1 70.312 (73.586)\tPrec@5 70.312 (73.586)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:07<00:00,  3.21it/s]\n",
      "Test: [0/79]\tTime 1.978 (1.978)\tLoss 0.5367 (0.5367)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 70.860 Prec@5 89.460\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 2.040 (2.040)\tData 1.862 (1.862)\tLoss 0.8328 (0.8328)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:06<02:50,  3.30it/s]Epoch: [2][1000/1563]\tTime 0.300 (0.307)\tData 0.016 (0.020)\tLoss 0.8401 (1.0475)\tPrec@1 81.250 (74.863)\tPrec@5 81.250 (74.863)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:59<00:00,  3.26it/s]\n",
      "Test: [0/79]\tTime 1.940 (1.940)\tLoss 0.5213 (0.5213)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.120 Prec@5 89.260\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 2.035 (2.035)\tData 1.822 (1.822)\tLoss 0.8975 (0.8975)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:13<02:50,  3.30it/s]Epoch: [3][1000/1563]\tTime 0.299 (0.314)\tData 0.022 (0.030)\tLoss 0.9236 (1.0175)\tPrec@1 79.688 (75.503)\tPrec@5 79.688 (75.503)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:05<00:00,  3.22it/s]\n",
      "Test: [0/79]\tTime 1.807 (1.807)\tLoss 0.5031 (0.5031)\tPrec@1 85.938 (85.938)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.320 Prec@5 89.580\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 2.030 (2.030)\tData 1.855 (1.855)\tLoss 1.0023 (1.0023)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:08<02:54,  3.22it/s]Epoch: [4][1000/1563]\tTime 0.299 (0.308)\tData 0.014 (0.022)\tLoss 1.1753 (0.9755)\tPrec@1 67.188 (76.377)\tPrec@5 67.188 (76.377)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:03<00:00,  3.23it/s]\n",
      "Test: [0/79]\tTime 1.882 (1.882)\tLoss 0.4998 (0.4998)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.640 Prec@5 89.280\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.995 (1.995)\tData 1.834 (1.834)\tLoss 1.0411 (1.0411)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:06<02:51,  3.28it/s]Epoch: [5][1000/1563]\tTime 0.312 (0.307)\tData 0.018 (0.021)\tLoss 1.1907 (0.9599)\tPrec@1 64.062 (76.745)\tPrec@5 64.062 (76.745)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:57<00:00,  3.27it/s]\n",
      "Test: [0/79]\tTime 1.875 (1.875)\tLoss 0.5295 (0.5295)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 71.760 Prec@5 89.060\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 2.058 (2.058)\tData 1.870 (1.870)\tLoss 0.9092 (0.9092)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:11<02:50,  3.31it/s]Epoch: [6][1000/1563]\tTime 0.303 (0.311)\tData 0.011 (0.026)\tLoss 0.8968 (0.9202)\tPrec@1 75.000 (77.744)\tPrec@5 75.000 (77.744)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:02<00:00,  3.24it/s]\n",
      "Test: [0/79]\tTime 1.857 (1.857)\tLoss 0.5327 (0.5327)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.980 Prec@5 89.420\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 2.174 (2.174)\tData 1.994 (1.994)\tLoss 0.6192 (0.6192)\tPrec@1 89.062 (89.062)\tPrec@5 89.062 (89.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:11<02:46,  3.38it/s]Epoch: [7][1000/1563]\tTime 0.296 (0.311)\tData 0.022 (0.024)\tLoss 0.8848 (0.9059)\tPrec@1 81.250 (78.231)\tPrec@5 81.250 (78.231)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:05<00:00,  3.22it/s]\n",
      "Test: [0/79]\tTime 2.942 (2.942)\tLoss 0.5571 (0.5571)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.080 Prec@5 89.740\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.991 (1.991)\tData 1.802 (1.802)\tLoss 0.8437 (0.8437)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:16<02:49,  3.32it/s]Epoch: [8][1000/1563]\tTime 0.301 (0.317)\tData 0.020 (0.032)\tLoss 0.4446 (0.8786)\tPrec@1 93.750 (78.848)\tPrec@5 93.750 (78.848)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:13<00:00,  3.17it/s]\n",
      "Test: [0/79]\tTime 1.875 (1.875)\tLoss 0.5185 (0.5185)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.200 Prec@5 89.800\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 2.038 (2.038)\tData 1.871 (1.871)\tLoss 0.8479 (0.8479)\tPrec@1 76.562 (76.562)\tPrec@5 76.562 (76.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:07<02:47,  3.36it/s]Epoch: [9][1000/1563]\tTime 0.309 (0.307)\tData 0.009 (0.021)\tLoss 0.8264 (0.8706)\tPrec@1 79.688 (79.260)\tPrec@5 79.688 (79.260)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [07:59<00:00,  3.26it/s]\n",
      "Test: [0/79]\tTime 1.851 (1.851)\tLoss 0.5178 (0.5178)\tPrec@1 89.062 (89.062)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 72.200 Prec@5 89.860\n",
      "resnet50tiny-round4.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 4 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round3.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010), tensor(0.0010)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(35, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(44, 211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(211, 43, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(43, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(46, 211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(211, 54, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(54, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(62, 211, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(211, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(211, 113, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(113, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(113, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(120, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(211, 393, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(393, 70, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(86, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(393, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(118, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(118, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(393, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(112, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(124, 393, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(393, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(216, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(224, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(393, 710, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(710, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(144, 191, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(191, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(191, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(710, 177, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(177, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(177, 233, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(233, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(710, 207, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(207, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(207, 229, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(229, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(229, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(710, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(208, 218, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(218, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(710, 225, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(225, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(226, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(226, 710, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(710, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(710, 447, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(447, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(447, 402, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(402, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(402, 793, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(793, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(710, 793, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(793, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(793, 242, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(242, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(255, 793, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(793, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(793, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(180, 246, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(246, 793, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(793, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=793, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 10.6M\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.780 (2.780)\tData 1.976 (1.976)\tLoss 2.5193 (2.5193)\tPrec@1 40.625 (40.625)\tPrec@5 40.625 (40.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<03:04,  3.05it/s]Epoch: [0][1000/1563]\tTime 0.314 (0.313)\tData 0.019 (0.021)\tLoss 1.4021 (1.9009)\tPrec@1 70.312 (54.862)\tPrec@5 70.312 (54.862)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:07<00:00,  3.21it/s]\n",
      "Test: [0/79]\tTime 1.870 (1.870)\tLoss 0.7698 (0.7698)\tPrec@1 81.250 (81.250)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 62.840 Prec@5 83.980\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 2.000 (2.000)\tData 1.832 (1.832)\tLoss 2.0245 (2.0245)\tPrec@1 53.125 (53.125)\tPrec@5 53.125 (53.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:11<02:52,  3.26it/s]Epoch: [1][1000/1563]\tTime 0.308 (0.312)\tData 0.022 (0.021)\tLoss 1.3182 (1.4948)\tPrec@1 60.938 (63.686)\tPrec@5 60.938 (63.686)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:06<00:00,  3.21it/s]\n",
      "Test: [0/79]\tTime 1.851 (1.851)\tLoss 0.5965 (0.5965)\tPrec@1 85.938 (85.938)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 68.520 Prec@5 87.560\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 2.034 (2.034)\tData 1.850 (1.850)\tLoss 1.1458 (1.1458)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<02:54,  3.23it/s]Epoch: [2][1000/1563]\tTime 0.309 (0.313)\tData 0.020 (0.020)\tLoss 1.2211 (1.3153)\tPrec@1 73.438 (67.888)\tPrec@5 73.438 (67.888)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:07<00:00,  3.21it/s]\n",
      "Test: [0/79]\tTime 1.844 (1.844)\tLoss 0.6172 (0.6172)\tPrec@1 84.375 (84.375)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 69.180 Prec@5 87.980\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 2.033 (2.033)\tData 1.867 (1.867)\tLoss 1.2323 (1.2323)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<02:51,  3.28it/s]Epoch: [3][1000/1563]\tTime 0.305 (0.312)\tData 0.014 (0.021)\tLoss 1.3225 (1.2388)\tPrec@1 65.625 (69.596)\tPrec@5 65.625 (69.596)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:06<00:00,  3.21it/s]\n",
      "Test: [0/79]\tTime 1.923 (1.923)\tLoss 0.5778 (0.5778)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 69.680 Prec@5 88.160\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 2.177 (2.177)\tData 2.011 (2.011)\tLoss 1.0240 (1.0240)\tPrec@1 75.000 (75.000)\tPrec@5 75.000 (75.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<02:55,  3.20it/s]Epoch: [4][1000/1563]\tTime 0.311 (0.312)\tData 0.024 (0.020)\tLoss 1.1566 (1.1726)\tPrec@1 67.188 (71.116)\tPrec@5 67.188 (71.116)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:08<00:00,  3.20it/s]\n",
      "Test: [0/79]\tTime 1.901 (1.901)\tLoss 0.5600 (0.5600)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 69.980 Prec@5 88.560\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 2.094 (2.094)\tData 1.920 (1.920)\tLoss 1.0786 (1.0786)\tPrec@1 71.875 (71.875)\tPrec@5 71.875 (71.875)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:14<02:57,  3.18it/s]Epoch: [5][1000/1563]\tTime 0.314 (0.314)\tData 0.010 (0.022)\tLoss 1.5604 (1.1451)\tPrec@1 53.125 (71.983)\tPrec@5 53.125 (71.983)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:09<00:00,  3.19it/s]\n",
      "Test: [0/79]\tTime 1.992 (1.992)\tLoss 0.5475 (0.5475)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 70.120 Prec@5 89.000\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 2.219 (2.219)\tData 2.038 (2.038)\tLoss 1.0284 (1.0284)\tPrec@1 79.688 (79.688)\tPrec@5 79.688 (79.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:14<02:53,  3.24it/s]Epoch: [6][1000/1563]\tTime 0.307 (0.315)\tData 0.020 (0.022)\tLoss 1.6118 (1.0927)\tPrec@1 57.812 (73.178)\tPrec@5 57.812 (73.178)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:10<00:00,  3.19it/s]\n",
      "Test: [0/79]\tTime 1.880 (1.880)\tLoss 0.5363 (0.5363)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.220 Prec@5 89.080\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 2.020 (2.020)\tData 1.854 (1.854)\tLoss 1.4181 (1.4181)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:16<02:54,  3.22it/s]Epoch: [7][1000/1563]\tTime 0.304 (0.317)\tData 0.024 (0.024)\tLoss 1.2856 (1.0583)\tPrec@1 65.625 (74.043)\tPrec@5 65.625 (74.043)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:13<00:00,  3.17it/s]\n",
      "Test: [0/79]\tTime 1.995 (1.995)\tLoss 0.5250 (0.5250)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.200 Prec@5 89.420\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 2.036 (2.036)\tData 1.860 (1.860)\tLoss 1.0490 (1.0490)\tPrec@1 73.438 (73.438)\tPrec@5 73.438 (73.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:12<02:55,  3.22it/s]Epoch: [8][1000/1563]\tTime 0.294 (0.313)\tData 0.018 (0.020)\tLoss 0.9224 (1.0292)\tPrec@1 75.000 (74.763)\tPrec@5 75.000 (74.763)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:07<00:00,  3.20it/s]\n",
      "Test: [0/79]\tTime 1.955 (1.955)\tLoss 0.4965 (0.4965)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.740 Prec@5 89.040\n",
      "resnet50tiny-round4.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 4.039 (4.039)\tData 3.850 (3.850)\tLoss 0.9297 (0.9297)\tPrec@1 78.125 (78.125)\tPrec@5 78.125 (78.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [05:15<03:02,  3.08it/s]Epoch: [9][1000/1563]\tTime 0.300 (0.315)\tData 0.021 (0.023)\tLoss 1.2402 (1.0154)\tPrec@1 78.125 (75.306)\tPrec@5 78.125 (75.306)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [08:10<00:00,  3.19it/s]\n",
      "Test: [0/79]\tTime 1.975 (1.975)\tLoss 0.4941 (0.4941)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 71.800 Prec@5 89.220\n",
      "resnet50tiny-round4.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.1 --print-freq 1000 --prune --round 4 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round4.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 39, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(39, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(42, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(174, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(46, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(38, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(174, 47, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(47, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(58, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(174, 83, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(83, 103, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(103, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(103, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(259, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(174, 259, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(259, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(259, 75, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(75, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(87, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(259, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(259, 99, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(99, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(100, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(259, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(259, 94, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(94, 109, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(109, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(109, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(259, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(259, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(199, 198, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(198, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(259, 381, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(381, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(144, 189, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(189, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(189, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(381, 132, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(132, 174, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(174, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(381, 114, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(114, 153, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(153, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(381, 113, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(113, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(113, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (bn2): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(133, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(381, 117, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(117, 135, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(135, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(135, 381, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(381, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(381, 378, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(378, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(378, 359, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(359, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(359, 494, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(494, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(381, 494, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(494, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(494, 287, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(287, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(287, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(244, 494, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(494, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(494, 212, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(212, 167, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(167, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(167, 494, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(494, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=494, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 6.3M\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.193 (2.193)\tData 1.347 (1.347)\tLoss 4.1259 (4.1259)\tPrec@1 15.625 (15.625)\tPrec@5 15.625 (15.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:06<02:14,  4.18it/s]Epoch: [0][1000/1563]\tTime 0.238 (0.246)\tData 0.029 (0.032)\tLoss 2.9612 (3.3131)\tPrec@1 34.375 (26.803)\tPrec@5 34.375 (26.803)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:24<00:00,  4.07it/s]\n",
      "Test: [0/79]\tTime 1.407 (1.407)\tLoss 1.3336 (1.3336)\tPrec@1 67.188 (67.188)\tPrec@5 82.812 (82.812)\n",
      " * Prec@1 42.520 Prec@5 68.600\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.612 (1.612)\tData 1.447 (1.447)\tLoss 2.6254 (2.6254)\tPrec@1 35.938 (35.938)\tPrec@5 35.938 (35.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:05<02:18,  4.05it/s]Epoch: [1][1000/1563]\tTime 0.257 (0.245)\tData 0.029 (0.031)\tLoss 1.8631 (2.3769)\tPrec@1 46.875 (44.187)\tPrec@5 46.875 (44.187)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:21<00:00,  4.09it/s]\n",
      "Test: [0/79]\tTime 1.406 (1.406)\tLoss 0.8707 (0.8707)\tPrec@1 81.250 (81.250)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 56.180 Prec@5 80.520\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.590 (1.590)\tData 1.416 (1.416)\tLoss 1.9479 (1.9479)\tPrec@1 53.125 (53.125)\tPrec@5 53.125 (53.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:05<02:15,  4.15it/s]Epoch: [2][1000/1563]\tTime 0.240 (0.245)\tData 0.031 (0.031)\tLoss 2.1596 (1.9680)\tPrec@1 45.312 (52.894)\tPrec@5 45.312 (52.894)\t\n",
      " 99%|██████████████████████████████████████▍| 1540/1563 [06:17<00:05,  4.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 1000/1563 [04:06<02:13,  4.21it/s]Epoch: [8][1000/1563]\tTime 0.252 (0.246)\tData 0.031 (0.032)\tLoss 1.3186 (1.4097)\tPrec@1 70.312 (66.009)\tPrec@5 70.312 (66.009)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:23<00:00,  4.08it/s]\n",
      "Test: [0/79]\tTime 2.084 (2.084)\tLoss 0.5186 (0.5186)\tPrec@1 82.812 (82.812)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 66.000 Prec@5 86.760\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.558 (1.558)\tData 1.389 (1.389)\tLoss 1.4995 (1.4995)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:06<02:15,  4.14it/s]Epoch: [9][1000/1563]\tTime 0.242 (0.246)\tData 0.036 (0.032)\tLoss 1.3969 (1.3978)\tPrec@1 68.750 (65.989)\tPrec@5 68.750 (65.989)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:23<00:00,  4.07it/s]\n",
      "Test: [0/79]\tTime 1.410 (1.410)\tLoss 0.5352 (0.5352)\tPrec@1 82.812 (82.812)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.100 Prec@5 86.520\n",
      "resnet50tiny-round5.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.5 --print-freq 1000 --prune --round 5 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round4.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250), tensor(0.0250)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(34, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(44, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(199, 43, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(43, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(199, 45, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(45, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(56, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(199, 79, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(79, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(100, 309, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(309, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(199, 309, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(309, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(309, 65, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(65, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(85, 309, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(309, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(309, 107, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(107, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(111, 309, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(309, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(309, 93, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(93, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(108, 309, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(309, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(309, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(124, 178, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(178, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(178, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(309, 516, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(516, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(96, 155, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(155, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(155, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(516, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(140, 193, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(193, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(193, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(516, 148, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(148, 191, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(191, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(191, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(516, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(160, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(184, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(516, 182, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(182, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(182, 173, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(173, 516, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(516, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(516, 332, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(332, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(288, 587, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(587, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(516, 587, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(587, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(587, 177, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(177, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(177, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(186, 587, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(587, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(587, 139, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(139, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(156, 587, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(587, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=587, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 6.2M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.216 (2.216)\tData 1.366 (1.366)\tLoss 4.3828 (4.3828)\tPrec@1 14.062 (14.062)\tPrec@5 14.062 (14.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:16<02:26,  3.85it/s]Epoch: [0][1000/1563]\tTime 0.251 (0.256)\tData 0.027 (0.032)\tLoss 3.0187 (3.3844)\tPrec@1 21.875 (24.942)\tPrec@5 21.875 (24.942)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:40<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.353 (1.353)\tLoss 1.3736 (1.3736)\tPrec@1 71.875 (71.875)\tPrec@5 81.250 (81.250)\n",
      " * Prec@1 43.520 Prec@5 68.980\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.616 (1.616)\tData 1.456 (1.456)\tLoss 2.6705 (2.6705)\tPrec@1 35.938 (35.938)\tPrec@5 35.938 (35.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:23,  3.91it/s]Epoch: [1][1000/1563]\tTime 0.257 (0.257)\tData 0.034 (0.032)\tLoss 1.9296 (2.3690)\tPrec@1 54.688 (44.256)\tPrec@5 54.688 (44.256)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:41<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.414 (1.414)\tLoss 0.9392 (0.9392)\tPrec@1 84.375 (84.375)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 57.540 Prec@5 81.780\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.918 (1.918)\tData 1.756 (1.756)\tLoss 1.8141 (1.8141)\tPrec@1 54.688 (54.688)\tPrec@5 54.688 (54.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:27,  3.81it/s]Epoch: [2][1000/1563]\tTime 0.257 (0.257)\tData 0.017 (0.032)\tLoss 1.7606 (1.9496)\tPrec@1 60.938 (52.864)\tPrec@5 60.938 (52.864)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:40<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.388 (1.388)\tLoss 0.7745 (0.7745)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 61.440 Prec@5 83.800\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.565 (1.565)\tData 1.408 (1.408)\tLoss 1.5823 (1.5823)\tPrec@1 56.250 (56.250)\tPrec@5 56.250 (56.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:22,  3.95it/s]Epoch: [3][1000/1563]\tTime 0.224 (0.257)\tData 0.026 (0.032)\tLoss 1.9100 (1.7789)\tPrec@1 56.250 (56.648)\tPrec@5 56.250 (56.648)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:41<00:00,  3.89it/s]\n",
      "Test: [0/79]\tTime 1.434 (1.434)\tLoss 0.7031 (0.7031)\tPrec@1 84.375 (84.375)\tPrec@5 90.625 (90.625)\n",
      " * Prec@1 63.060 Prec@5 85.020\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.578 (1.578)\tData 1.423 (1.423)\tLoss 1.6101 (1.6101)\tPrec@1 62.500 (62.500)\tPrec@5 62.500 (62.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:19,  4.02it/s]Epoch: [4][1000/1563]\tTime 0.251 (0.257)\tData 0.028 (0.032)\tLoss 1.5848 (1.6714)\tPrec@1 67.188 (59.373)\tPrec@5 67.188 (59.373)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:41<00:00,  3.89it/s]\n",
      "Test: [0/79]\tTime 1.421 (1.421)\tLoss 0.6567 (0.6567)\tPrec@1 81.250 (81.250)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 64.360 Prec@5 85.280\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.617 (1.617)\tData 1.464 (1.464)\tLoss 1.4966 (1.4966)\tPrec@1 59.375 (59.375)\tPrec@5 59.375 (59.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:22,  3.95it/s]Epoch: [5][1000/1563]\tTime 0.235 (0.257)\tData 0.029 (0.032)\tLoss 1.7305 (1.6010)\tPrec@1 60.938 (60.969)\tPrec@5 60.938 (60.969)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:41<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.397 (1.397)\tLoss 0.6445 (0.6445)\tPrec@1 82.812 (82.812)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 65.440 Prec@5 85.700\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.623 (1.623)\tData 1.455 (1.455)\tLoss 1.6058 (1.6058)\tPrec@1 60.938 (60.938)\tPrec@5 60.938 (60.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:16<02:24,  3.89it/s]Epoch: [6][1000/1563]\tTime 0.257 (0.257)\tData 0.034 (0.032)\tLoss 1.2238 (1.5233)\tPrec@1 68.750 (62.756)\tPrec@5 68.750 (62.756)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:40<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.519 (1.519)\tLoss 0.5810 (0.5810)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.180 Prec@5 86.620\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 2.176 (2.176)\tData 2.013 (2.013)\tLoss 1.1940 (1.1940)\tPrec@1 70.312 (70.312)\tPrec@5 70.312 (70.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:23,  3.92it/s]Epoch: [7][1000/1563]\tTime 0.267 (0.258)\tData 0.029 (0.032)\tLoss 1.5146 (1.4568)\tPrec@1 59.375 (64.329)\tPrec@5 59.375 (64.329)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:40<00:00,  3.90it/s]\n",
      "Test: [0/79]\tTime 1.402 (1.402)\tLoss 0.5562 (0.5562)\tPrec@1 85.938 (85.938)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 66.840 Prec@5 86.920\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.625 (1.625)\tData 1.459 (1.459)\tLoss 1.5031 (1.5031)\tPrec@1 54.688 (54.688)\tPrec@5 54.688 (54.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:17<02:24,  3.90it/s]Epoch: [8][1000/1563]\tTime 0.247 (0.258)\tData 0.037 (0.032)\tLoss 1.1987 (1.4246)\tPrec@1 76.562 (65.155)\tPrec@5 76.562 (65.155)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:41<00:00,  3.89it/s]\n",
      "Test: [0/79]\tTime 1.428 (1.428)\tLoss 0.5760 (0.5760)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 67.260 Prec@5 87.280\n",
      "resnet50tiny-round5.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.597 (1.597)\tData 1.439 (1.439)\tLoss 1.7956 (1.7956)\tPrec@1 57.812 (57.812)\tPrec@5 57.812 (57.812)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [04:16<02:24,  3.91it/s]Epoch: [9][1000/1563]\tTime 0.245 (0.257)\tData 0.013 (0.032)\tLoss 1.5475 (1.3956)\tPrec@1 64.062 (65.858)\tPrec@5 64.062 (65.858)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:40<00:00,  3.91it/s]\n",
      "Test: [0/79]\tTime 1.430 (1.430)\tLoss 0.5839 (0.5839)\tPrec@1 84.375 (84.375)\tPrec@5 93.750 (93.750)\n",
      " * Prec@1 67.640 Prec@5 87.400\n",
      "resnet50tiny-round5.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.5 --print-freq 1000 --prune --round 5 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round5.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(34, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(44, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(198, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(42, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(198, 45, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(45, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(52, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(198, 79, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(79, 91, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(91, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(198, 307, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(307, 65, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(65, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(307, 77, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(77, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(77, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(111, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(307, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(92, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(98, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(307, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(108, 119, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(119, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(307, 361, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(361, 91, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(91, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(133, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(361, 103, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(103, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(103, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(156, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(361, 86, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(86, 130, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(130, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(361, 91, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(91, 137, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(137, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(137, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(361, 130, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(130, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(114, 361, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(361, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(361, 167, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(167, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(167, 130, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(130, 446, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(361, 446, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(446, 71, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(71, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(116, 446, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(446, 33, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(33, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(29, 446, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=446, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 3.1M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.111 (2.111)\tData 1.283 (1.283)\tLoss 5.2021 (5.2021)\tPrec@1 3.125 (3.125)\tPrec@5 3.125 (3.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:56<02:12,  4.25it/s]Epoch: [0][1000/1563]\tTime 0.233 (0.236)\tData 0.028 (0.035)\tLoss 3.6340 (4.0579)\tPrec@1 28.125 (13.115)\tPrec@5 28.125 (13.115)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:08<00:00,  4.24it/s]\n",
      "Test: [0/79]\tTime 1.376 (1.376)\tLoss 1.7340 (1.7340)\tPrec@1 64.062 (64.062)\tPrec@5 79.688 (79.688)\n",
      " * Prec@1 27.700 Prec@5 55.900\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 1.809 (1.809)\tData 1.644 (1.644)\tLoss 2.8776 (2.8776)\tPrec@1 31.250 (31.250)\tPrec@5 31.250 (31.250)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:57<02:14,  4.19it/s]Epoch: [1][1000/1563]\tTime 0.238 (0.237)\tData 0.027 (0.035)\tLoss 3.2179 (3.0762)\tPrec@1 28.125 (29.825)\tPrec@5 28.125 (29.825)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:10<00:00,  4.22it/s]\n",
      "Test: [0/79]\tTime 1.363 (1.363)\tLoss 1.1010 (1.1010)\tPrec@1 70.312 (70.312)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 44.060 Prec@5 71.340\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 1.495 (1.495)\tData 1.343 (1.343)\tLoss 2.8740 (2.8740)\tPrec@1 29.688 (29.688)\tPrec@5 29.688 (29.688)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:57<02:09,  4.34it/s]Epoch: [2][1000/1563]\tTime 0.246 (0.237)\tData 0.043 (0.034)\tLoss 2.4164 (2.6189)\tPrec@1 34.375 (38.597)\tPrec@5 34.375 (38.597)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:10<00:00,  4.22it/s]\n",
      "Test: [0/79]\tTime 1.403 (1.403)\tLoss 1.0025 (1.0025)\tPrec@1 75.000 (75.000)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 49.140 Prec@5 75.740\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 1.488 (1.488)\tData 1.332 (1.332)\tLoss 2.0440 (2.0440)\tPrec@1 48.438 (48.438)\tPrec@5 48.438 (48.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:57<02:10,  4.30it/s]Epoch: [3][1000/1563]\tTime 0.227 (0.237)\tData 0.025 (0.035)\tLoss 2.5024 (2.4126)\tPrec@1 43.750 (42.735)\tPrec@5 43.750 (42.735)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:08<00:00,  4.24it/s]\n",
      "Test: [0/79]\tTime 1.425 (1.425)\tLoss 0.7745 (0.7745)\tPrec@1 76.562 (76.562)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 51.840 Prec@5 78.000\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.588 (1.588)\tData 1.417 (1.417)\tLoss 2.4871 (2.4871)\tPrec@1 35.938 (35.938)\tPrec@5 35.938 (35.938)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:57<02:09,  4.35it/s]Epoch: [4][1000/1563]\tTime 0.229 (0.237)\tData 0.030 (0.033)\tLoss 2.0731 (2.2833)\tPrec@1 51.562 (45.589)\tPrec@5 51.562 (45.589)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:09<00:00,  4.23it/s]\n",
      "Test: [0/79]\tTime 1.371 (1.371)\tLoss 0.8874 (0.8874)\tPrec@1 78.125 (78.125)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 53.840 Prec@5 79.420\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 1.501 (1.501)\tData 1.338 (1.338)\tLoss 2.0197 (2.0197)\tPrec@1 51.562 (51.562)\tPrec@5 51.562 (51.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:56<02:10,  4.33it/s]Epoch: [5][1000/1563]\tTime 0.217 (0.237)\tData 0.027 (0.033)\tLoss 2.0948 (2.1919)\tPrec@1 46.875 (47.364)\tPrec@5 46.875 (47.364)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:09<00:00,  4.24it/s]\n",
      "Test: [0/79]\tTime 1.389 (1.389)\tLoss 0.7922 (0.7922)\tPrec@1 78.125 (78.125)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 55.180 Prec@5 80.420\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 1.460 (1.460)\tData 1.311 (1.311)\tLoss 2.2424 (2.2424)\tPrec@1 48.438 (48.438)\tPrec@5 48.438 (48.438)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:55<02:12,  4.26it/s]Epoch: [6][1000/1563]\tTime 0.246 (0.235)\tData 0.042 (0.033)\tLoss 2.4591 (2.1192)\tPrec@1 34.375 (49.102)\tPrec@5 34.375 (49.102)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:06<00:00,  4.27it/s]\n",
      "Test: [0/79]\tTime 1.316 (1.316)\tLoss 0.7813 (0.7813)\tPrec@1 78.125 (78.125)\tPrec@5 89.062 (89.062)\n",
      " * Prec@1 57.340 Prec@5 81.060\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 1.441 (1.441)\tData 1.282 (1.282)\tLoss 2.1074 (2.1074)\tPrec@1 45.312 (45.312)\tPrec@5 45.312 (45.312)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:55<02:08,  4.38it/s]Epoch: [7][1000/1563]\tTime 0.249 (0.235)\tData 0.036 (0.032)\tLoss 2.2964 (2.0434)\tPrec@1 46.875 (50.816)\tPrec@5 46.875 (50.816)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:06<00:00,  4.27it/s]\n",
      "Test: [0/79]\tTime 1.295 (1.295)\tLoss 0.7697 (0.7697)\tPrec@1 76.562 (76.562)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 58.280 Prec@5 81.980\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.429 (1.429)\tData 1.262 (1.262)\tLoss 2.0319 (2.0319)\tPrec@1 50.000 (50.000)\tPrec@5 50.000 (50.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:55<02:09,  4.34it/s]Epoch: [8][1000/1563]\tTime 0.223 (0.235)\tData 0.022 (0.033)\tLoss 2.1898 (1.9780)\tPrec@1 51.562 (52.405)\tPrec@5 51.562 (52.405)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:05<00:00,  4.27it/s]\n",
      "Test: [0/79]\tTime 1.308 (1.308)\tLoss 0.7762 (0.7762)\tPrec@1 79.688 (79.688)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 58.680 Prec@5 82.600\n",
      "resnet50tiny-round6.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 1.433 (1.433)\tData 1.299 (1.299)\tLoss 1.9370 (1.9370)\tPrec@1 50.000 (50.000)\tPrec@5 50.000 (50.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:54<02:10,  4.32it/s]Epoch: [9][1000/1563]\tTime 0.233 (0.235)\tData 0.029 (0.033)\tLoss 1.6332 (1.9457)\tPrec@1 51.562 (53.119)\tPrec@5 51.562 (53.119)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:06<00:00,  4.27it/s]\n",
      "Test: [0/79]\tTime 1.329 (1.329)\tLoss 0.7343 (0.7343)\tPrec@1 81.250 (81.250)\tPrec@5 92.188 (92.188)\n",
      " * Prec@1 59.180 Prec@5 82.780\n",
      "resnet50tiny-round6.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.5 --print-freq 1000 --prune --round 6 --epochs 10 --adj 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "=> loading checkpoint 'resnet50tiny-round6.pth'\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/structure_pruning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050), tensor(0.0050)]\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(34, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(44, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(198, 39, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(39, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(198, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(31, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(52, 198, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(198, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(37, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(60, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(198, 307, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(307, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(307, 77, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(77, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(77, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(111, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(307, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(42, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(97, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(307, 68, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(68, 118, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(118, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(307, 206, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(206, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(56, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(90, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(206, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(56, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(98, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(206, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(38, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(101, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(206, 45, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(45, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(49, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(206, 83, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(83, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(79, 206, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(206, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(19, 365, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(365, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(206, 365, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(365, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(365, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(30, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(37, 365, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(365, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(365, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(17, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(29, 365, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(365, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=365, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 1.5M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [0][0/1563]\tTime 2.943 (2.943)\tData 2.033 (2.033)\tLoss 5.4587 (5.4587)\tPrec@1 3.125 (3.125)\tPrec@5 3.125 (3.125)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:52<02:05,  4.49it/s]Epoch: [0][1000/1563]\tTime 0.234 (0.232)\tData 0.047 (0.035)\tLoss 3.9784 (4.1870)\tPrec@1 7.812 (10.813)\tPrec@5 7.812 (10.813)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [06:01<00:00,  4.32it/s]\n",
      "Test: [0/79]\tTime 2.397 (2.397)\tLoss 2.5610 (2.5610)\tPrec@1 50.000 (50.000)\tPrec@5 68.750 (68.750)\n",
      " * Prec@1 22.460 Prec@5 49.880\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [1][0/1563]\tTime 2.113 (2.113)\tData 1.946 (1.946)\tLoss 3.5845 (3.5845)\tPrec@1 18.750 (18.750)\tPrec@5 18.750 (18.750)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:41<02:04,  4.53it/s]Epoch: [1][1000/1563]\tTime 0.227 (0.222)\tData 0.032 (0.038)\tLoss 3.1063 (3.4198)\tPrec@1 31.250 (22.641)\tPrec@5 31.250 (22.641)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:45<00:00,  4.52it/s]\n",
      "Test: [0/79]\tTime 1.997 (1.997)\tLoss 1.9427 (1.9427)\tPrec@1 59.375 (59.375)\tPrec@5 76.562 (76.562)\n",
      " * Prec@1 33.400 Prec@5 63.760\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [2][0/1563]\tTime 2.080 (2.080)\tData 1.921 (1.921)\tLoss 3.1625 (3.1625)\tPrec@1 26.562 (26.562)\tPrec@5 26.562 (26.562)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<02:05,  4.49it/s]Epoch: [2][1000/1563]\tTime 0.197 (0.223)\tData 0.035 (0.040)\tLoss 2.6888 (3.0636)\tPrec@1 40.625 (28.969)\tPrec@5 40.625 (28.969)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:46<00:00,  4.51it/s]\n",
      "Test: [0/79]\tTime 1.880 (1.880)\tLoss 1.5298 (1.5298)\tPrec@1 65.625 (65.625)\tPrec@5 79.688 (79.688)\n",
      " * Prec@1 37.160 Prec@5 67.320\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [3][0/1563]\tTime 2.087 (2.087)\tData 1.932 (1.932)\tLoss 3.2103 (3.2103)\tPrec@1 25.000 (25.000)\tPrec@5 25.000 (25.000)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<02:01,  4.64it/s]Epoch: [3][1000/1563]\tTime 0.216 (0.222)\tData 0.034 (0.037)\tLoss 2.6520 (2.8802)\tPrec@1 40.625 (32.784)\tPrec@5 40.625 (32.784)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:45<00:00,  4.52it/s]\n",
      "Test: [0/79]\tTime 1.961 (1.961)\tLoss 1.2744 (1.2744)\tPrec@1 70.312 (70.312)\tPrec@5 84.375 (84.375)\n",
      " * Prec@1 40.280 Prec@5 69.360\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [4][0/1563]\tTime 1.882 (1.882)\tData 1.720 (1.720)\tLoss 3.1564 (3.1564)\tPrec@1 34.375 (34.375)\tPrec@5 34.375 (34.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<02:03,  4.56it/s]Epoch: [4][1000/1563]\tTime 0.228 (0.222)\tData 0.025 (0.038)\tLoss 2.6775 (2.7767)\tPrec@1 37.500 (34.668)\tPrec@5 37.500 (34.668)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:46<00:00,  4.52it/s]\n",
      "Test: [0/79]\tTime 1.947 (1.947)\tLoss 1.3799 (1.3799)\tPrec@1 65.625 (65.625)\tPrec@5 82.812 (82.812)\n",
      " * Prec@1 42.360 Prec@5 71.720\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [5][0/1563]\tTime 2.000 (2.000)\tData 1.850 (1.850)\tLoss 2.6865 (2.6865)\tPrec@1 37.500 (37.500)\tPrec@5 37.500 (37.500)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<02:03,  4.55it/s]Epoch: [5][1000/1563]\tTime 0.216 (0.223)\tData 0.031 (0.042)\tLoss 2.9802 (2.6921)\tPrec@1 28.125 (36.688)\tPrec@5 28.125 (36.688)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:46<00:00,  4.51it/s]\n",
      "Test: [0/79]\tTime 2.005 (2.005)\tLoss 1.3627 (1.3627)\tPrec@1 60.938 (60.938)\tPrec@5 85.938 (85.938)\n",
      " * Prec@1 45.060 Prec@5 73.060\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [6][0/1563]\tTime 2.087 (2.087)\tData 1.933 (1.933)\tLoss 2.7681 (2.7681)\tPrec@1 34.375 (34.375)\tPrec@5 34.375 (34.375)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:43<02:33,  3.67it/s]Epoch: [6][1000/1563]\tTime 0.237 (0.223)\tData 0.044 (0.042)\tLoss 2.6746 (2.6175)\tPrec@1 39.062 (38.301)\tPrec@5 39.062 (38.301)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:47<00:00,  4.50it/s]\n",
      "Test: [0/79]\tTime 1.846 (1.846)\tLoss 1.2116 (1.2116)\tPrec@1 70.312 (70.312)\tPrec@5 84.375 (84.375)\n",
      " * Prec@1 45.440 Prec@5 73.560\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [7][0/1563]\tTime 2.088 (2.088)\tData 1.922 (1.922)\tLoss 2.5002 (2.5002)\tPrec@1 42.188 (42.188)\tPrec@5 42.188 (42.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<01:49,  5.14it/s]Epoch: [7][1000/1563]\tTime 0.184 (0.223)\tData 0.009 (0.042)\tLoss 2.1402 (2.5530)\tPrec@1 45.312 (39.422)\tPrec@5 45.312 (39.422)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:46<00:00,  4.51it/s]\n",
      "Test: [0/79]\tTime 2.420 (2.420)\tLoss 1.1515 (1.1515)\tPrec@1 70.312 (70.312)\tPrec@5 84.375 (84.375)\n",
      " * Prec@1 47.180 Prec@5 75.380\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [8][0/1563]\tTime 1.971 (1.971)\tData 1.831 (1.831)\tLoss 2.6528 (2.6528)\tPrec@1 39.062 (39.062)\tPrec@5 39.062 (39.062)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:44<01:57,  4.78it/s]Epoch: [8][1000/1563]\tTime 0.219 (0.225)\tData 0.045 (0.046)\tLoss 2.3758 (2.4842)\tPrec@1 39.062 (40.815)\tPrec@5 39.062 (40.815)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:53<00:00,  4.42it/s]\n",
      "Test: [0/79]\tTime 2.217 (2.217)\tLoss 1.0833 (1.0833)\tPrec@1 67.188 (67.188)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 48.200 Prec@5 75.520\n",
      "resnet50tiny-round7.pth\n",
      "  0%|                                                  | 0/1563 [00:00<?, ?it/s]Epoch: [9][0/1563]\tTime 2.094 (2.094)\tData 1.930 (1.930)\tLoss 2.6418 (2.6418)\tPrec@1 42.188 (42.188)\tPrec@5 42.188 (42.188)\t\n",
      " 64%|████████████████████████▉              | 1000/1563 [03:42<02:02,  4.60it/s]Epoch: [9][1000/1563]\tTime 0.225 (0.222)\tData 0.053 (0.041)\tLoss 2.7961 (2.4644)\tPrec@1 35.938 (41.519)\tPrec@5 35.938 (41.519)\t\n",
      "100%|███████████████████████████████████████| 1563/1563 [05:45<00:00,  4.52it/s]\n",
      "Test: [0/79]\tTime 1.864 (1.864)\tLoss 1.0658 (1.0658)\tPrec@1 68.750 (68.750)\tPrec@5 87.500 (87.500)\n",
      " * Prec@1 48.440 Prec@5 75.760\n",
      "resnet50tiny-round7.pth\n"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main_tiny.py '/nfs/shared/tiny-imagenet-200/' -a resnet50 --lr 0.001 --pretrained --batch-size 64 --percent 0.5 --print-freq 1000 --prune --round 7 --epochs 10 --adj 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
