{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet50'\n",
      "=> no checkpoint found at ''\n",
      "/nfs/homedirs/rachwan/miniconda3/envs/paper/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(58, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(241, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(60, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(63, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(241, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 241, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(241, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(127, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 362, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(362, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(241, 362, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(362, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(362, 87, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(87, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(110, 362, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(362, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(362, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(112, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 362, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(362, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(362, 125, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(125, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(127, 362, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(362, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(362, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(255, 248, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(248, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(362, 460, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(460, 199, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(199, 237, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(237, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(237, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(460, 179, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(179, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(220, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(460, 197, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(197, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(197, 230, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(230, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(460, 201, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(201, 215, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(215, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(215, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(460, 227, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(227, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(240, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(460, 494, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(494, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(494, 454, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(454, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(454, 350, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(460, 350, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(350, 464, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(464, 435, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(435, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(435, 350, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(350, 436, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(436, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(436, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(320, 350, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=350, out_features=1000, bias=True)\n",
      ")\n",
      "Number of Parameters: 11.7M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [0][0/5005]\tTime 8.675 (8.675)\tData 8.278 (8.278)\tLoss 6.2075 (6.2075)\tPrec@1 2.734 (2.734)\tPrec@5 2.734 (2.734)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:26<3:03:30,  2.75s/it]Epoch: [0][1000/5005]\tTime 0.700 (2.065)\tData 0.068 (1.527)\tLoss 2.5087 (3.0792)\tPrec@1 45.703 (38.082)\tPrec@5 45.703 (38.082)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:09:32<2:21:50,  2.83s/it]Epoch: [0][2000/5005]\tTime 0.694 (2.086)\tData 0.075 (1.542)\tLoss 2.2217 (2.6821)\tPrec@1 49.219 (43.711)\tPrec@5 49.219 (43.711)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:44:46<1:34:30,  2.83s/it]Epoch: [0][3000/5005]\tTime 0.678 (2.095)\tData 0.096 (1.562)\tLoss 2.0587 (2.4986)\tPrec@1 53.516 (46.366)\tPrec@5 53.516 (46.366)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:20:32<47:27,  2.83s/it]Epoch: [0][4000/5005]\tTime 0.632 (2.108)\tData 0.076 (1.580)\tLoss 2.2820 (2.3863)\tPrec@1 47.656 (48.086)\tPrec@5 47.656 (48.086)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:56:10<00:11,  2.35s/it]Epoch: [0][5000/5005]\tTime 0.587 (2.114)\tData 0.031 (1.585)\tLoss 2.1268 (2.3076)\tPrec@1 55.469 (49.338)\tPrec@5 55.469 (49.338)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:56:15<00:00,  2.11s/it]\n",
      "Test: [0/196]\tTime 11.516 (11.516)\tLoss 0.9895 (0.9895)\tPrec@1 71.875 (71.875)\tPrec@5 93.359 (93.359)\n",
      " * Prec@1 57.416 Prec@5 82.116\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [1][0/5005]\tTime 9.071 (9.071)\tData 8.712 (8.712)\tLoss 1.8406 (1.8406)\tPrec@1 58.203 (58.203)\tPrec@5 58.203 (58.203)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:15<1:49:12,  1.64s/it]Epoch: [1][1000/5005]\tTime 2.773 (2.056)\tData 2.369 (1.526)\tLoss 1.6600 (1.9148)\tPrec@1 58.984 (55.810)\tPrec@5 58.984 (55.810)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:10:26<2:21:06,  2.82s/it]Epoch: [1][2000/5005]\tTime 0.698 (2.113)\tData 0.082 (1.584)\tLoss 1.9195 (1.9129)\tPrec@1 57.812 (55.886)\tPrec@5 57.812 (55.886)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:46:56<1:18:37,  2.35s/it]Epoch: [1][3000/5005]\tTime 0.665 (2.138)\tData 0.103 (1.603)\tLoss 1.8308 (1.9031)\tPrec@1 55.078 (56.056)\tPrec@5 55.078 (56.056)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:23:13<40:41,  2.43s/it]Epoch: [1][4000/5005]\tTime 0.660 (2.148)\tData 0.075 (1.610)\tLoss 1.7997 (1.8946)\tPrec@1 58.594 (56.243)\tPrec@5 58.594 (56.243)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:59:12<00:09,  1.84s/it]Epoch: [1][5000/5005]\tTime 0.620 (2.150)\tData 0.058 (1.613)\tLoss 1.7229 (1.8835)\tPrec@1 58.203 (56.463)\tPrec@5 58.203 (56.463)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:59:19<00:00,  2.15s/it]\n",
      "Test: [0/196]\tTime 11.722 (11.722)\tLoss 1.0384 (1.0384)\tPrec@1 70.312 (70.312)\tPrec@5 91.797 (91.797)\n",
      " * Prec@1 61.262 Prec@5 84.740\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [2][0/5005]\tTime 8.523 (8.523)\tData 8.178 (8.178)\tLoss 1.6061 (1.6061)\tPrec@1 65.234 (65.234)\tPrec@5 65.234 (65.234)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:22<2:28:13,  2.22s/it]Epoch: [2][1000/5005]\tTime 0.693 (2.061)\tData 0.092 (1.518)\tLoss 1.8040 (1.7936)\tPrec@1 58.594 (58.300)\tPrec@5 58.594 (58.300)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:09:31<1:55:11,  2.30s/it]Epoch: [2][2000/5005]\tTime 0.687 (2.085)\tData 0.075 (1.539)\tLoss 2.0730 (1.7926)\tPrec@1 52.344 (58.305)\tPrec@5 52.344 (58.305)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:45:11<1:19:07,  2.37s/it]Epoch: [2][3000/5005]\tTime 0.681 (2.103)\tData 0.077 (1.557)\tLoss 1.7257 (1.7920)\tPrec@1 60.938 (58.327)\tPrec@5 60.938 (58.327)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:21:24<39:12,  2.34s/it]Epoch: [2][4000/5005]\tTime 0.699 (2.121)\tData 0.076 (1.575)\tLoss 1.9219 (1.7886)\tPrec@1 56.641 (58.402)\tPrec@5 56.641 (58.402)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:57:55<00:09,  1.94s/it]Epoch: [2][5000/5005]\tTime 0.637 (2.135)\tData 0.068 (1.588)\tLoss 1.6124 (1.7843)\tPrec@1 60.547 (58.481)\tPrec@5 60.547 (58.481)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:58:03<00:00,  2.13s/it]\n",
      "Test: [0/196]\tTime 11.139 (11.139)\tLoss 0.9022 (0.9022)\tPrec@1 77.734 (77.734)\tPrec@5 94.141 (94.141)\n",
      " * Prec@1 62.722 Prec@5 85.484\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [3][0/5005]\tTime 9.336 (9.336)\tData 8.965 (8.965)\tLoss 1.6151 (1.6151)\tPrec@1 59.766 (59.766)\tPrec@5 59.766 (59.766)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:22<1:34:20,  1.41s/it]Epoch: [3][1000/5005]\tTime 7.707 (2.068)\tData 7.411 (1.515)\tLoss 1.7057 (1.7192)\tPrec@1 63.281 (59.846)\tPrec@5 63.281 (59.846)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:09:58<1:17:16,  1.54s/it]Epoch: [3][2000/5005]\tTime 6.684 (2.102)\tData 6.255 (1.551)\tLoss 1.9085 (1.7236)\tPrec@1 56.641 (59.763)\tPrec@5 56.641 (59.763)\t\n",
      " 60%|██████████████████████▏              | 3000/5005 [1:46:22<48:02,  1.44s/it]Epoch: [3][3000/5005]\tTime 7.295 (2.129)\tData 6.913 (1.581)\tLoss 1.6335 (1.7218)\tPrec@1 61.719 (59.809)\tPrec@5 61.719 (59.809)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:22:54<24:44,  1.48s/it]Epoch: [3][4000/5005]\tTime 6.507 (2.145)\tData 6.186 (1.597)\tLoss 1.7158 (1.7209)\tPrec@1 59.766 (59.832)\tPrec@5 59.766 (59.832)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:59:43<00:07,  1.56s/it]Epoch: [3][5000/5005]\tTime 4.366 (2.157)\tData 4.073 (1.610)\tLoss 1.9602 (1.7194)\tPrec@1 57.422 (59.867)\tPrec@5 57.422 (59.867)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:59:51<00:00,  2.16s/it]\n",
      "Test: [0/196]\tTime 11.087 (11.087)\tLoss 0.9222 (0.9222)\tPrec@1 75.000 (75.000)\tPrec@5 94.531 (94.531)\n",
      " * Prec@1 61.746 Prec@5 84.556\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [4][0/5005]\tTime 8.861 (8.861)\tData 8.507 (8.507)\tLoss 1.4921 (1.4921)\tPrec@1 67.188 (67.188)\tPrec@5 67.188 (67.188)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:13<3:10:43,  2.86s/it]Epoch: [4][1000/5005]\tTime 0.698 (2.052)\tData 0.108 (1.516)\tLoss 1.8503 (1.6778)\tPrec@1 57.812 (60.723)\tPrec@5 57.812 (60.723)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:09:33<2:33:17,  3.06s/it]Epoch: [4][2000/5005]\tTime 0.693 (2.086)\tData 0.116 (1.545)\tLoss 1.5674 (1.6806)\tPrec@1 62.891 (60.648)\tPrec@5 62.891 (60.648)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:45:45<1:42:09,  3.06s/it]Epoch: [4][3000/5005]\tTime 0.710 (2.115)\tData 0.073 (1.572)\tLoss 1.7603 (1.6819)\tPrec@1 57.422 (60.635)\tPrec@5 57.422 (60.635)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:22:18<52:35,  3.14s/it]Epoch: [4][4000/5005]\tTime 0.713 (2.134)\tData 0.089 (1.591)\tLoss 1.7259 (1.6807)\tPrec@1 63.281 (60.643)\tPrec@5 63.281 (60.643)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:59:16<00:13,  2.80s/it]Epoch: [4][5000/5005]\tTime 0.583 (2.151)\tData 0.042 (1.608)\tLoss 1.5793 (1.6801)\tPrec@1 61.719 (60.647)\tPrec@5 61.719 (60.647)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:59:24<00:00,  2.15s/it]\n",
      "Test: [0/196]\tTime 11.199 (11.199)\tLoss 0.7840 (0.7840)\tPrec@1 78.906 (78.906)\tPrec@5 96.094 (96.094)\n",
      " * Prec@1 64.542 Prec@5 86.738\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [5][0/5005]\tTime 9.715 (9.715)\tData 9.329 (9.329)\tLoss 1.6213 (1.6213)\tPrec@1 60.547 (60.547)\tPrec@5 60.547 (60.547)\t\n",
      " 20%|███████▍                             | 1000/5005 [35:00<3:06:06,  2.79s/it]Epoch: [5][1000/5005]\tTime 0.669 (2.099)\tData 0.084 (1.559)\tLoss 1.5226 (1.6312)\tPrec@1 59.375 (61.655)\tPrec@5 59.375 (61.655)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:10:11<2:31:33,  3.03s/it]Epoch: [5][2000/5005]\tTime 0.698 (2.105)\tData 0.115 (1.563)\tLoss 1.7961 (1.6383)\tPrec@1 57.031 (61.557)\tPrec@5 57.031 (61.557)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:47:13<1:51:36,  3.34s/it]Epoch: [5][3000/5005]\tTime 0.662 (2.144)\tData 0.064 (1.609)\tLoss 1.4829 (1.6385)\tPrec@1 64.062 (61.503)\tPrec@5 64.062 (61.503)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████▉       | 4000/5005 [2:32:52<1:01:07,  3.65s/it]Epoch: [5][4000/5005]\tTime 0.592 (2.293)\tData 0.129 (1.759)\tLoss 1.9300 (1.6428)\tPrec@1 53.906 (61.427)\tPrec@5 53.906 (61.427)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [3:21:43<00:16,  3.40s/it]Epoch: [5][5000/5005]\tTime 0.581 (2.420)\tData 0.062 (1.889)\tLoss 1.6509 (1.6449)\tPrec@1 59.766 (61.392)\tPrec@5 59.766 (61.392)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [3:21:53<00:00,  2.42s/it]\n",
      "Test: [0/196]\tTime 12.558 (12.558)\tLoss 0.9070 (0.9070)\tPrec@1 73.047 (73.047)\tPrec@5 94.531 (94.531)\n",
      " * Prec@1 64.604 Prec@5 86.674\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [6][0/5005]\tTime 12.229 (12.229)\tData 11.918 (11.918)\tLoss 1.5235 (1.5235)\tPrec@1 62.891 (62.891)\tPrec@5 62.891 (62.891)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:33<3:21:46,  3.02s/it]Epoch: [6][1000/5005]\tTime 0.710 (2.073)\tData 0.055 (1.532)\tLoss 1.4126 (1.6056)\tPrec@1 67.578 (62.204)\tPrec@5 67.578 (62.204)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:09:11<1:44:07,  2.08s/it]Epoch: [6][2000/5005]\tTime 0.711 (2.075)\tData 0.103 (1.532)\tLoss 1.5318 (1.6101)\tPrec@1 58.594 (62.067)\tPrec@5 58.594 (62.067)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:45:17<1:44:11,  3.12s/it]Epoch: [6][3000/5005]\tTime 0.685 (2.105)\tData 0.050 (1.568)\tLoss 1.6404 (1.6144)\tPrec@1 60.547 (61.970)\tPrec@5 60.547 (61.970)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:21:20<50:35,  3.02s/it]Epoch: [6][4000/5005]\tTime 0.680 (2.120)\tData 0.103 (1.580)\tLoss 1.6615 (1.6172)\tPrec@1 59.375 (61.908)\tPrec@5 59.375 (61.908)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:56:23<00:11,  2.21s/it]Epoch: [6][5000/5005]\tTime 0.588 (2.116)\tData 0.049 (1.575)\tLoss 1.5892 (1.6192)\tPrec@1 64.844 (61.887)\tPrec@5 64.844 (61.887)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:56:29<00:00,  2.12s/it]\n",
      "Test: [0/196]\tTime 11.842 (11.842)\tLoss 0.8447 (0.8447)\tPrec@1 75.000 (75.000)\tPrec@5 95.312 (95.312)\n",
      " * Prec@1 65.926 Prec@5 87.482\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [7][0/5005]\tTime 9.342 (9.342)\tData 8.941 (8.941)\tLoss 1.5556 (1.5556)\tPrec@1 64.453 (64.453)\tPrec@5 64.453 (64.453)\t\n",
      " 20%|███████▍                             | 1000/5005 [33:56<1:50:53,  1.66s/it]Epoch: [7][1000/5005]\tTime 0.740 (2.035)\tData 0.099 (1.493)\tLoss 1.5163 (1.5879)\tPrec@1 62.500 (62.597)\tPrec@5 62.500 (62.597)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:47<1:26:04,  1.72s/it]Epoch: [7][2000/5005]\tTime 0.701 (2.063)\tData 0.099 (1.517)\tLoss 1.6203 (1.5945)\tPrec@1 62.500 (62.451)\tPrec@5 62.500 (62.451)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:44:02<1:01:43,  1.85s/it]Epoch: [7][3000/5005]\tTime 0.698 (2.080)\tData 0.113 (1.533)\tLoss 1.7330 (1.5959)\tPrec@1 58.984 (62.454)\tPrec@5 58.984 (62.454)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:19:41<30:42,  1.83s/it]Epoch: [7][4000/5005]\tTime 0.680 (2.095)\tData 0.095 (1.548)\tLoss 1.4874 (1.5979)\tPrec@1 61.328 (62.416)\tPrec@5 61.328 (62.416)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:55:10<00:08,  1.67s/it]Epoch: [7][5000/5005]\tTime 0.648 (2.102)\tData 0.089 (1.554)\tLoss 1.8608 (1.6008)\tPrec@1 58.594 (62.358)\tPrec@5 58.594 (62.358)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:55:16<00:00,  2.10s/it]\n",
      "Test: [0/196]\tTime 11.231 (11.231)\tLoss 0.8502 (0.8502)\tPrec@1 75.781 (75.781)\tPrec@5 94.531 (94.531)\n",
      " * Prec@1 65.430 Prec@5 87.306\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [8][0/5005]\tTime 8.533 (8.533)\tData 8.181 (8.181)\tLoss 1.3030 (1.3030)\tPrec@1 70.703 (70.703)\tPrec@5 70.703 (70.703)\t\n",
      " 20%|███████▍                             | 1000/5005 [34:00<2:29:12,  2.24s/it]Epoch: [8][1000/5005]\tTime 0.711 (2.039)\tData 0.083 (1.513)\tLoss 1.8348 (1.5715)\tPrec@1 60.547 (62.798)\tPrec@5 60.547 (62.798)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:47<1:53:05,  2.26s/it]Epoch: [8][2000/5005]\tTime 0.726 (2.063)\tData 0.127 (1.528)\tLoss 1.4780 (1.5726)\tPrec@1 67.969 (62.865)\tPrec@5 67.969 (62.865)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:43:57<1:00:50,  1.82s/it]Epoch: [8][3000/5005]\tTime 0.695 (2.079)\tData 0.109 (1.545)\tLoss 1.4892 (1.5771)\tPrec@1 66.406 (62.806)\tPrec@5 66.406 (62.806)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:19:22<50:30,  3.02s/it]Epoch: [8][4000/5005]\tTime 0.671 (2.090)\tData 0.090 (1.558)\tLoss 1.5213 (1.5797)\tPrec@1 63.672 (62.739)\tPrec@5 63.672 (62.739)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:54:37<00:09,  1.99s/it]Epoch: [8][5000/5005]\tTime 0.595 (2.095)\tData 0.049 (1.561)\tLoss 1.6656 (1.5829)\tPrec@1 64.453 (62.657)\tPrec@5 64.453 (62.657)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:54:43<00:00,  2.09s/it]\n",
      "Test: [0/196]\tTime 11.759 (11.759)\tLoss 0.8301 (0.8301)\tPrec@1 78.516 (78.516)\tPrec@5 94.922 (94.922)\n",
      " * Prec@1 66.524 Prec@5 87.752\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [9][0/5005]\tTime 10.076 (10.076)\tData 9.709 (9.709)\tLoss 1.4851 (1.4851)\tPrec@1 62.891 (62.891)\tPrec@5 62.891 (62.891)\t\n",
      " 20%|███████▍                             | 1000/5005 [33:57<3:26:38,  3.10s/it]Epoch: [9][1000/5005]\tTime 0.720 (2.036)\tData 0.094 (1.502)\tLoss 1.5867 (1.5531)\tPrec@1 62.891 (63.180)\tPrec@5 62.891 (63.180)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:44<2:26:24,  2.92s/it]Epoch: [9][2000/5005]\tTime 0.723 (2.061)\tData 0.114 (1.518)\tLoss 1.6521 (1.5610)\tPrec@1 62.891 (63.034)\tPrec@5 62.891 (63.034)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [1:44:08<1:34:12,  2.82s/it]Epoch: [9][3000/5005]\tTime 0.705 (2.082)\tData 0.072 (1.538)\tLoss 1.6031 (1.5660)\tPrec@1 62.891 (63.012)\tPrec@5 62.891 (63.012)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:19:26<47:42,  2.85s/it]Epoch: [9][4000/5005]\tTime 0.748 (2.091)\tData 0.114 (1.546)\tLoss 1.6222 (1.5698)\tPrec@1 58.984 (62.950)\tPrec@5 58.984 (62.950)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:54:34<00:12,  2.41s/it]Epoch: [9][5000/5005]\tTime 0.602 (2.095)\tData 0.042 (1.550)\tLoss 1.7934 (1.5711)\tPrec@1 58.984 (62.951)\tPrec@5 58.984 (62.951)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:54:39<00:00,  2.09s/it]\n",
      "Test: [0/196]\tTime 11.433 (11.433)\tLoss 0.7871 (0.7871)\tPrec@1 80.078 (80.078)\tPrec@5 96.094 (96.094)\n",
      " * Prec@1 66.166 Prec@5 87.512\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [10][0/5005]\tTime 8.884 (8.884)\tData 8.543 (8.543)\tLoss 1.3621 (1.3621)\tPrec@1 67.969 (67.969)\tPrec@5 67.969 (67.969)\t\n",
      " 20%|███████▍                             | 1000/5005 [33:48<1:48:21,  1.62s/it]Epoch: [10][1000/5005]\tTime 2.825 (2.030)\tData 2.411 (1.506)\tLoss 1.4506 (1.5476)\tPrec@1 66.016 (63.370)\tPrec@5 66.016 (63.370)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:31<1:15:16,  1.50s/it]Epoch: [10][2000/5005]\tTime 6.939 (2.058)\tData 6.538 (1.530)\tLoss 1.5427 (1.5508)\tPrec@1 65.625 (63.354)\tPrec@5 65.625 (63.354)\t\n",
      " 60%|██████████████████████▏              | 3000/5005 [1:43:43<51:16,  1.53s/it]Epoch: [10][3000/5005]\tTime 3.889 (2.075)\tData 3.480 (1.542)\tLoss 1.6716 (1.5544)\tPrec@1 61.328 (63.305)\tPrec@5 61.328 (63.305)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:19:09<24:50,  1.48s/it]Epoch: [10][4000/5005]\tTime 6.880 (2.089)\tData 6.487 (1.553)\tLoss 1.6760 (1.5577)\tPrec@1 57.031 (63.257)\tPrec@5 57.031 (63.257)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:55:00<00:07,  1.49s/it]Epoch: [10][5000/5005]\tTime 4.064 (2.101)\tData 3.764 (1.564)\tLoss 1.5063 (1.5604)\tPrec@1 62.109 (63.221)\tPrec@5 62.109 (63.221)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:55:07<00:00,  2.10s/it]\n",
      "Test: [0/196]\tTime 11.088 (11.088)\tLoss 0.7721 (0.7721)\tPrec@1 78.516 (78.516)\tPrec@5 95.703 (95.703)\n",
      " * Prec@1 66.696 Prec@5 87.902\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [11][0/5005]\tTime 9.192 (9.192)\tData 8.836 (8.836)\tLoss 1.5020 (1.5020)\tPrec@1 65.625 (65.625)\tPrec@5 65.625 (65.625)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|███████▍                             | 1000/5005 [33:57<1:33:15,  1.40s/it]Epoch: [11][1000/5005]\tTime 6.125 (2.041)\tData 5.739 (1.500)\tLoss 1.5867 (1.5298)\tPrec@1 62.109 (63.831)\tPrec@5 62.109 (63.831)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:52<1:15:00,  1.50s/it]Epoch: [11][2000/5005]\tTime 6.326 (2.068)\tData 5.936 (1.532)\tLoss 1.4680 (1.5368)\tPrec@1 62.109 (63.661)\tPrec@5 62.109 (63.661)\t\n",
      " 60%|██████████████████████▏              | 3000/5005 [1:44:35<48:13,  1.44s/it]Epoch: [11][3000/5005]\tTime 6.130 (2.093)\tData 5.781 (1.553)\tLoss 1.5988 (1.5412)\tPrec@1 62.891 (63.564)\tPrec@5 62.891 (63.564)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:20:36<25:40,  1.53s/it]Epoch: [11][4000/5005]\tTime 6.341 (2.110)\tData 5.961 (1.569)\tLoss 1.7094 (1.5445)\tPrec@1 60.156 (63.526)\tPrec@5 60.156 (63.526)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [2:56:52<00:07,  1.56s/it]Epoch: [11][5000/5005]\tTime 4.592 (2.123)\tData 4.288 (1.582)\tLoss 1.4756 (1.5484)\tPrec@1 61.719 (63.443)\tPrec@5 61.719 (63.443)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [2:57:00<00:00,  2.12s/it]\n",
      "Test: [0/196]\tTime 11.376 (11.376)\tLoss 0.8207 (0.8207)\tPrec@1 77.734 (77.734)\tPrec@5 96.094 (96.094)\n",
      " * Prec@1 66.696 Prec@5 87.904\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [12][0/5005]\tTime 8.593 (8.593)\tData 8.251 (8.251)\tLoss 1.5097 (1.5097)\tPrec@1 62.109 (62.109)\tPrec@5 62.109 (62.109)\t\n",
      " 20%|███████▍                             | 1000/5005 [33:46<2:20:11,  2.10s/it]Epoch: [12][1000/5005]\tTime 0.725 (2.025)\tData 0.167 (1.480)\tLoss 1.3965 (1.5275)\tPrec@1 68.750 (63.991)\tPrec@5 68.750 (63.991)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:08:19<1:34:46,  1.89s/it]Epoch: [12][2000/5005]\tTime 3.242 (2.051)\tData 2.855 (1.509)\tLoss 1.4245 (1.5324)\tPrec@1 63.281 (63.850)\tPrec@5 63.281 (63.850)\t\n",
      " 60%|██████████████████████▏              | 3000/5005 [1:44:42<58:07,  1.74s/it]Epoch: [12][3000/5005]\tTime 7.446 (2.096)\tData 7.077 (1.563)\tLoss 1.3360 (1.5349)\tPrec@1 66.406 (63.799)\tPrec@5 66.406 (63.799)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:23:51<26:35,  1.59s/it]Epoch: [12][4000/5005]\tTime 5.426 (2.159)\tData 5.016 (1.630)\tLoss 1.7654 (1.5371)\tPrec@1 59.375 (63.751)\tPrec@5 59.375 (63.751)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [3:00:48<00:12,  2.42s/it]Epoch: [12][5000/5005]\tTime 0.627 (2.169)\tData 0.062 (1.643)\tLoss 1.2706 (1.5400)\tPrec@1 65.625 (63.697)\tPrec@5 65.625 (63.697)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [3:00:55<00:00,  2.17s/it]\n",
      "Test: [0/196]\tTime 12.464 (12.464)\tLoss 0.6674 (0.6674)\tPrec@1 79.688 (79.688)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 66.570 Prec@5 87.930\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [13][0/5005]\tTime 8.962 (8.962)\tData 8.630 (8.630)\tLoss 1.5850 (1.5850)\tPrec@1 62.891 (62.891)\tPrec@5 62.891 (62.891)\t\n",
      " 20%|███████▍                             | 1000/5005 [36:11<2:13:52,  2.01s/it]Epoch: [13][1000/5005]\tTime 0.695 (2.170)\tData 0.055 (1.644)\tLoss 1.3899 (1.5076)\tPrec@1 68.750 (64.355)\tPrec@5 68.750 (64.355)\t\n",
      " 40%|█████████████▉                     | 2000/5005 [1:16:55<1:42:11,  2.04s/it]Epoch: [13][2000/5005]\tTime 7.116 (2.310)\tData 6.740 (1.791)\tLoss 1.7395 (1.5147)\tPrec@1 59.766 (64.160)\tPrec@5 59.766 (64.160)\t\n",
      " 60%|████████████████████▉              | 3000/5005 [2:07:32<1:27:30,  2.62s/it]Epoch: [13][3000/5005]\tTime 0.699 (2.550)\tData 0.117 (2.040)\tLoss 1.4889 (1.5224)\tPrec@1 58.984 (63.963)\tPrec@5 58.984 (63.963)\t\n",
      " 80%|█████████████████████████████▌       | 4000/5005 [2:51:12<40:10,  2.40s/it]Epoch: [13][4000/5005]\tTime 0.707 (2.568)\tData 0.086 (2.056)\tLoss 1.5175 (1.5269)\tPrec@1 64.453 (63.861)\tPrec@5 64.453 (63.861)\t\n",
      "100%|████████████████████████████████████▉| 5000/5005 [3:27:13<00:08,  1.70s/it]Epoch: [13][5000/5005]\tTime 2.714 (2.487)\tData 2.377 (1.975)\tLoss 1.4720 (1.5298)\tPrec@1 66.797 (63.814)\tPrec@5 66.797 (63.814)\t\n",
      "100%|█████████████████████████████████████| 5005/5005 [3:27:20<00:00,  2.49s/it]\n",
      "Test: [0/196]\tTime 11.488 (11.488)\tLoss 0.8028 (0.8028)\tPrec@1 78.125 (78.125)\tPrec@5 96.094 (96.094)\n",
      " * Prec@1 66.536 Prec@5 87.866\n",
      "  0%|                                                  | 0/5005 [00:00<?, ?it/s]Epoch: [14][0/5005]\tTime 8.322 (8.322)\tData 7.985 (7.985)\tLoss 1.6300 (1.6300)\tPrec@1 63.281 (63.281)\tPrec@5 63.281 (63.281)\t\n",
      " 19%|███████                               | 935/5005 [32:19<1:59:19,  1.76s/it]"
     ]
    }
   ],
   "source": [
    "!python /nfs/homedirs/rachwan/Torch-Pruning/examples/imagenet/ImageNet/main.py '/nfs/shared/imagenet2012/' -a resnet50 --lr 0.01 --pretrained --batch-size 256 --percent 0.3 --print-freq 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
