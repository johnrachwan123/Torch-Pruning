{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2e43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/rachwan/Torch-Pruning/benchmarks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f1e55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mmode: prune\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mmodel: resnet50\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mverbose: False\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mdataset: cifar100\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mbatch_size: 128\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mtotal_epochs: 100\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mlr_decay_milestones: 60,80\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mlr_decay_gamma: 0.1\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mlr: 0.01\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mrestore: True\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0moutput_dir: run/cifar100/prune/cifar100-global-random-resnet50\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mmethod: random\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mspeed_up: 100.0\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mmax_sparsity: 1.0\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msoft_keeping_ratio: 0.0\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mreg: 0.0005\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mweight_decay: 0.0005\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mseed: None\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mglobal_pruning: True\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msl_total_epochs: 100\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msl_lr: 0.01\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msl_lr_decay_milestones: 60,80\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msl_reg_warmup: 0\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0msl_restore: False\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0miterative_steps: 400\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mlogger: <Logger cifar100-global-random-resnet50 (DEBUG)>\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mdevice: cuda\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mnum_classes: 100\n",
      "\u001b[32m[03/29 18:00:06 cifar100-global-random-resnet50]: \u001b[0mLoading model from True\n",
      "\u001b[32m[03/29 18:00:21 cifar100-global-random-resnet50]: \u001b[0mPruning...\n",
      "\u001b[32m[03/29 18:00:38 cifar100-global-random-resnet50]: \u001b[0mResNet(\n",
      "  (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(9, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(9, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(23, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(4, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(23, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(10, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(23, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 5, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(5, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(23, 46, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(46, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(13, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(46, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(11, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(46, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(14, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(46, 29, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(29, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(19, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(46, 88, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(88, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(30, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(35, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(24, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(19, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(24, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(23, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(88, 29, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(29, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(35, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(88, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(27, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(14, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(88, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(55, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(47, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(88, 248, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(248, 54, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(54, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(50, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(248, 47, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(47, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=248, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/29 18:00:39 cifar100-global-random-resnet50]: \u001b[0mParams: 23.71 M => 0.26 M (1.10%)\n",
      "\u001b[32m[03/29 18:00:39 cifar100-global-random-resnet50]: \u001b[0mFLOPs: 1304.88 M => 12.79 M (0.98%, 102.05X )\n",
      "\u001b[32m[03/29 18:00:39 cifar100-global-random-resnet50]: \u001b[0mAcc: 0.7891 => 0.0100\n",
      "\u001b[32m[03/29 18:00:39 cifar100-global-random-resnet50]: \u001b[0mVal Loss: 0.9788 => 4.6106\n",
      "\u001b[32m[03/29 18:00:39 cifar100-global-random-resnet50]: \u001b[0mFinetuning...\n",
      "\u001b[32m[03/29 18:01:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 0/100, Acc=0.0429, Val Loss=4.2418, lr=0.0100\n",
      "\u001b[32m[03/29 18:01:20 cifar100-global-random-resnet50]: \u001b[0mEpoch 1/100, Acc=0.0610, Val Loss=4.0039, lr=0.0100\n",
      "\u001b[32m[03/29 18:01:40 cifar100-global-random-resnet50]: \u001b[0mEpoch 2/100, Acc=0.1137, Val Loss=3.6611, lr=0.0100\n",
      "\u001b[32m[03/29 18:02:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 3/100, Acc=0.1481, Val Loss=3.4707, lr=0.0100\n",
      "\u001b[32m[03/29 18:02:20 cifar100-global-random-resnet50]: \u001b[0mEpoch 4/100, Acc=0.1920, Val Loss=3.2578, lr=0.0100\n",
      "\u001b[32m[03/29 18:02:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 5/100, Acc=0.2156, Val Loss=3.1574, lr=0.0100\n",
      "\u001b[32m[03/29 18:03:01 cifar100-global-random-resnet50]: \u001b[0mEpoch 6/100, Acc=0.2295, Val Loss=3.1374, lr=0.0100\n",
      "\u001b[32m[03/29 18:03:21 cifar100-global-random-resnet50]: \u001b[0mEpoch 7/100, Acc=0.2829, Val Loss=2.8150, lr=0.0100\n",
      "\u001b[32m[03/29 18:03:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 8/100, Acc=0.2793, Val Loss=2.8386, lr=0.0100\n",
      "\u001b[32m[03/29 18:04:02 cifar100-global-random-resnet50]: \u001b[0mEpoch 9/100, Acc=0.3345, Val Loss=2.5228, lr=0.0100\n",
      "\u001b[32m[03/29 18:04:22 cifar100-global-random-resnet50]: \u001b[0mEpoch 10/100, Acc=0.3525, Val Loss=2.4553, lr=0.0100\n",
      "\u001b[32m[03/29 18:04:42 cifar100-global-random-resnet50]: \u001b[0mEpoch 11/100, Acc=0.3341, Val Loss=2.5634, lr=0.0100\n",
      "\u001b[32m[03/29 18:05:02 cifar100-global-random-resnet50]: \u001b[0mEpoch 12/100, Acc=0.3640, Val Loss=2.4234, lr=0.0100\n",
      "\u001b[32m[03/29 18:05:22 cifar100-global-random-resnet50]: \u001b[0mEpoch 13/100, Acc=0.3756, Val Loss=2.3409, lr=0.0100\n",
      "\u001b[32m[03/29 18:05:43 cifar100-global-random-resnet50]: \u001b[0mEpoch 14/100, Acc=0.3823, Val Loss=2.3584, lr=0.0100\n",
      "\u001b[32m[03/29 18:06:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 15/100, Acc=0.4262, Val Loss=2.1604, lr=0.0100\n",
      "\u001b[32m[03/29 18:06:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 16/100, Acc=0.4217, Val Loss=2.1355, lr=0.0100\n",
      "\u001b[32m[03/29 18:06:43 cifar100-global-random-resnet50]: \u001b[0mEpoch 17/100, Acc=0.4196, Val Loss=2.1673, lr=0.0100\n",
      "\u001b[32m[03/29 18:07:04 cifar100-global-random-resnet50]: \u001b[0mEpoch 18/100, Acc=0.4430, Val Loss=2.0475, lr=0.0100\n",
      "\u001b[32m[03/29 18:07:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 19/100, Acc=0.4261, Val Loss=2.1495, lr=0.0100\n",
      "\u001b[32m[03/29 18:07:44 cifar100-global-random-resnet50]: \u001b[0mEpoch 20/100, Acc=0.4556, Val Loss=2.0189, lr=0.0100\n",
      "\u001b[32m[03/29 18:08:04 cifar100-global-random-resnet50]: \u001b[0mEpoch 21/100, Acc=0.4481, Val Loss=2.0706, lr=0.0100\n",
      "\u001b[32m[03/29 18:08:24 cifar100-global-random-resnet50]: \u001b[0mEpoch 22/100, Acc=0.4496, Val Loss=2.0209, lr=0.0100\n",
      "\u001b[32m[03/29 18:08:44 cifar100-global-random-resnet50]: \u001b[0mEpoch 23/100, Acc=0.4410, Val Loss=2.0975, lr=0.0100\n",
      "\u001b[32m[03/29 18:09:04 cifar100-global-random-resnet50]: \u001b[0mEpoch 24/100, Acc=0.4630, Val Loss=1.9783, lr=0.0100\n",
      "\u001b[32m[03/29 18:09:25 cifar100-global-random-resnet50]: \u001b[0mEpoch 25/100, Acc=0.4497, Val Loss=2.0223, lr=0.0100\n",
      "\u001b[32m[03/29 18:09:45 cifar100-global-random-resnet50]: \u001b[0mEpoch 26/100, Acc=0.4962, Val Loss=1.8577, lr=0.0100\n",
      "\u001b[32m[03/29 18:10:05 cifar100-global-random-resnet50]: \u001b[0mEpoch 27/100, Acc=0.4756, Val Loss=1.9674, lr=0.0100\n",
      "\u001b[32m[03/29 18:10:26 cifar100-global-random-resnet50]: \u001b[0mEpoch 28/100, Acc=0.4802, Val Loss=1.9169, lr=0.0100\n",
      "\u001b[32m[03/29 18:10:46 cifar100-global-random-resnet50]: \u001b[0mEpoch 29/100, Acc=0.4857, Val Loss=1.9112, lr=0.0100\n",
      "\u001b[32m[03/29 18:11:06 cifar100-global-random-resnet50]: \u001b[0mEpoch 30/100, Acc=0.4802, Val Loss=1.9296, lr=0.0100\n",
      "\u001b[32m[03/29 18:11:27 cifar100-global-random-resnet50]: \u001b[0mEpoch 31/100, Acc=0.4831, Val Loss=1.8715, lr=0.0100\n",
      "\u001b[32m[03/29 18:11:47 cifar100-global-random-resnet50]: \u001b[0mEpoch 32/100, Acc=0.5026, Val Loss=1.8214, lr=0.0100\n",
      "\u001b[32m[03/29 18:12:07 cifar100-global-random-resnet50]: \u001b[0mEpoch 33/100, Acc=0.4704, Val Loss=2.0040, lr=0.0100\n",
      "\u001b[32m[03/29 18:12:28 cifar100-global-random-resnet50]: \u001b[0mEpoch 34/100, Acc=0.5111, Val Loss=1.8159, lr=0.0100\n",
      "\u001b[32m[03/29 18:12:48 cifar100-global-random-resnet50]: \u001b[0mEpoch 35/100, Acc=0.5013, Val Loss=1.8624, lr=0.0100\n",
      "\u001b[32m[03/29 18:13:08 cifar100-global-random-resnet50]: \u001b[0mEpoch 36/100, Acc=0.5160, Val Loss=1.8017, lr=0.0100\n",
      "\u001b[32m[03/29 18:13:28 cifar100-global-random-resnet50]: \u001b[0mEpoch 37/100, Acc=0.5163, Val Loss=1.7706, lr=0.0100\n",
      "\u001b[32m[03/29 18:13:48 cifar100-global-random-resnet50]: \u001b[0mEpoch 38/100, Acc=0.5205, Val Loss=1.7483, lr=0.0100\n",
      "\u001b[32m[03/29 18:14:09 cifar100-global-random-resnet50]: \u001b[0mEpoch 39/100, Acc=0.5192, Val Loss=1.7355, lr=0.0100\n",
      "\u001b[32m[03/29 18:14:29 cifar100-global-random-resnet50]: \u001b[0mEpoch 40/100, Acc=0.5255, Val Loss=1.7711, lr=0.0100\n",
      "\u001b[32m[03/29 18:14:49 cifar100-global-random-resnet50]: \u001b[0mEpoch 41/100, Acc=0.5277, Val Loss=1.7363, lr=0.0100\n",
      "\u001b[32m[03/29 18:15:10 cifar100-global-random-resnet50]: \u001b[0mEpoch 42/100, Acc=0.5261, Val Loss=1.7468, lr=0.0100\n",
      "\u001b[32m[03/29 18:15:30 cifar100-global-random-resnet50]: \u001b[0mEpoch 43/100, Acc=0.5283, Val Loss=1.7492, lr=0.0100\n",
      "\u001b[32m[03/29 18:15:50 cifar100-global-random-resnet50]: \u001b[0mEpoch 44/100, Acc=0.5350, Val Loss=1.7193, lr=0.0100\n",
      "\u001b[32m[03/29 18:16:11 cifar100-global-random-resnet50]: \u001b[0mEpoch 45/100, Acc=0.5146, Val Loss=1.8251, lr=0.0100\n",
      "\u001b[32m[03/29 18:16:30 cifar100-global-random-resnet50]: \u001b[0mEpoch 46/100, Acc=0.5118, Val Loss=1.8268, lr=0.0100\n",
      "\u001b[32m[03/29 18:16:51 cifar100-global-random-resnet50]: \u001b[0mEpoch 47/100, Acc=0.5326, Val Loss=1.7164, lr=0.0100\n",
      "\u001b[32m[03/29 18:17:11 cifar100-global-random-resnet50]: \u001b[0mEpoch 48/100, Acc=0.5448, Val Loss=1.6715, lr=0.0100\n",
      "\u001b[32m[03/29 18:17:31 cifar100-global-random-resnet50]: \u001b[0mEpoch 49/100, Acc=0.5060, Val Loss=1.8698, lr=0.0100\n",
      "\u001b[32m[03/29 18:17:51 cifar100-global-random-resnet50]: \u001b[0mEpoch 50/100, Acc=0.5470, Val Loss=1.6459, lr=0.0100\n",
      "\u001b[32m[03/29 18:18:11 cifar100-global-random-resnet50]: \u001b[0mEpoch 51/100, Acc=0.5236, Val Loss=1.7676, lr=0.0100\n",
      "\u001b[32m[03/29 18:18:32 cifar100-global-random-resnet50]: \u001b[0mEpoch 52/100, Acc=0.5315, Val Loss=1.7220, lr=0.0100\n",
      "\u001b[32m[03/29 18:18:52 cifar100-global-random-resnet50]: \u001b[0mEpoch 53/100, Acc=0.5278, Val Loss=1.7680, lr=0.0100\n",
      "\u001b[32m[03/29 18:19:12 cifar100-global-random-resnet50]: \u001b[0mEpoch 54/100, Acc=0.5365, Val Loss=1.7253, lr=0.0100\n",
      "\u001b[32m[03/29 18:19:33 cifar100-global-random-resnet50]: \u001b[0mEpoch 55/100, Acc=0.5485, Val Loss=1.6656, lr=0.0100\n",
      "\u001b[32m[03/29 18:19:53 cifar100-global-random-resnet50]: \u001b[0mEpoch 56/100, Acc=0.5536, Val Loss=1.6336, lr=0.0100\n",
      "\u001b[32m[03/29 18:20:13 cifar100-global-random-resnet50]: \u001b[0mEpoch 57/100, Acc=0.5568, Val Loss=1.6178, lr=0.0100\n",
      "\u001b[32m[03/29 18:20:33 cifar100-global-random-resnet50]: \u001b[0mEpoch 58/100, Acc=0.5314, Val Loss=1.7501, lr=0.0100\n",
      "\u001b[32m[03/29 18:20:54 cifar100-global-random-resnet50]: \u001b[0mEpoch 59/100, Acc=0.5394, Val Loss=1.6830, lr=0.0100\n",
      "\u001b[32m[03/29 18:21:14 cifar100-global-random-resnet50]: \u001b[0mEpoch 60/100, Acc=0.6108, Val Loss=1.3849, lr=0.0010\n",
      "\u001b[32m[03/29 18:21:34 cifar100-global-random-resnet50]: \u001b[0mEpoch 61/100, Acc=0.6156, Val Loss=1.3686, lr=0.0010\n",
      "\u001b[32m[03/29 18:21:55 cifar100-global-random-resnet50]: \u001b[0mEpoch 62/100, Acc=0.6163, Val Loss=1.3659, lr=0.0010\n",
      "\u001b[32m[03/29 18:22:15 cifar100-global-random-resnet50]: \u001b[0mEpoch 63/100, Acc=0.6199, Val Loss=1.3652, lr=0.0010\n",
      "\u001b[32m[03/29 18:22:35 cifar100-global-random-resnet50]: \u001b[0mEpoch 64/100, Acc=0.6199, Val Loss=1.3631, lr=0.0010\n",
      "\u001b[32m[03/29 18:22:56 cifar100-global-random-resnet50]: \u001b[0mEpoch 65/100, Acc=0.6215, Val Loss=1.3657, lr=0.0010\n",
      "\u001b[32m[03/29 18:23:16 cifar100-global-random-resnet50]: \u001b[0mEpoch 66/100, Acc=0.6240, Val Loss=1.3595, lr=0.0010\n",
      "\u001b[32m[03/29 18:23:37 cifar100-global-random-resnet50]: \u001b[0mEpoch 67/100, Acc=0.6261, Val Loss=1.3609, lr=0.0010\n",
      "\u001b[32m[03/29 18:23:57 cifar100-global-random-resnet50]: \u001b[0mEpoch 68/100, Acc=0.6268, Val Loss=1.3510, lr=0.0010\n",
      "\u001b[32m[03/29 18:24:17 cifar100-global-random-resnet50]: \u001b[0mEpoch 69/100, Acc=0.6212, Val Loss=1.3654, lr=0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/29 18:24:37 cifar100-global-random-resnet50]: \u001b[0mEpoch 70/100, Acc=0.6209, Val Loss=1.3687, lr=0.0010\n",
      "\u001b[32m[03/29 18:24:58 cifar100-global-random-resnet50]: \u001b[0mEpoch 71/100, Acc=0.6245, Val Loss=1.3629, lr=0.0010\n",
      "\u001b[32m[03/29 18:25:18 cifar100-global-random-resnet50]: \u001b[0mEpoch 72/100, Acc=0.6233, Val Loss=1.3609, lr=0.0010\n",
      "\u001b[32m[03/29 18:25:39 cifar100-global-random-resnet50]: \u001b[0mEpoch 73/100, Acc=0.6240, Val Loss=1.3659, lr=0.0010\n",
      "\u001b[32m[03/29 18:25:59 cifar100-global-random-resnet50]: \u001b[0mEpoch 74/100, Acc=0.6236, Val Loss=1.3517, lr=0.0010\n",
      "\u001b[32m[03/29 18:26:19 cifar100-global-random-resnet50]: \u001b[0mEpoch 75/100, Acc=0.6230, Val Loss=1.3584, lr=0.0010\n",
      "\u001b[32m[03/29 18:26:39 cifar100-global-random-resnet50]: \u001b[0mEpoch 76/100, Acc=0.6227, Val Loss=1.3776, lr=0.0010\n",
      "\u001b[32m[03/29 18:27:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 77/100, Acc=0.6231, Val Loss=1.3675, lr=0.0010\n",
      "\u001b[32m[03/29 18:27:20 cifar100-global-random-resnet50]: \u001b[0mEpoch 78/100, Acc=0.6262, Val Loss=1.3595, lr=0.0010\n",
      "\u001b[32m[03/29 18:27:40 cifar100-global-random-resnet50]: \u001b[0mEpoch 79/100, Acc=0.6233, Val Loss=1.3577, lr=0.0010\n",
      "\u001b[32m[03/29 18:28:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 80/100, Acc=0.6288, Val Loss=1.3418, lr=0.0001\n",
      "\u001b[32m[03/29 18:28:20 cifar100-global-random-resnet50]: \u001b[0mEpoch 81/100, Acc=0.6290, Val Loss=1.3417, lr=0.0001\n",
      "\u001b[32m[03/29 18:28:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 82/100, Acc=0.6307, Val Loss=1.3453, lr=0.0001\n",
      "\u001b[32m[03/29 18:29:01 cifar100-global-random-resnet50]: \u001b[0mEpoch 83/100, Acc=0.6327, Val Loss=1.3412, lr=0.0001\n",
      "\u001b[32m[03/29 18:29:21 cifar100-global-random-resnet50]: \u001b[0mEpoch 84/100, Acc=0.6313, Val Loss=1.3424, lr=0.0001\n",
      "\u001b[32m[03/29 18:29:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 85/100, Acc=0.6312, Val Loss=1.3429, lr=0.0001\n",
      "\u001b[32m[03/29 18:30:01 cifar100-global-random-resnet50]: \u001b[0mEpoch 86/100, Acc=0.6298, Val Loss=1.3433, lr=0.0001\n",
      "\u001b[32m[03/29 18:30:22 cifar100-global-random-resnet50]: \u001b[0mEpoch 87/100, Acc=0.6304, Val Loss=1.3437, lr=0.0001\n",
      "\u001b[32m[03/29 18:30:42 cifar100-global-random-resnet50]: \u001b[0mEpoch 88/100, Acc=0.6287, Val Loss=1.3447, lr=0.0001\n",
      "\u001b[32m[03/29 18:31:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 89/100, Acc=0.6287, Val Loss=1.3465, lr=0.0001\n",
      "\u001b[32m[03/29 18:31:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 90/100, Acc=0.6313, Val Loss=1.3415, lr=0.0001\n",
      "\u001b[32m[03/29 18:31:43 cifar100-global-random-resnet50]: \u001b[0mEpoch 91/100, Acc=0.6295, Val Loss=1.3430, lr=0.0001\n",
      "\u001b[32m[03/29 18:32:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 92/100, Acc=0.6320, Val Loss=1.3433, lr=0.0001\n",
      "\u001b[32m[03/29 18:32:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 93/100, Acc=0.6307, Val Loss=1.3434, lr=0.0001\n",
      "\u001b[32m[03/29 18:32:43 cifar100-global-random-resnet50]: \u001b[0mEpoch 94/100, Acc=0.6300, Val Loss=1.3437, lr=0.0001\n",
      "\u001b[32m[03/29 18:33:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 95/100, Acc=0.6308, Val Loss=1.3467, lr=0.0001\n",
      "\u001b[32m[03/29 18:33:24 cifar100-global-random-resnet50]: \u001b[0mEpoch 96/100, Acc=0.6302, Val Loss=1.3445, lr=0.0001\n",
      "\u001b[32m[03/29 18:33:44 cifar100-global-random-resnet50]: \u001b[0mEpoch 97/100, Acc=0.6293, Val Loss=1.3434, lr=0.0001\n",
      "\u001b[32m[03/29 18:34:04 cifar100-global-random-resnet50]: \u001b[0mEpoch 98/100, Acc=0.6288, Val Loss=1.3451, lr=0.0001\n",
      "\u001b[32m[03/29 18:34:24 cifar100-global-random-resnet50]: \u001b[0mEpoch 99/100, Acc=0.6299, Val Loss=1.3403, lr=0.0001\n",
      "\u001b[32m[03/29 18:34:24 cifar100-global-random-resnet50]: \u001b[0mBest Acc=0.6327\n"
     ]
    }
   ],
   "source": [
    "!python main.py --mode prune --model resnet50 --batch-size 128 --restore True --dataset cifar100  --method random --speed-up 100 --global-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900fe86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mmode: prune\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mmodel: resnet50\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mverbose: False\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mdataset: cifar100\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mbatch_size: 128\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mtotal_epochs: 100\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mlr_decay_milestones: 60,80\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mlr_decay_gamma: 0.1\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mlr: 0.01\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mrestore: True\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0moutput_dir: run/cifar100/prune/cifar100-global-random-resnet50\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mmethod: random\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mspeed_up: 100.0\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mmax_sparsity: 1.0\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msoft_keeping_ratio: 0.0\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mreg: 0.0005\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mweight_decay: 0.0005\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mseed: None\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mglobal_pruning: True\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msl_total_epochs: 100\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msl_lr: 0.01\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msl_lr_decay_milestones: 60,80\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msl_reg_warmup: 0\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0msl_restore: False\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0miterative_steps: 400\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mlogger: <Logger cifar100-global-random-resnet50 (DEBUG)>\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mdevice: cuda\n",
      "\u001b[32m[03/31 12:57:24 cifar100-global-random-resnet50]: \u001b[0mnum_classes: 100\n",
      "\u001b[32m[03/31 12:57:25 cifar100-global-random-resnet50]: \u001b[0mLoading model from True\n",
      "\u001b[32m[03/31 12:57:38 cifar100-global-random-resnet50]: \u001b[0mPruning...\n",
      "\u001b[32m[03/31 12:57:53 cifar100-global-random-resnet50]: \u001b[0mResNet(\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(3, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(3, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(19, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(19, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(19, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(15, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(19, 57, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(57, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(10, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(57, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(12, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(57, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(14, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(57, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(23, 22, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(22, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(57, 106, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(106, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(34, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(24, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(106, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(27, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(31, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(106, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(22, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(24, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(106, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(25, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(17, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(106, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(31, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(106, 49, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(49, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(48, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(106, 180, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(180, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(40, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(57, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(180, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(55, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(45, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=180, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/31 12:57:55 cifar100-global-random-resnet50]: \u001b[0mParams: 23.71 M => 0.24 M (1.01%)\n",
      "\u001b[32m[03/31 12:57:55 cifar100-global-random-resnet50]: \u001b[0mFLOPs: 1304.88 M => 12.31 M (0.94%, 105.97X )\n",
      "\u001b[32m[03/31 12:57:55 cifar100-global-random-resnet50]: \u001b[0mAcc: 0.7891 => 0.0100\n",
      "\u001b[32m[03/31 12:57:55 cifar100-global-random-resnet50]: \u001b[0mVal Loss: 0.9788 => 4.6069\n",
      "\u001b[32m[03/31 12:57:55 cifar100-global-random-resnet50]: \u001b[0mFinetuning...\n",
      "\u001b[32m[03/31 12:58:15 cifar100-global-random-resnet50]: \u001b[0mEpoch 0/100, Acc=0.0530, Val Loss=4.2449, lr=0.0100\n",
      "\u001b[32m[03/31 12:58:35 cifar100-global-random-resnet50]: \u001b[0mEpoch 1/100, Acc=0.0906, Val Loss=3.9503, lr=0.0100\n",
      "\u001b[32m[03/31 12:58:53 cifar100-global-random-resnet50]: \u001b[0mEpoch 2/100, Acc=0.1194, Val Loss=3.7493, lr=0.0100\n",
      "\u001b[32m[03/31 12:59:12 cifar100-global-random-resnet50]: \u001b[0mEpoch 3/100, Acc=0.1413, Val Loss=3.6086, lr=0.0100\n",
      "\u001b[32m[03/31 12:59:31 cifar100-global-random-resnet50]: \u001b[0mEpoch 4/100, Acc=0.1626, Val Loss=3.5166, lr=0.0100\n",
      "\u001b[32m[03/31 12:59:50 cifar100-global-random-resnet50]: \u001b[0mEpoch 5/100, Acc=0.1783, Val Loss=3.4337, lr=0.0100\n",
      "\u001b[32m[03/31 13:00:08 cifar100-global-random-resnet50]: \u001b[0mEpoch 6/100, Acc=0.2005, Val Loss=3.3100, lr=0.0100\n",
      "\u001b[32m[03/31 13:00:27 cifar100-global-random-resnet50]: \u001b[0mEpoch 7/100, Acc=0.2224, Val Loss=3.1945, lr=0.0100\n",
      "\u001b[32m[03/31 13:00:46 cifar100-global-random-resnet50]: \u001b[0mEpoch 8/100, Acc=0.2273, Val Loss=3.1582, lr=0.0100\n",
      "\u001b[32m[03/31 13:01:05 cifar100-global-random-resnet50]: \u001b[0mEpoch 9/100, Acc=0.2384, Val Loss=3.0919, lr=0.0100\n",
      "\u001b[32m[03/31 13:01:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 10/100, Acc=0.2510, Val Loss=3.0035, lr=0.0100\n",
      "\u001b[32m[03/31 13:01:42 cifar100-global-random-resnet50]: \u001b[0mEpoch 11/100, Acc=0.2676, Val Loss=2.9208, lr=0.0100\n",
      "\u001b[32m[03/31 13:02:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 12/100, Acc=0.2821, Val Loss=2.8530, lr=0.0100\n",
      "\u001b[32m[03/31 13:02:19 cifar100-global-random-resnet50]: \u001b[0mEpoch 13/100, Acc=0.3022, Val Loss=2.7580, lr=0.0100\n",
      "\u001b[32m[03/31 13:02:38 cifar100-global-random-resnet50]: \u001b[0mEpoch 14/100, Acc=0.3023, Val Loss=2.7891, lr=0.0100\n",
      "\u001b[32m[03/31 13:02:54 cifar100-global-random-resnet50]: \u001b[0mEpoch 15/100, Acc=0.3236, Val Loss=2.6610, lr=0.0100\n",
      "\u001b[32m[03/31 13:03:13 cifar100-global-random-resnet50]: \u001b[0mEpoch 16/100, Acc=0.3302, Val Loss=2.6449, lr=0.0100\n",
      "\u001b[32m[03/31 13:03:31 cifar100-global-random-resnet50]: \u001b[0mEpoch 17/100, Acc=0.2922, Val Loss=2.9174, lr=0.0100\n",
      "\u001b[32m[03/31 13:03:50 cifar100-global-random-resnet50]: \u001b[0mEpoch 18/100, Acc=0.3285, Val Loss=2.6120, lr=0.0100\n",
      "\u001b[32m[03/31 13:04:09 cifar100-global-random-resnet50]: \u001b[0mEpoch 19/100, Acc=0.3622, Val Loss=2.4830, lr=0.0100\n",
      "\u001b[32m[03/31 13:04:28 cifar100-global-random-resnet50]: \u001b[0mEpoch 20/100, Acc=0.3568, Val Loss=2.4762, lr=0.0100\n",
      "\u001b[32m[03/31 13:04:47 cifar100-global-random-resnet50]: \u001b[0mEpoch 21/100, Acc=0.2320, Val Loss=3.4111, lr=0.0100\n",
      "\u001b[32m[03/31 13:05:06 cifar100-global-random-resnet50]: \u001b[0mEpoch 22/100, Acc=0.3702, Val Loss=2.4440, lr=0.0100\n",
      "\u001b[32m[03/31 13:05:25 cifar100-global-random-resnet50]: \u001b[0mEpoch 23/100, Acc=0.3862, Val Loss=2.3625, lr=0.0100\n",
      "\u001b[32m[03/31 13:05:44 cifar100-global-random-resnet50]: \u001b[0mEpoch 24/100, Acc=0.4047, Val Loss=2.2988, lr=0.0100\n",
      "\u001b[32m[03/31 13:06:01 cifar100-global-random-resnet50]: \u001b[0mEpoch 25/100, Acc=0.4055, Val Loss=2.2621, lr=0.0100\n",
      "\u001b[32m[03/31 13:06:19 cifar100-global-random-resnet50]: \u001b[0mEpoch 26/100, Acc=0.4063, Val Loss=2.2710, lr=0.0100\n",
      "\u001b[32m[03/31 13:06:36 cifar100-global-random-resnet50]: \u001b[0mEpoch 27/100, Acc=0.4200, Val Loss=2.2049, lr=0.0100\n",
      "\u001b[32m[03/31 13:06:55 cifar100-global-random-resnet50]: \u001b[0mEpoch 28/100, Acc=0.4236, Val Loss=2.1694, lr=0.0100\n",
      "\u001b[32m[03/31 13:07:14 cifar100-global-random-resnet50]: \u001b[0mEpoch 29/100, Acc=0.4218, Val Loss=2.1967, lr=0.0100\n",
      "\u001b[32m[03/31 13:07:33 cifar100-global-random-resnet50]: \u001b[0mEpoch 30/100, Acc=0.4060, Val Loss=2.3005, lr=0.0100\n",
      "\u001b[32m[03/31 13:07:52 cifar100-global-random-resnet50]: \u001b[0mEpoch 31/100, Acc=0.4081, Val Loss=2.2793, lr=0.0100\n",
      "\u001b[32m[03/31 13:08:12 cifar100-global-random-resnet50]: \u001b[0mEpoch 32/100, Acc=0.4615, Val Loss=2.0193, lr=0.0100\n",
      "\u001b[32m[03/31 13:08:31 cifar100-global-random-resnet50]: \u001b[0mEpoch 33/100, Acc=0.4482, Val Loss=2.0827, lr=0.0100\n",
      "\u001b[32m[03/31 13:08:50 cifar100-global-random-resnet50]: \u001b[0mEpoch 34/100, Acc=0.4415, Val Loss=2.0973, lr=0.0100\n",
      "\u001b[32m[03/31 13:09:09 cifar100-global-random-resnet50]: \u001b[0mEpoch 35/100, Acc=0.4593, Val Loss=2.0282, lr=0.0100\n",
      "\u001b[32m[03/31 13:09:28 cifar100-global-random-resnet50]: \u001b[0mEpoch 36/100, Acc=0.4464, Val Loss=2.1190, lr=0.0100\n",
      "\u001b[32m[03/31 13:09:45 cifar100-global-random-resnet50]: \u001b[0mEpoch 37/100, Acc=0.4652, Val Loss=2.0012, lr=0.0100\n",
      "\u001b[32m[03/31 13:10:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 38/100, Acc=0.4759, Val Loss=1.9635, lr=0.0100\n",
      "\u001b[32m[03/31 13:10:19 cifar100-global-random-resnet50]: \u001b[0mEpoch 39/100, Acc=0.4692, Val Loss=1.9765, lr=0.0100\n",
      "\u001b[32m[03/31 13:10:39 cifar100-global-random-resnet50]: \u001b[0mEpoch 40/100, Acc=0.4631, Val Loss=2.0307, lr=0.0100\n",
      "\u001b[32m[03/31 13:10:58 cifar100-global-random-resnet50]: \u001b[0mEpoch 41/100, Acc=0.4553, Val Loss=2.0796, lr=0.0100\n",
      "\u001b[32m[03/31 13:11:17 cifar100-global-random-resnet50]: \u001b[0mEpoch 42/100, Acc=0.4674, Val Loss=2.0414, lr=0.0100\n",
      "\u001b[32m[03/31 13:11:37 cifar100-global-random-resnet50]: \u001b[0mEpoch 43/100, Acc=0.4709, Val Loss=1.9972, lr=0.0100\n",
      "\u001b[32m[03/31 13:11:55 cifar100-global-random-resnet50]: \u001b[0mEpoch 44/100, Acc=0.4844, Val Loss=1.9109, lr=0.0100\n",
      "\u001b[32m[03/31 13:12:14 cifar100-global-random-resnet50]: \u001b[0mEpoch 45/100, Acc=0.4916, Val Loss=1.8660, lr=0.0100\n",
      "\u001b[32m[03/31 13:12:33 cifar100-global-random-resnet50]: \u001b[0mEpoch 46/100, Acc=0.4919, Val Loss=1.8969, lr=0.0100\n",
      "\u001b[32m[03/31 13:12:48 cifar100-global-random-resnet50]: \u001b[0mEpoch 47/100, Acc=0.4872, Val Loss=1.9528, lr=0.0100\n",
      "\u001b[32m[03/31 13:13:07 cifar100-global-random-resnet50]: \u001b[0mEpoch 48/100, Acc=0.4803, Val Loss=1.9476, lr=0.0100\n",
      "\u001b[32m[03/31 13:13:25 cifar100-global-random-resnet50]: \u001b[0mEpoch 49/100, Acc=0.4762, Val Loss=1.9908, lr=0.0100\n",
      "\u001b[32m[03/31 13:13:44 cifar100-global-random-resnet50]: \u001b[0mEpoch 50/100, Acc=0.4914, Val Loss=1.9071, lr=0.0100\n",
      "\u001b[32m[03/31 13:14:03 cifar100-global-random-resnet50]: \u001b[0mEpoch 51/100, Acc=0.4204, Val Loss=2.3809, lr=0.0100\n",
      "\u001b[32m[03/31 13:14:22 cifar100-global-random-resnet50]: \u001b[0mEpoch 52/100, Acc=0.4985, Val Loss=1.8660, lr=0.0100\n",
      "\u001b[32m[03/31 13:14:40 cifar100-global-random-resnet50]: \u001b[0mEpoch 53/100, Acc=0.4371, Val Loss=2.2244, lr=0.0100\n",
      "\u001b[32m[03/31 13:14:59 cifar100-global-random-resnet50]: \u001b[0mEpoch 54/100, Acc=0.4977, Val Loss=1.8624, lr=0.0100\n",
      "\u001b[32m[03/31 13:15:17 cifar100-global-random-resnet50]: \u001b[0mEpoch 55/100, Acc=0.4975, Val Loss=1.8615, lr=0.0100\n",
      "\u001b[32m[03/31 13:15:35 cifar100-global-random-resnet50]: \u001b[0mEpoch 56/100, Acc=0.5104, Val Loss=1.8044, lr=0.0100\n",
      "\u001b[32m[03/31 13:15:55 cifar100-global-random-resnet50]: \u001b[0mEpoch 57/100, Acc=0.4820, Val Loss=1.9876, lr=0.0100\n",
      "\u001b[32m[03/31 13:16:14 cifar100-global-random-resnet50]: \u001b[0mEpoch 58/100, Acc=0.4646, Val Loss=2.0862, lr=0.0100\n",
      "\u001b[32m[03/31 13:16:33 cifar100-global-random-resnet50]: \u001b[0mEpoch 59/100, Acc=0.5152, Val Loss=1.7874, lr=0.0100\n",
      "\u001b[32m[03/31 13:16:51 cifar100-global-random-resnet50]: \u001b[0mEpoch 60/100, Acc=0.5780, Val Loss=1.5199, lr=0.0010\n",
      "\u001b[32m[03/31 13:17:10 cifar100-global-random-resnet50]: \u001b[0mEpoch 61/100, Acc=0.5820, Val Loss=1.5082, lr=0.0010\n",
      "\u001b[32m[03/31 13:17:29 cifar100-global-random-resnet50]: \u001b[0mEpoch 62/100, Acc=0.5855, Val Loss=1.4999, lr=0.0010\n",
      "\u001b[32m[03/31 13:17:48 cifar100-global-random-resnet50]: \u001b[0mEpoch 63/100, Acc=0.5865, Val Loss=1.4934, lr=0.0010\n",
      "\u001b[32m[03/31 13:18:04 cifar100-global-random-resnet50]: \u001b[0mEpoch 64/100, Acc=0.5852, Val Loss=1.4967, lr=0.0010\n",
      "\u001b[32m[03/31 13:18:23 cifar100-global-random-resnet50]: \u001b[0mEpoch 65/100, Acc=0.5845, Val Loss=1.4942, lr=0.0010\n",
      "\u001b[32m[03/31 13:18:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 66/100, Acc=0.5860, Val Loss=1.4883, lr=0.0010\n",
      "\u001b[32m[03/31 13:18:58 cifar100-global-random-resnet50]: \u001b[0mEpoch 67/100, Acc=0.5829, Val Loss=1.4836, lr=0.0010\n",
      "\u001b[32m[03/31 13:19:16 cifar100-global-random-resnet50]: \u001b[0mEpoch 68/100, Acc=0.5881, Val Loss=1.4895, lr=0.0010\n",
      "\u001b[32m[03/31 13:19:35 cifar100-global-random-resnet50]: \u001b[0mEpoch 69/100, Acc=0.5905, Val Loss=1.4876, lr=0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/31 13:19:53 cifar100-global-random-resnet50]: \u001b[0mEpoch 70/100, Acc=0.5855, Val Loss=1.5011, lr=0.0010\n",
      "\u001b[32m[03/31 13:20:12 cifar100-global-random-resnet50]: \u001b[0mEpoch 71/100, Acc=0.5852, Val Loss=1.5054, lr=0.0010\n",
      "\u001b[32m[03/31 13:20:31 cifar100-global-random-resnet50]: \u001b[0mEpoch 72/100, Acc=0.5912, Val Loss=1.4862, lr=0.0010\n",
      "\u001b[32m[03/31 13:20:49 cifar100-global-random-resnet50]: \u001b[0mEpoch 73/100, Acc=0.5897, Val Loss=1.4850, lr=0.0010\n",
      "\u001b[32m[03/31 13:21:08 cifar100-global-random-resnet50]: \u001b[0mEpoch 74/100, Acc=0.5896, Val Loss=1.4862, lr=0.0010\n",
      "\u001b[32m[03/31 13:21:27 cifar100-global-random-resnet50]: \u001b[0mEpoch 75/100, Acc=0.5880, Val Loss=1.4944, lr=0.0010\n",
      "\u001b[32m[03/31 13:21:46 cifar100-global-random-resnet50]: \u001b[0mEpoch 76/100, Acc=0.5870, Val Loss=1.5102, lr=0.0010\n",
      "\u001b[32m[03/31 13:22:02 cifar100-global-random-resnet50]: \u001b[0mEpoch 77/100, Acc=0.5854, Val Loss=1.5004, lr=0.0010\n",
      "\u001b[32m[03/31 13:22:21 cifar100-global-random-resnet50]: \u001b[0mEpoch 78/100, Acc=0.5895, Val Loss=1.4869, lr=0.0010\n",
      "\u001b[32m[03/31 13:22:40 cifar100-global-random-resnet50]: \u001b[0mEpoch 79/100, Acc=0.5915, Val Loss=1.4866, lr=0.0010\n",
      "\u001b[32m[03/31 13:22:58 cifar100-global-random-resnet50]: \u001b[0mEpoch 80/100, Acc=0.5952, Val Loss=1.4716, lr=0.0001\n",
      "\u001b[32m[03/31 13:23:17 cifar100-global-random-resnet50]: \u001b[0mEpoch 81/100, Acc=0.5944, Val Loss=1.4695, lr=0.0001\n",
      "\u001b[32m[03/31 13:23:36 cifar100-global-random-resnet50]: \u001b[0mEpoch 82/100, Acc=0.5938, Val Loss=1.4723, lr=0.0001\n",
      "\u001b[32m[03/31 13:23:54 cifar100-global-random-resnet50]: \u001b[0mEpoch 83/100, Acc=0.5955, Val Loss=1.4694, lr=0.0001\n",
      "\u001b[32m[03/31 13:24:13 cifar100-global-random-resnet50]: \u001b[0mEpoch 84/100, Acc=0.5933, Val Loss=1.4712, lr=0.0001\n",
      "\u001b[32m[03/31 13:24:32 cifar100-global-random-resnet50]: \u001b[0mEpoch 85/100, Acc=0.5929, Val Loss=1.4733, lr=0.0001\n",
      "\u001b[32m[03/31 13:24:51 cifar100-global-random-resnet50]: \u001b[0mEpoch 86/100, Acc=0.5949, Val Loss=1.4720, lr=0.0001\n",
      "\u001b[32m[03/31 13:25:08 cifar100-global-random-resnet50]: \u001b[0mEpoch 87/100, Acc=0.5968, Val Loss=1.4715, lr=0.0001\n",
      "\u001b[32m[03/31 13:25:27 cifar100-global-random-resnet50]: \u001b[0mEpoch 88/100, Acc=0.5949, Val Loss=1.4700, lr=0.0001\n",
      "\u001b[32m[03/31 13:25:46 cifar100-global-random-resnet50]: \u001b[0mEpoch 89/100, Acc=0.5958, Val Loss=1.4705, lr=0.0001\n",
      "\u001b[32m[03/31 13:26:05 cifar100-global-random-resnet50]: \u001b[0mEpoch 90/100, Acc=0.5970, Val Loss=1.4673, lr=0.0001\n",
      "\u001b[32m[03/31 13:26:24 cifar100-global-random-resnet50]: \u001b[0mEpoch 91/100, Acc=0.5959, Val Loss=1.4685, lr=0.0001\n",
      "\u001b[32m[03/31 13:26:43 cifar100-global-random-resnet50]: \u001b[0mEpoch 92/100, Acc=0.5953, Val Loss=1.4671, lr=0.0001\n",
      "\u001b[32m[03/31 13:27:02 cifar100-global-random-resnet50]: \u001b[0mEpoch 93/100, Acc=0.5950, Val Loss=1.4700, lr=0.0001\n",
      "\u001b[32m[03/31 13:27:22 cifar100-global-random-resnet50]: \u001b[0mEpoch 94/100, Acc=0.5940, Val Loss=1.4693, lr=0.0001\n",
      "\u001b[32m[03/31 13:27:41 cifar100-global-random-resnet50]: \u001b[0mEpoch 95/100, Acc=0.5947, Val Loss=1.4700, lr=0.0001\n",
      "\u001b[32m[03/31 13:28:00 cifar100-global-random-resnet50]: \u001b[0mEpoch 96/100, Acc=0.5940, Val Loss=1.4702, lr=0.0001\n",
      "\u001b[32m[03/31 13:28:18 cifar100-global-random-resnet50]: \u001b[0mEpoch 97/100, Acc=0.5947, Val Loss=1.4682, lr=0.0001\n",
      "\u001b[32m[03/31 13:28:37 cifar100-global-random-resnet50]: \u001b[0mEpoch 98/100, Acc=0.5964, Val Loss=1.4679, lr=0.0001\n",
      "\u001b[32m[03/31 13:28:55 cifar100-global-random-resnet50]: \u001b[0mEpoch 99/100, Acc=0.5942, Val Loss=1.4687, lr=0.0001\n",
      "\u001b[32m[03/31 13:28:55 cifar100-global-random-resnet50]: \u001b[0mBest Acc=0.5970\n"
     ]
    }
   ],
   "source": [
    "!python main.py --mode prune --model resnet50 --batch-size 128 --restore True --dataset cifar100  --method random --speed-up 100 --global-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de2b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python main.py --mode pretrain --dataset cifar100 --model vit_cifar --lr 0.1 --total-epochs 200 --lr-decay-milestones 120,150,180 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path='/nfs/shared/imagenet2012/', model='vit_b_16', pretrained=True, device='cuda', batch_size=64, epochs=90, workers=16, opt='sgd', lr=0.01, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='steplr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=100, output_dir='/nfs/homedirs/rachwan/Torch-Pruning/benchmarks/run/imagenet/prune/', resume='', start_epoch=0, cache_dataset=True, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=False, world_size=1, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, prune=True, method='l1', global_pruning=True, target_flops=5.0, soft_keeping_ratio=0.0, reg=0.0001, max_ch_sparsity=1.0, sl_epochs=None, sl_resume=None, sl_lr=None, sl_lr_step_size=None, sl_lr_warmup_epochs=None)\n",
      "Loading data...\n",
      "Loading training data...\n",
      "Loading dataset_train from /nfs/homedirs/rachwan/.torch/vision/datasets/imagefolder/a27aad9d5e.pt\n",
      "Data loading took 1.1509568691253662\n",
      "Loading validation data...\n",
      "Loading dataset_test from /nfs/homedirs/rachwan/.torch/vision/datasets/imagefolder/09d97dbe34.pt\n",
      "Creating data loaders...\n",
      "Creating model\n",
      "================\n",
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 86.5677 M\n",
      "ops: 17.5860 G\n",
      "================\n",
      "Pruning model...\n",
      "================\n",
      "After pruning:\n",
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 468, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=240, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=240, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=780, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=780, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=684, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=684, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=84, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=84, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=48, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=48, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=48, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=48, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=72, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=72, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=276, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=276, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=1140, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1140, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=660, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=660, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=384, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=384, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=468, out_features=468, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=468, out_features=456, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=456, out_features=468, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((468,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=468, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Params: 86.57 M => 16.05 M (18.54%)\n",
      "Ops: 17.59 G => 3.49 G (19.84%, 5.04X )\n",
      "================\n",
      "Loading data...\n",
      "Loading training data...\n",
      "Loading dataset_train from /nfs/homedirs/rachwan/.torch/vision/datasets/imagefolder/a27aad9d5e.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading took 1.0255894660949707\n",
      "Loading validation data...\n",
      "Loading dataset_test from /nfs/homedirs/rachwan/.torch/vision/datasets/imagefolder/09d97dbe34.pt\n",
      "Creating data loaders...\n",
      "Finetuning...\n",
      "Test:   [  0/782]  eta: 1:46:03  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 8.1371  data: 3.8938  max mem: 371\n",
      "Test:   [100/782]  eta: 0:02:05  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1460  data: 0.0729  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:26  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1066  data: 0.0349  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:10  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1608  data: 0.0875  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:53  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1022  data: 0.0285  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:38  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1164  data: 0.0433  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:24  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1191  data: 0.0453  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1124  data: 0.0384  max mem: 371\n",
      "Test:  Total time: 0:01:40\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 0/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:46:52  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.5963  data: 3.5164  max mem: 371\n",
      "Test:   [100/782]  eta: 0:02:03  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.2285  data: 0.1550  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:26  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1051  data: 0.0310  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:08  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1424  data: 0.0667  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:51  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1122  data: 0.0336  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:37  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1489  data: 0.0714  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:23  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1421  data: 0.0677  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1240  data: 0.0491  max mem: 371\n",
      "Test:  Total time: 0:01:39\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 1/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:44:59  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.4524  data: 3.3713  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:43  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1124  data: 0.0374  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:19  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1135  data: 0.0361  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:07  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1864  data: 0.1103  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:51  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1030  data: 0.0279  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:36  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.0986  data: 0.0232  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:23  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.0947  data: 0.0146  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1217  data: 0.0459  max mem: 371\n",
      "Test:  Total time: 0:01:39\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 2/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:51:31  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.9527  data: 3.8672  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:48  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1425  data: 0.0671  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:20  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1107  data: 0.0361  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:05  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1254  data: 0.0497  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:49  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.0897  data: 0.0115  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:37  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1249  data: 0.0496  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:23  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1091  data: 0.0334  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1382  data: 0.0622  max mem: 371\n",
      "Test:  Total time: 0:01:38\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 3/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:46:50  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.5940  data: 3.5143  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:45  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1271  data: 0.0500  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:21  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.0998  data: 0.0250  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:06  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1350  data: 0.0598  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:50  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1104  data: 0.0356  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:36  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1031  data: 0.0240  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:22  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.0910  data: 0.0131  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1212  data: 0.0459  max mem: 371\n",
      "Test:  Total time: 0:01:37\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 4/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:42:21  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.2496  data: 3.1536  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:46  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1460  data: 0.0718  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:19  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.0869  data: 0.0117  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:03  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1448  data: 0.0658  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:49  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1109  data: 0.0355  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:35  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1076  data: 0.0325  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:22  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1349  data: 0.0603  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.0905  data: 0.0116  max mem: 371\n",
      "Test:  Total time: 0:01:35\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 5/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:45:10  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.4657  data: 3.3828  max mem: 371\n",
      "Test:   [100/782]  eta: 0:02:12  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1769  data: 0.1021  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:31  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1014  data: 0.0261  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:10  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1121  data: 0.0377  max mem: 371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:   [400/782]  eta: 0:00:53  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1033  data: 0.0276  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:37  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1229  data: 0.0455  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:26  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1184  data: 0.0430  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:11  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1030  data: 0.0280  max mem: 371\n",
      "Test:  Total time: 0:01:46\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 6/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:42:51  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.2886  data: 3.2099  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:49  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1546  data: 0.0782  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:22  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1188  data: 0.0442  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:06  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1158  data: 0.0413  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:50  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1061  data: 0.0303  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:36  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1281  data: 0.0536  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:23  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.0972  data: 0.0198  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1343  data: 0.0597  max mem: 371\n",
      "Test:  Total time: 0:01:37\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 7/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:42:41  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.2755  data: 3.1960  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:44  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1339  data: 0.0581  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:18  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1022  data: 0.0276  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:03  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1277  data: 0.0509  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:48  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1086  data: 0.0337  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:35  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1148  data: 0.0393  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:22  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.1073  data: 0.0322  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1255  data: 0.0512  max mem: 371\n",
      "Test:  Total time: 0:01:36\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 8/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:41:14  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.1645  data: 3.0719  max mem: 371\n",
      "Test:   [100/782]  eta: 0:01:41  loss: 7.0572 (7.0714)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.1083)  time: 0.1293  data: 0.0553  max mem: 371\n",
      "Test:   [200/782]  eta: 0:01:20  loss: 7.0634 (7.0878)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.3498)  time: 0.1302  data: 0.0556  max mem: 371\n",
      "Test:   [300/782]  eta: 0:01:04  loss: 7.1144 (7.0699)  acc1: 0.0000 (0.0311)  acc5: 0.0000 (0.4153)  time: 0.1366  data: 0.0618  max mem: 371\n",
      "Test:   [400/782]  eta: 0:00:49  loss: 6.9716 (7.0409)  acc1: 0.0000 (0.0234)  acc5: 0.0000 (0.3780)  time: 0.1061  data: 0.0309  max mem: 371\n",
      "Test:   [500/782]  eta: 0:00:36  loss: 6.9389 (7.0259)  acc1: 0.0000 (0.0187)  acc5: 0.0000 (0.3680)  time: 0.1267  data: 0.0488  max mem: 371\n",
      "Test:   [600/782]  eta: 0:00:22  loss: 7.0213 (7.0062)  acc1: 0.0000 (0.0416)  acc5: 0.0000 (0.5122)  time: 0.0977  data: 0.0189  max mem: 371\n",
      "Test:   [700/782]  eta: 0:00:10  loss: 6.7176 (6.9986)  acc1: 0.0000 (0.1092)  acc5: 0.0000 (0.5617)  time: 0.1361  data: 0.0616  max mem: 371\n",
      "Test:  Total time: 0:01:36\n",
      "Test:  Acc@1 0.098 Acc@5 0.510\n",
      "Epoch 9/90, Current Best Acc = 0.098000\n",
      "Test:   [  0/782]  eta: 0:42:52  loss: 7.4170 (7.4170)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 3.2901  data: 3.2111  max mem: 371\n"
     ]
    }
   ],
   "source": [
    "!python main_imagenet.py --model vit_b_16 --epochs 90 --batch-size 64 --lr-step-size 30 --lr 0.01 --prune --method l1 --pretrained --output-dir run/imagenet/resnet50_sl --target-flops 5.00 --cache-dataset --print-freq 100 --workers 16 --global-pruning --data-path '/nfs/shared/imagenet2012/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5005b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
